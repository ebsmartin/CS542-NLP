{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CS542 Fall 2021 Homework 3\n",
    "# Part-of-speech Tagging with Structured Perceptrons\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "import random\n",
    "from random import Random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "class POSTagger():\n",
    "\n",
    "    def __init__(self):\n",
    "        # for testing with the toy corpus from worked example\n",
    "        # self.tag_dict = {'nn': 0, 'vb': 1, 'dt': 2}\n",
    "        # self.word_dict = {'Alice': 0, 'admired': 1, 'Dorothy': 2, 'every': 3,\n",
    "        #                   'dwarf': 4, 'cheered': 5}\n",
    "\n",
    "        self.tag_dict = {}\n",
    "        self.word_dict = {}\n",
    "\n",
    "\n",
    "        # initial tag weights [shape = (len(tag_dict),)]\n",
    "        initial_shape = (len(self.tag_dict),)\n",
    "        # looks like we can use the * to unpack the shape tuple since the randn() needs that\n",
    "        self.initial = np.random.randn(*initial_shape) # create a random normal distribution of weights\n",
    "        # self.initial = np.array([-0.3, -0.7, 0.3])\n",
    "        # tag-to-tag transition weights [shape = (len(tag_dict),len(tag_dict))]\n",
    "        transition_shape = (len(self.tag_dict), len(self.tag_dict))\n",
    "        self.transition = np.random.randn(*transition_shape)\n",
    "        # self.transition = np.array([[-0.7, 0.3, -0.3],\n",
    "        #                             [-0.3, -0.7, 0.3],\n",
    "        #                             [0.3, -0.3, -0.7]])\n",
    "        # tag emission weights [shape = (len(word_dict),len(tag_dict))]\n",
    "        emission_shape = (len(self.word_dict), len(self.tag_dict))\n",
    "        self.emission = np.random.randn(*emission_shape)\n",
    "        # self.emission = np.array([[-0.3, -0.7, 0.3],\n",
    "        #                           [0.3, -0.3, -0.7],\n",
    "        #                           [-0.3, 0.3, -0.7],\n",
    "        #                           [-0.7, -0.3, 0.3],\n",
    "        #                           [0.3, -0.7, -0.3],\n",
    "        #                           [-0.7, 0.3, -0.3]])\n",
    "        self.unk_index = -1\n",
    "\n",
    "    '''\n",
    "    Fills in self.tag_dict and self.word_dict, based on the training data.\n",
    "    '''\n",
    "    def make_dicts(self, train_set):\n",
    "        tag_vocabulary = set()\n",
    "        word_vocabulary = set()\n",
    "        # iterate over training documents\n",
    "        for root, dirs, files in os.walk(train_set):\n",
    "            for name in files:\n",
    "                with open(os.path.join(root, name)) as f:\n",
    "                    # BEGIN STUDENT CODE\n",
    "                    # create vocabularies of every tag and word that exists in the training data\n",
    "                    for line in f:\n",
    "                        line = line.strip()\n",
    "                        if line == '':\n",
    "                            continue\n",
    "                        word_tag_pairs = line.split()\n",
    "                        for pair in word_tag_pairs:\n",
    "                            word, tag = pair.rsplit('/', maxsplit=1)\n",
    "                            tag_vocabulary.add(tag)\n",
    "                            word_vocabulary.add(word)\n",
    "                    # END STUDENT CODE\n",
    "        # create tag_dict and word_dict\n",
    "        # if you implemented the rest of this function correctly, these should be formatte as they are above in __init__\n",
    "        self.tag_dict = {v: k for k, v in enumerate(tag_vocabulary)}\n",
    "        self.word_dict = {v: k for k, v in enumerate(word_vocabulary)}\n",
    "\n",
    "    '''\n",
    "    # Loads a dataset. Specifically, returns a list of sentence_ids, and\n",
    "    # dictionaries of tag_lists and word_lists such that:\n",
    "    # tag_lists[sentence_id] = list of part-of-speech tags in the sentence\n",
    "    # word_lists[sentence_id] = list of words in the sentence\n",
    "    # '''\n",
    "    # def load_data(self, data_set):\n",
    "    #     sentence_ids = [] # doc name + ordinal number of sentence (e.g., ca010)\n",
    "    #     sentences = dict()\n",
    "    #     tag_lists = dict()\n",
    "    #     word_lists = dict()\n",
    "    #     # iterate over documents\n",
    "    #     for root, dirs, files in os.walk(data_set):\n",
    "    #         for name in files:\n",
    "    #             with open(os.path.join(root, name)) as f:\n",
    "    #                 # be sure to split documents into sentences here\n",
    "    #                 # BEGIN STUDENT CODE\n",
    "    #                 # for each sentence in the document\n",
    "    #                 index = 0\n",
    "    #                 for sentence in f: # each line is a sentence\n",
    "    #                     #  1) create a list of tags and list of words that appear in this sentence\n",
    "    #                     tag_list = []\n",
    "    #                     word_list = []\n",
    "    #                     sentence = sentence.strip()\n",
    "    #                     if sentence == '':\n",
    "    #                         continue\n",
    "    #                     word_tag_pairs = sentence.split()\n",
    "    #                     for pair in word_tag_pairs:\n",
    "    #                         word, tag = pair.rsplit('/', maxsplit=1)\n",
    "    #                         tag_list.append(tag)\n",
    "    #                         word_list.append(word)\n",
    "\n",
    "    #                     #  2) create the sentence ID, add it to sentence_ids\n",
    "    #                     sentence_id = name + str(index)\n",
    "    #                     sentence_ids.append(sentence_id)\n",
    "                        \n",
    "                        \n",
    "    #                     #  3) add this sentence's tag list to tag_lists and word list to word_lists\n",
    "    #                     tag_lists[sentence_id] = tag_list\n",
    "    #                     word_lists[sentence_id] = word_list\n",
    "    #                     sentences[sentence_id] = sentence\n",
    "                        \n",
    "    #                     index += 1\n",
    "    #                 # END STUDENT CODE\n",
    "\n",
    "    #     return sentence_ids, sentences, tag_lists, word_lists\n",
    "\n",
    "    def load_data(self, data_set):\n",
    "        sentence_ids = []  # doc name + ordinal number of sentence (e.g., ca010)\n",
    "        sentences = dict()\n",
    "        tag_lists = dict()\n",
    "        word_lists = dict()\n",
    "        # iterate over documents\n",
    "        for root, dirs, files in os.walk(data_set):\n",
    "            for name in files:\n",
    "                with open(os.path.join(root, name)) as f:\n",
    "                    # be sure to split documents into sentences here\n",
    "                    # BEGIN STUDENT CODE\n",
    "                    # for each sentence in the document\n",
    "                    index = 0\n",
    "                    for sentence in f:  # each line is a sentence\n",
    "                        # 1) create a list of tags and list of words that appear in this sentence\n",
    "                        tag_list = []\n",
    "                        word_list = []\n",
    "                        sentence = sentence.strip()\n",
    "                        if sentence == '':\n",
    "                            continue\n",
    "                        word_tag_pairs = sentence.split()\n",
    "                        for pair in word_tag_pairs:\n",
    "                            word, tag = pair.rsplit('/', maxsplit=1)\n",
    "                            tag_idx = self.tag_dict.get(tag, -1)\n",
    "                            word_idx = self.word_dict.get(word, -1)\n",
    "                            tag_list.append(tag_idx)\n",
    "                            word_list.append(word_idx)\n",
    "\n",
    "                        # 2) create the sentence ID, add it to sentence_ids\n",
    "                        sentence_id = name + str(index)\n",
    "                        sentence_ids.append(sentence_id)\n",
    "\n",
    "                        # 3) add this sentence's tag list to tag_lists and word list to word_lists\n",
    "                        tag_lists[sentence_id] = tag_list\n",
    "                        word_lists[sentence_id] = word_list\n",
    "                        sentences[sentence_id] = sentence\n",
    "\n",
    "                        index += 1\n",
    "                    # END STUDENT CODE\n",
    "\n",
    "        return sentence_ids, sentences, tag_lists, word_lists\n",
    "\n",
    "\n",
    "    '''\n",
    "    Implements the Viterbi algorithm.\n",
    "    Use v and backpointer to find the best_path.\n",
    "    '''\n",
    "    def viterbi(self, sentence):\n",
    "        T = len(sentence)\n",
    "        N = len(self.tag_dict)\n",
    "        v = np.zeros((N, T))\n",
    "        backpointer = np.zeros((N, T), dtype=int)\n",
    "        best_path = []\n",
    "        # BEGIN STUDENT CODE\n",
    "        \n",
    "        # initialization step\n",
    "        for s in range(N):\n",
    "            word_index = self.word_dict.get(sentence[0], self.unk_index)\n",
    "            v[s, 0] = self.initial[s] + self.emission[word_index, s]\n",
    "            \n",
    "        # recursion step\n",
    "        for t in range(1, T):\n",
    "            word_index = self.word_dict.get(sentence[t], self.unk_index)\n",
    "            for s in range(N):\n",
    "                # 1) fill out the t-th column of viterbi trellis with the max of the t-1-th column of trellis\n",
    "                #      + transition weights to each state\n",
    "                #      + emission weights of t-th observateion\n",
    "                trans_probs = v[:, t-1] + self.transition[:, s]\n",
    "                max_trans_prob = np.max(trans_probs)\n",
    "                v[s, t] = max_trans_prob + self.emission[word_index, s]\n",
    "                \n",
    "                #  2) fill out the t-th column of the backpointer trellis with the associated argmax values\n",
    "                backpointer[s, t] = np.argmax(trans_probs)\n",
    "                \n",
    "        # termination step\n",
    "        #  1) get the most likely ending state, insert it into best_path\n",
    "        best_path.append(np.argmax(v[:, T-1]))\n",
    "        \n",
    "        #  2) fill out best_path from backpointer trellis\n",
    "        for t in range(T-1, 0, -1):\n",
    "            best_path.insert(0, backpointer[best_path[0], t])\n",
    "        \n",
    "        # END STUDENT CODE\n",
    "        return best_path\n",
    "\n",
    "\n",
    "    '''\n",
    "    Trains a structured perceptron part-of-speech tagger on a training set.\n",
    "    '''\n",
    "    def train(self, train_set, dummy_data=None):\n",
    "        self.make_dicts(train_set)\n",
    "        sentence_ids, sentences, tag_lists, word_lists = self.load_data(train_set)\n",
    "        if dummy_data is None: # for automated testing: DO NOT CHANGE!!\n",
    "            Random(0).shuffle(sentence_ids)\n",
    "            self.initial = np.zeros(len(self.tag_dict))\n",
    "            self.transition = np.zeros((len(self.tag_dict), len(self.tag_dict)))\n",
    "            self.emission = np.zeros((len(self.word_dict), len(self.tag_dict)))\n",
    "        else:\n",
    "            sentence_ids = dummy_data[0]\n",
    "            sentences = dummy_data[1]\n",
    "            tag_lists = dummy_data[2]\n",
    "            word_lists = dummy_data[3]\n",
    "        for i, sentence_id in enumerate(sentence_ids):\n",
    "            # BEGIN STUDENT CODE\n",
    "            # get the word sequence for this sentence and the correct tag sequence\n",
    "            sentence = word_lists[sentence_id]\n",
    "            correct_tags = tag_lists[sentence_id]\n",
    "\n",
    "            # use viterbi to predict\n",
    "            predicted_tags = self.viterbi(sentence)\n",
    "            # if mistake\n",
    "            if predicted_tags != correct_tags:\n",
    "                for t, word in enumerate(sentence):\n",
    "                    word_idx = word\n",
    "\n",
    "                    # promote weights that appear in correct sequence\n",
    "                    self.initial[correct_tags[t]] += 1  # just add one to the weight\n",
    "                    self.emission[word_idx, correct_tags[t]] += 1 \n",
    "\n",
    "                    if t > 0:  # if not the first word\n",
    "                        self.transition[correct_tags[t-1], correct_tags[t]] += 1  # add one to the weight\n",
    "\n",
    "                    # demote weights that appear in (incorrect) predicted sequence\n",
    "                    self.initial[predicted_tags[t]] -= 1  # subtract one from the weight\n",
    "                    self.emission[word_idx, predicted_tags[t]] -= 1\n",
    "\n",
    "                    if t > 0:  # if not the first word\n",
    "                        self.transition[predicted_tags[t-1], predicted_tags[t]] -= 1\n",
    "\n",
    "            # END STUDENT CODE\n",
    "            if (i + 1) % 1000 == 0 or i + 1 == len(sentence_ids):\n",
    "                print(i + 1, 'training sentences tagged')\n",
    "\n",
    "\n",
    "            # get the word sequence for this sentence and the correct tag sequence\n",
    "            words = word_lists[sentence_id]\n",
    "            correct_tags = tag_lists[sentence_id]\n",
    "            \n",
    "            # use viterbi to predict\n",
    "            predicted_tags = self.viterbi(words)\n",
    "            \n",
    "            # if mistake\n",
    "            if predicted_tags != correct_tags:\n",
    "                for t, word in enumerate(words):\n",
    "                    word_idx = word\n",
    "                    \n",
    "                    # promote weights that appear in correct sequence\n",
    "                    self.initial[correct_tags[t]] += 1\n",
    "                    self.emission[word_idx, correct_tags[t]] += 1\n",
    "                    if t > 0:\n",
    "                        self.transition[correct_tags[t-1], correct_tags[t]] += 1\n",
    "                        \n",
    "                    # demote weights that appear in (incorrect) predicted sequence\n",
    "                    self.initial[predicted_tags[t]] -= 1\n",
    "                    self.emission[word_idx, predicted_tags[t]] -= 1\n",
    "                    if t > 0:\n",
    "                        self.transition[predicted_tags[t-1], predicted_tags[t]] -= 1\n",
    "\n",
    "\n",
    "    '''\n",
    "    Tests the tagger on a development or test set.\n",
    "    Returns a dictionary of sentence_ids mapped to their correct and predicted\n",
    "    sequences of part-of-speech tags such that:\n",
    "    results[sentence_id]['correct'] = correct sequence of tags\n",
    "    results[sentence_id]['predicted'] = predicted sequence of tags\n",
    "    '''\n",
    "    def test(self, dev_set, dummy_data=None):\n",
    "        results = defaultdict(dict)\n",
    "        sentence_ids, sentences, tag_lists, word_lists = self.load_data(dev_set)\n",
    "        if dummy_data is not None: # for automated testing: DO NOT CHANGE!!\n",
    "            sentence_ids = dummy_data[0]\n",
    "            sentences = dummy_data[1]\n",
    "            tag_lists = dummy_data[2]\n",
    "            word_lists = dummy_data[3]\n",
    "        for i, sentence_id in enumerate(sentence_ids):\n",
    "            # BEGIN STUDENT CODE\n",
    "            # should be very similar to train function before mistake check\n",
    "            # get the word sequence for this sentence and the correct tag sequence\n",
    "            sentence = word_lists[sentence_id]\n",
    "            correct_tags = tag_lists[sentence_id]\n",
    "            # use viterbi to predict\n",
    "            predicted_tags = self.viterbi(sentence)\n",
    "            results[sentence_id]['correct'] = correct_tags  # makes a dictionary of dictionaries, for example: results['ca01']['correct'] = ['nn', 'vb', 'dt']\n",
    "            results[sentence_id]['predicted'] = predicted_tags\n",
    "            # END STUDENT CODE\n",
    "            if (i + 1) % 1000 == 0 or i + 1 == len(sentence_ids):\n",
    "                print(i + 1, 'testing sentences tagged')\n",
    "        return sentences, results\n",
    "\n",
    "    '''\n",
    "    Given results, calculates overall accuracy.\n",
    "    This evaluate function calculates accuracy ONLY,\n",
    "    no precision or recall calculations are required.\n",
    "    '''\n",
    "    def evaluate(self, sentences, results, dummy_data=False):\n",
    "        if not dummy_data:\n",
    "            self.sample_results(sentences, results)\n",
    "        accuracy = 0.0\n",
    "        # BEGIN STUDENT CODE\n",
    "        # for each sentence, how many words were correctly tagged out of the total words in that sentence?'\n",
    "        # number of words correctly tagged / total number of words)\n",
    "        # sum up the number of words correctly tagged for all sentences\n",
    "        # divide by the total number of words in all sentences\n",
    "        total_words = 0\n",
    "        total_correct = 0\n",
    "        for sentence_id in results:\n",
    "            correct_tags = results[sentence_id]['correct']\n",
    "            predicted_tags = results[sentence_id]['predicted']\n",
    "            total_words += len(correct_tags)\n",
    "            for i in range(len(correct_tags)):\n",
    "                if correct_tags[i] == predicted_tags[i]:\n",
    "                    total_correct += 1\n",
    "        accuracy = total_correct / total_words\n",
    "        # END STUDENT CODE\n",
    "        return accuracy\n",
    "        \n",
    "    '''\n",
    "    Prints out some sample results, with original sentence,\n",
    "    correct tag sequence, and predicted tag sequence.\n",
    "    This is just to view some results in an interpretable format.\n",
    "    You do not need to do anything in this function.\n",
    "    '''\n",
    "    def sample_results(self, sentences, results, size=2):\n",
    "        print('\\nSample results')\n",
    "        results_sample = [random.choice(list(results)) for i in range(size)]\n",
    "        inv_tag_dict = {v: k for k, v in self.tag_dict.items()}\n",
    "        for sentence_id in results_sample:\n",
    "            length = len(results[sentence_id]['correct'])\n",
    "            correct_tags = [inv_tag_dict[results[sentence_id]['correct'][i]] for i in range(length)]\n",
    "            predicted_tags = [inv_tag_dict[results[sentence_id]['predicted'][i]] for i in range(length)]\n",
    "            print(sentence_id,\\\n",
    "                sentences[sentence_id],\\\n",
    "                'Correct:\\t',correct_tags,\\\n",
    "                '\\n Predicted:\\t',predicted_tags,'\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3 training sentences tagged\n",
      "2 testing sentences tagged\n",
      "\n",
      "Sample results\n",
      "ca050 Goldilocks/nn cheered/vb Correct:\t ['nn', 'vb'] \n",
      " Predicted:\t ['nn', 'vb'] \n",
      "\n",
      "ca050 Goldilocks/nn cheered/vb Correct:\t ['nn', 'vb'] \n",
      " Predicted:\t ['nn', 'vb'] \n",
      "\n",
      "\n",
      "Accuracy: 1.0\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    pos = POSTagger()\n",
    "    # make sure these point to the right directories\n",
    "    pos.train('data_small/train') # train: toy data\n",
    "    #pos.train('brown_news/train') # train: news data only\n",
    "    #pos.train('brown/train') # train: full data\n",
    "    sentences, results = pos.test('data_small/test') # test: toy data\n",
    "    #sentences, results = pos.test('brown_news/dev') # test: news data only\n",
    "    #sentences, results = pos.test('brown/dev') # test: full data\n",
    "    print('\\nAccuracy:', pos.evaluate(sentences, results))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "machinelearning_20220719",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
