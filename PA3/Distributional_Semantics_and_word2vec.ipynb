{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "11e5a081",
   "metadata": {},
   "source": [
    "# Distributional Semantics and word2vec\n",
    "\n",
    "Partially adapted from Graham Neubig (CMU)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe632ad0",
   "metadata": {},
   "source": [
    "Previously, we explored neural nets for NLP on the sentiment analysis task.  They have a lot of advantages but a lot of disadvantages.\n",
    "\n",
    "It was difficult to do much better than logistic regression using a neural net.\n",
    "\n",
    "One issue was that we were just representing words as indices in a vocabulary and a document was just a vector of numbers representing each word's location in the vocabulary.  This is called the \"Bag of Words\" (BOW) approach."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82e4ea10",
   "metadata": {},
   "source": [
    "We will be using the DyNet package by CMU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ba95c897",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: dynet in /s/parsons/a/fac/nkrishna/anaconda3/lib/python3.9/site-packages (2.1.2)\n",
      "Requirement already satisfied: numpy in /s/parsons/a/fac/nkrishna/anaconda3/lib/python3.9/site-packages (from dynet) (1.20.3)\n",
      "Requirement already satisfied: cython in /s/parsons/a/fac/nkrishna/anaconda3/lib/python3.9/site-packages (from dynet) (0.29.24)\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "!{sys.executable} -m pip install dynet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1e46606c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[dynet] random seed: 224520371\n",
      "[dynet] allocating memory: 512MB\n",
      "[dynet] memory allocation done.\n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "import time\n",
    "import random\n",
    "import dynet as dy\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "366a0149",
   "metadata": {},
   "source": [
    "Create functions to read in the corpus.\n",
    "\n",
    "This turns all words and tags into indices.\n",
    "\n",
    "Our data is movie reviews again, but a different dataset.  This data consists of a line from a movie review and then the rating of that movie (0-4 stars).\n",
    "\n",
    "This makes it a multinomial task, one well-suited in principle to a neural net with softmax layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "31e9f0f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Functions to read in the corpus\n",
    "w2i = defaultdict(lambda: len(w2i))\n",
    "t2i = defaultdict(lambda: len(t2i))\n",
    "UNK = w2i[\"<unk>\"] # create an index for the unknown token\n",
    "def read_dataset(filename):\n",
    "    with open(filename, \"r\") as f:\n",
    "        for line in f:\n",
    "            tag, words = line.lower().strip().split(\" ||| \")\n",
    "            yield ([w2i[x] for x in words.split(\" \")], t2i[tag])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c364e008",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in the data\n",
    "train = list(read_dataset(\"data/classes/train.txt\"))\n",
    "w2i = defaultdict(lambda: UNK, w2i) # put unknown index on the front\n",
    "dev = list(read_dataset(\"data/classes/test.txt\"))\n",
    "nwords = len(w2i)\n",
    "ntags = len(t2i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "86444c28",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18648"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nwords"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aefc8228",
   "metadata": {},
   "source": [
    "This has fewer words than the 45K word vocabulary from last week.  The data's also been curated so that the lines selected from each review communicate the essential thrust of the entire review.\n",
    "\n",
    "This should make for an easier ML task."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0fbca9a",
   "metadata": {},
   "source": [
    "**Possible content warning**: We are going to be examining the data by sampling randomly and I do not know every single word that may occur in this dataset.  Some words may be troubling or offensive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0c8fe758",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(18648,\n",
       " defaultdict(<function __main__.<lambda>()>,\n",
       "             {'<unk>': 0,\n",
       "              'the': 1,\n",
       "              'rock': 2,\n",
       "              'is': 3,\n",
       "              'destined': 4,\n",
       "              'to': 5,\n",
       "              'be': 6,\n",
       "              '21st': 7,\n",
       "              'century': 8,\n",
       "              \"'s\": 9,\n",
       "              'new': 10,\n",
       "              '``': 11,\n",
       "              'conan': 12,\n",
       "              \"''\": 13,\n",
       "              'and': 14,\n",
       "              'that': 15,\n",
       "              'he': 16,\n",
       "              'going': 17,\n",
       "              'make': 18,\n",
       "              'a': 19,\n",
       "              'splash': 20,\n",
       "              'even': 21,\n",
       "              'greater': 22,\n",
       "              'than': 23,\n",
       "              'arnold': 24,\n",
       "              'schwarzenegger': 25,\n",
       "              ',': 26,\n",
       "              'jean-claud': 27,\n",
       "              'van': 28,\n",
       "              'damme': 29,\n",
       "              'or': 30,\n",
       "              'steven': 31,\n",
       "              'segal': 32,\n",
       "              '.': 33,\n",
       "              'gorgeously': 34,\n",
       "              'elaborate': 35,\n",
       "              'continuation': 36,\n",
       "              'of': 37,\n",
       "              'lord': 38,\n",
       "              'rings': 39,\n",
       "              'trilogy': 40,\n",
       "              'so': 41,\n",
       "              'huge': 42,\n",
       "              'column': 43,\n",
       "              'words': 44,\n",
       "              'can': 45,\n",
       "              'not': 46,\n",
       "              'adequately': 47,\n",
       "              'describe': 48,\n",
       "              'co-writer\\\\/director': 49,\n",
       "              'peter': 50,\n",
       "              'jackson': 51,\n",
       "              'expanded': 52,\n",
       "              'vision': 53,\n",
       "              'j.r.r.': 54,\n",
       "              'tolkien': 55,\n",
       "              'middle-earth': 56,\n",
       "              'singer\\\\/composer': 57,\n",
       "              'bryan': 58,\n",
       "              'adams': 59,\n",
       "              'contributes': 60,\n",
       "              'slew': 61,\n",
       "              'songs': 62,\n",
       "              '--': 63,\n",
       "              'few': 64,\n",
       "              'potential': 65,\n",
       "              'hits': 66,\n",
       "              'more': 67,\n",
       "              'simply': 68,\n",
       "              'intrusive': 69,\n",
       "              'story': 70,\n",
       "              'but': 71,\n",
       "              'whole': 72,\n",
       "              'package': 73,\n",
       "              'certainly': 74,\n",
       "              'captures': 75,\n",
       "              'intended': 76,\n",
       "              'er': 77,\n",
       "              'spirit': 78,\n",
       "              'piece': 79,\n",
       "              'you': 80,\n",
       "              \"'d\": 81,\n",
       "              'think': 82,\n",
       "              'by': 83,\n",
       "              'now': 84,\n",
       "              'america': 85,\n",
       "              'would': 86,\n",
       "              'have': 87,\n",
       "              'had': 88,\n",
       "              'enough': 89,\n",
       "              'plucky': 90,\n",
       "              'british': 91,\n",
       "              'eccentrics': 92,\n",
       "              'with': 93,\n",
       "              'hearts': 94,\n",
       "              'gold': 95,\n",
       "              'yet': 96,\n",
       "              'act': 97,\n",
       "              'still': 98,\n",
       "              'charming': 99,\n",
       "              'here': 100,\n",
       "              'whether': 101,\n",
       "              \"'re\": 102,\n",
       "              'enlightened': 103,\n",
       "              'any': 104,\n",
       "              'derrida': 105,\n",
       "              'lectures': 106,\n",
       "              'on': 107,\n",
       "              'other': 108,\n",
       "              'self': 109,\n",
       "              'an': 110,\n",
       "              'undeniably': 111,\n",
       "              'fascinating': 112,\n",
       "              'playful': 113,\n",
       "              'fellow': 114,\n",
       "              'just': 115,\n",
       "              'labour': 116,\n",
       "              'involved': 117,\n",
       "              'in': 118,\n",
       "              'creating': 119,\n",
       "              'layered': 120,\n",
       "              'richness': 121,\n",
       "              'imagery': 122,\n",
       "              'this': 123,\n",
       "              'chiaroscuro': 124,\n",
       "              'madness': 125,\n",
       "              'light': 126,\n",
       "              'astonishing': 127,\n",
       "              'part': 128,\n",
       "              'charm': 129,\n",
       "              'satin': 130,\n",
       "              'rouge': 131,\n",
       "              'it': 132,\n",
       "              'avoids': 133,\n",
       "              'obvious': 134,\n",
       "              'humour': 135,\n",
       "              'lightness': 136,\n",
       "              'screenplay': 137,\n",
       "              'ingeniously': 138,\n",
       "              'constructed': 139,\n",
       "              'memento': 140,\n",
       "              'extreme': 141,\n",
       "              'ops': 142,\n",
       "              'exceeds': 143,\n",
       "              'expectations': 144,\n",
       "              'good': 145,\n",
       "              'fun': 146,\n",
       "              'action': 147,\n",
       "              'acting': 148,\n",
       "              'dialogue': 149,\n",
       "              'pace': 150,\n",
       "              'cinematography': 151,\n",
       "              'should': 152,\n",
       "              'pay': 153,\n",
       "              'nine': 154,\n",
       "              'bucks': 155,\n",
       "              'for': 156,\n",
       "              ':': 157,\n",
       "              'because': 158,\n",
       "              'hear': 159,\n",
       "              'about': 160,\n",
       "              'suffering': 161,\n",
       "              'afghan': 162,\n",
       "              'refugees': 163,\n",
       "              'news': 164,\n",
       "              'unaffected': 165,\n",
       "              'dramas': 166,\n",
       "              'like': 167,\n",
       "              'human': 168,\n",
       "              'thunderous': 169,\n",
       "              'ride': 170,\n",
       "              'at': 171,\n",
       "              'first': 172,\n",
       "              'quiet': 173,\n",
       "              'cadences': 174,\n",
       "              'pure': 175,\n",
       "              'finesse': 176,\n",
       "              'are': 177,\n",
       "              'far': 178,\n",
       "              'between': 179,\n",
       "              ';': 180,\n",
       "              'their': 181,\n",
       "              'shortage': 182,\n",
       "              'dilutes': 183,\n",
       "              'potency': 184,\n",
       "              'otherwise': 185,\n",
       "              'respectable': 186,\n",
       "              'flick': 187,\n",
       "              'host': 188,\n",
       "              'some': 189,\n",
       "              'truly': 190,\n",
       "              'excellent': 191,\n",
       "              'sequences': 192,\n",
       "              'australian': 193,\n",
       "              'actor\\\\/director': 194,\n",
       "              'john': 195,\n",
       "              'polson': 196,\n",
       "              'award-winning': 197,\n",
       "              'english': 198,\n",
       "              'cinematographer': 199,\n",
       "              'giles': 200,\n",
       "              'nuttgens': 201,\n",
       "              'terrific': 202,\n",
       "              'effort': 203,\n",
       "              'disguising': 204,\n",
       "              'energy': 205,\n",
       "              'innovation': 206,\n",
       "              'walk': 207,\n",
       "              'out': 208,\n",
       "              'girl': 209,\n",
       "              'mixed': 210,\n",
       "              'emotions': 211,\n",
       "              'disapproval': 212,\n",
       "              'justine': 213,\n",
       "              'combined': 214,\n",
       "              'tinge': 215,\n",
       "              'understanding': 216,\n",
       "              'her': 217,\n",
       "              'actions': 218,\n",
       "              'post': 219,\n",
       "              '9\\\\/11': 220,\n",
       "              'philosophical': 221,\n",
       "              'message': 222,\n",
       "              'personal': 223,\n",
       "              'freedom': 224,\n",
       "              'might': 225,\n",
       "              'as': 226,\n",
       "              'palatable': 227,\n",
       "              'absorbing': 228,\n",
       "              'character': 229,\n",
       "              'study': 230,\n",
       "              'andré': 231,\n",
       "              'turpin': 232,\n",
       "              'if': 233,\n",
       "              'love': 234,\n",
       "              'reading': 235,\n",
       "              'and\\\\/or': 236,\n",
       "              'poetry': 237,\n",
       "              'then': 238,\n",
       "              'all': 239,\n",
       "              'means': 240,\n",
       "              'check': 241,\n",
       "              \"'ll\": 242,\n",
       "              'probably': 243,\n",
       "              'frailty': 244,\n",
       "              'has': 245,\n",
       "              'been': 246,\n",
       "              'written': 247,\n",
       "              'well': 248,\n",
       "              'simple': 249,\n",
       "              'goddammit': 250,\n",
       "              '!': 251,\n",
       "              'near': 252,\n",
       "              'end': 253,\n",
       "              'takes': 254,\n",
       "              'meaning': 255,\n",
       "              'grenier': 256,\n",
       "              'bringing': 257,\n",
       "              'unforced': 258,\n",
       "              'rapid-fire': 259,\n",
       "              'delivery': 260,\n",
       "              'toback': 261,\n",
       "              'heidegger': 262,\n",
       "              '-': 263,\n",
       "              'nietzsche-referencing': 264,\n",
       "              'sundance': 265,\n",
       "              'film': 266,\n",
       "              'festival': 267,\n",
       "              'become': 268,\n",
       "              'buzz-obsessed': 269,\n",
       "              'fans': 270,\n",
       "              'producers': 271,\n",
       "              'descend': 272,\n",
       "              'upon': 273,\n",
       "              'utah': 274,\n",
       "              'each': 275,\n",
       "              'january': 276,\n",
       "              'ferret': 277,\n",
       "              'next': 278,\n",
       "              'great': 279,\n",
       "              'thing': 280,\n",
       "              '`': 281,\n",
       "              'tadpole': 282,\n",
       "              \"'\": 283,\n",
       "              'was': 284,\n",
       "              'one': 285,\n",
       "              'films': 286,\n",
       "              'declared': 287,\n",
       "              'year': 288,\n",
       "              'really': 289,\n",
       "              'pretty': 290,\n",
       "              'actors': 291,\n",
       "              'fantastic': 292,\n",
       "              'they': 293,\n",
       "              'what': 294,\n",
       "              'makes': 295,\n",
       "              'worth': 296,\n",
       "              'trip': 297,\n",
       "              'theatre': 298,\n",
       "              '-lrb-': 299,\n",
       "              'taymor': 300,\n",
       "              '-rrb-': 301,\n",
       "              'utilizes': 302,\n",
       "              'idea': 303,\n",
       "              'making': 304,\n",
       "              'kahlo': 305,\n",
       "              'art': 306,\n",
       "              'living': 307,\n",
       "              'breathing': 308,\n",
       "              'movie': 309,\n",
       "              'often': 310,\n",
       "              'catapulting': 311,\n",
       "              'artist': 312,\n",
       "              'into': 313,\n",
       "              'own': 314,\n",
       "              'work': 315,\n",
       "              \"n't\": 316,\n",
       "              'done': 317,\n",
       "              'before': 318,\n",
       "              'never': 319,\n",
       "              'vividly': 320,\n",
       "              'much': 321,\n",
       "              'passion': 322,\n",
       "              'take': 323,\n",
       "              'care': 324,\n",
       "              'my': 325,\n",
       "              'cat': 326,\n",
       "              'honestly': 327,\n",
       "              'nice': 328,\n",
       "              'little': 329,\n",
       "              'us': 330,\n",
       "              'examination': 331,\n",
       "              'young': 332,\n",
       "              'adult': 333,\n",
       "              'life': 334,\n",
       "              'urban': 335,\n",
       "              'south': 336,\n",
       "              'korea': 337,\n",
       "              'through': 338,\n",
       "              'minds': 339,\n",
       "              'five': 340,\n",
       "              'principals': 341,\n",
       "              'life-affirming': 342,\n",
       "              'its': 343,\n",
       "              'vulgar': 344,\n",
       "              'mean': 345,\n",
       "              'i': 346,\n",
       "              'liked': 347,\n",
       "              'empire': 348,\n",
       "              'lacks': 349,\n",
       "              'depth': 350,\n",
       "              'up': 351,\n",
       "              'heart': 352,\n",
       "              'silly': 353,\n",
       "              'photographed': 354,\n",
       "              'colour': 355,\n",
       "              'rather': 356,\n",
       "              'time': 357,\n",
       "              'tongue-in-cheek': 358,\n",
       "              'preposterousness': 359,\n",
       "              'always': 360,\n",
       "              'most': 361,\n",
       "              'wilde': 362,\n",
       "              'droll': 363,\n",
       "              'whimsy': 364,\n",
       "              'helps': 365,\n",
       "              'being': 366,\n",
       "              'earnest': 367,\n",
       "              'overcome': 368,\n",
       "              'weaknesses': 369,\n",
       "              'parker': 370,\n",
       "              'creative': 371,\n",
       "              'interference': 372,\n",
       "              '...': 373,\n",
       "              'lies': 374,\n",
       "              'utter': 375,\n",
       "              'cuteness': 376,\n",
       "              'stuart': 377,\n",
       "              'margolo': 378,\n",
       "              'computer-animated': 379,\n",
       "              'faces': 380,\n",
       "              'very': 381,\n",
       "              'expressive': 382,\n",
       "              'path': 383,\n",
       "              'ice': 384,\n",
       "              'age': 385,\n",
       "              'follows': 386,\n",
       "              'closely': 387,\n",
       "              'though': 388,\n",
       "              'established': 389,\n",
       "              'warner': 390,\n",
       "              'bros.': 391,\n",
       "              'giant': 392,\n",
       "              'chuck': 393,\n",
       "              'jones': 394,\n",
       "              'who': 395,\n",
       "              'died': 396,\n",
       "              'matter': 397,\n",
       "              'weeks': 398,\n",
       "              'release': 399,\n",
       "              'spiced': 400,\n",
       "              'humor': 401,\n",
       "              'speak': 402,\n",
       "              'fluent': 403,\n",
       "              'flatula': 404,\n",
       "              'advises': 405,\n",
       "              'denlopp': 406,\n",
       "              'after': 407,\n",
       "              'bubbly': 408,\n",
       "              'exchange': 409,\n",
       "              'alien': 410,\n",
       "              'deckhand': 411,\n",
       "              'witty': 412,\n",
       "              'updatings': 413,\n",
       "              'silver': 414,\n",
       "              'parrot': 415,\n",
       "              'replaced': 416,\n",
       "              'morph': 417,\n",
       "              'cute': 418,\n",
       "              'creature': 419,\n",
       "              'mimics': 420,\n",
       "              'everyone': 421,\n",
       "              'everything': 422,\n",
       "              'around': 423,\n",
       "              'there': 424,\n",
       "              'y': 425,\n",
       "              'tu': 426,\n",
       "              'mamá': 427,\n",
       "              'también': 428,\n",
       "              'comes': 429,\n",
       "              'from': 430,\n",
       "              'brave': 431,\n",
       "              'uninhibited': 432,\n",
       "              'performances': 433,\n",
       "              'lead': 434,\n",
       "              '13': 435,\n",
       "              'conversations': 436,\n",
       "              'holds': 437,\n",
       "              'goodwill': 438,\n",
       "              'close': 439,\n",
       "              'relatively': 440,\n",
       "              'slow': 441,\n",
       "              'come': 442,\n",
       "              'point': 443,\n",
       "              'auto': 444,\n",
       "              'focus': 445,\n",
       "              'works': 446,\n",
       "              'unusual': 447,\n",
       "              'biopic': 448,\n",
       "              'document': 449,\n",
       "              'male': 450,\n",
       "              'swingers': 451,\n",
       "              'playboy': 452,\n",
       "              'era': 453,\n",
       "              'mr.': 454,\n",
       "              'zhang': 455,\n",
       "              'subject': 456,\n",
       "              'degree': 457,\n",
       "              'least': 458,\n",
       "              'quintessentially': 459,\n",
       "              'american': 460,\n",
       "              'his': 461,\n",
       "              'approach': 462,\n",
       "              'storytelling': 463,\n",
       "              'called': 464,\n",
       "              'iranian': 465,\n",
       "              'sour': 466,\n",
       "              'core': 467,\n",
       "              'exploration': 468,\n",
       "              'emptiness': 469,\n",
       "              'underlay': 470,\n",
       "              'relentless': 471,\n",
       "              'gaiety': 472,\n",
       "              '1920': 473,\n",
       "              'ending': 474,\n",
       "              '?': 475,\n",
       "              'feeling': 476,\n",
       "              'deal': 477,\n",
       "              'cremaster': 478,\n",
       "              '3': 479,\n",
       "              'warning': 480,\n",
       "              'serious': 481,\n",
       "              'buffs': 482,\n",
       "              'only': 483,\n",
       "              'made': 484,\n",
       "              'me': 485,\n",
       "              'unintentionally': 486,\n",
       "              'famous': 487,\n",
       "              'queasy-stomached': 488,\n",
       "              'critic': 489,\n",
       "              'staggered': 490,\n",
       "              'theater': 491,\n",
       "              'blacked': 492,\n",
       "              'lobby': 493,\n",
       "              'believe': 494,\n",
       "              'beautiful': 495,\n",
       "              'evocative': 496,\n",
       "              \"'ve\": 497,\n",
       "              'seen': 498,\n",
       "              'garcía': 499,\n",
       "              'bernal': 500,\n",
       "              'talancón': 501,\n",
       "              'immensely': 502,\n",
       "              'appealing': 503,\n",
       "              'couple': 504,\n",
       "              'predictable': 505,\n",
       "              'want': 506,\n",
       "              'things': 507,\n",
       "              'spoof': 508,\n",
       "              'comedy': 509,\n",
       "              'carries': 510,\n",
       "              'share': 511,\n",
       "              'laughs': 512,\n",
       "              'sometimes': 513,\n",
       "              'chuckle': 514,\n",
       "              'guffaw': 515,\n",
       "              'pleasure': 516,\n",
       "              'occasional': 517,\n",
       "              'belly': 518,\n",
       "              'laugh': 519,\n",
       "              'city': 520,\n",
       "              'reminds': 521,\n",
       "              'how': 522,\n",
       "              'realistically': 523,\n",
       "              'nuanced': 524,\n",
       "              'robert': 525,\n",
       "              'de': 526,\n",
       "              'niro': 527,\n",
       "              'performance': 528,\n",
       "              'when': 529,\n",
       "              'lucratively': 530,\n",
       "              'engaged': 531,\n",
       "              'shameless': 532,\n",
       "              'self-caricature': 533,\n",
       "              'analyze': 534,\n",
       "              '1999': 535,\n",
       "              'promised': 536,\n",
       "              'threatened': 537,\n",
       "              'later': 538,\n",
       "              'wanton': 539,\n",
       "              'slipperiness': 540,\n",
       "              '\\\\*': 541,\n",
       "              'corpus': 542,\n",
       "              'amiable': 543,\n",
       "              'jerking': 544,\n",
       "              'reshaping': 545,\n",
       "              'physical': 546,\n",
       "              'space': 547,\n",
       "              'watch': 548,\n",
       "              'kids': 549,\n",
       "              'use': 550,\n",
       "              'introduce': 551,\n",
       "              'video': 552,\n",
       "              'starts': 553,\n",
       "              'typical': 554,\n",
       "              'bible': 555,\n",
       "              'killer': 556,\n",
       "              'turns': 557,\n",
       "              'significantly': 558,\n",
       "              'different': 559,\n",
       "              'better': 560,\n",
       "              'theme': 561,\n",
       "              'those': 562,\n",
       "              'pride': 563,\n",
       "              'themselves': 564,\n",
       "              'sophisticated': 565,\n",
       "              'discerning': 566,\n",
       "              'taste': 567,\n",
       "              'seem': 568,\n",
       "              'proper': 569,\n",
       "              'cup': 570,\n",
       "              'tea': 571,\n",
       "              'however': 572,\n",
       "              'almost': 573,\n",
       "              'guaranteed': 574,\n",
       "              'stuffiest': 575,\n",
       "              'cinema': 576,\n",
       "              'goers': 577,\n",
       "              'will': 578,\n",
       "              '\\\\*\\\\*\\\\*': 579,\n",
       "              'off': 580,\n",
       "              'hour-and-a-half': 581,\n",
       "              'cuts': 582,\n",
       "              'actually': 583,\n",
       "              'face': 584,\n",
       "              'your': 585,\n",
       "              'fears': 586,\n",
       "              'world': 587,\n",
       "              'boys': 588,\n",
       "              'boy': 589,\n",
       "              'big': 590,\n",
       "              'metaphorical': 591,\n",
       "              'wave': 592,\n",
       "              'wherever': 593,\n",
       "              'welcome': 594,\n",
       "              'relief': 595,\n",
       "              'baseball': 596,\n",
       "              'movies': 597,\n",
       "              'try': 598,\n",
       "              'too': 599,\n",
       "              'hard': 600,\n",
       "              'mythic': 601,\n",
       "              'sweet': 602,\n",
       "              'modest': 603,\n",
       "              'ultimately': 604,\n",
       "              'winning': 605,\n",
       "              'crisp': 606,\n",
       "              'psychological': 607,\n",
       "              'drama': 608,\n",
       "              'thriller': 609,\n",
       "              'perfect': 610,\n",
       "              'old': 611,\n",
       "              'twilight': 612,\n",
       "              'zone': 613,\n",
       "              'episode': 614,\n",
       "              'moments': 615,\n",
       "              'insightful': 616,\n",
       "              'fondly': 617,\n",
       "              'remembered': 618,\n",
       "              'endlessly': 619,\n",
       "              'challenging': 620,\n",
       "              'maze': 621,\n",
       "              'moviegoing': 622,\n",
       "              'opening': 623,\n",
       "              'contrived': 624,\n",
       "              'banter': 625,\n",
       "              'cliches': 626,\n",
       "              'loose': 627,\n",
       "              'ends': 628,\n",
       "              'second': 629,\n",
       "              'half': 630,\n",
       "              'uncluttered': 631,\n",
       "              'resonant': 632,\n",
       "              'gem': 633,\n",
       "              'relays': 634,\n",
       "              'universal': 635,\n",
       "              'points': 636,\n",
       "              'without': 637,\n",
       "              'confrontations': 638,\n",
       "              'cockettes': 639,\n",
       "              'provides': 640,\n",
       "              'window': 641,\n",
       "              'subculture': 642,\n",
       "              'hell-bent': 643,\n",
       "              'expressing': 644,\n",
       "              'itself': 645,\n",
       "              'every': 646,\n",
       "              'way': 647,\n",
       "              'imaginable': 648,\n",
       "              'smart': 649,\n",
       "              'steamy': 650,\n",
       "              'mix': 651,\n",
       "              'road': 652,\n",
       "              'coming-of-age': 653,\n",
       "              'political': 654,\n",
       "              'satire': 655,\n",
       "              'modern-day': 656,\n",
       "              'royals': 657,\n",
       "              'nothing': 658,\n",
       "              'these': 659,\n",
       "              'guys': 660,\n",
       "              'scandals': 661,\n",
       "              'fairy': 662,\n",
       "              'tales': 663,\n",
       "              'princesses': 664,\n",
       "              'married': 665,\n",
       "              'reason': 666,\n",
       "              'live': 667,\n",
       "              'happily': 668,\n",
       "              'ever': 669,\n",
       "              'b': 670,\n",
       "              'fact': 671,\n",
       "              'best': 672,\n",
       "              'recent': 673,\n",
       "              'memory': 674,\n",
       "              'birthday': 675,\n",
       "              'actor': 676,\n",
       "              'foremost': 677,\n",
       "              'walked': 678,\n",
       "              'away': 679,\n",
       "              'version': 680,\n",
       "              'e.t.': 681,\n",
       "              'hoped': 682,\n",
       "              'moist': 683,\n",
       "              'eyes': 684,\n",
       "              'devotees': 685,\n",
       "              'french': 686,\n",
       "              'safe': 687,\n",
       "              'conduct': 688,\n",
       "              'rich': 689,\n",
       "              'period': 690,\n",
       "              'minutiae': 691,\n",
       "              'dying': 692,\n",
       "              'celluloid': 693,\n",
       "              'heaven': 694,\n",
       "              'characters': 695,\n",
       "              'resemblance': 696,\n",
       "              'everyday': 697,\n",
       "              'children': 698,\n",
       "              'shamelessly': 699,\n",
       "              'resorting': 700,\n",
       "              'pee-related': 701,\n",
       "              'sight': 702,\n",
       "              'gags': 703,\n",
       "              'cause': 704,\n",
       "              'tom': 705,\n",
       "              'green': 706,\n",
       "              'grimace': 707,\n",
       "              'myer': 708,\n",
       "              'silliness': 709,\n",
       "              'eventually': 710,\n",
       "              'prevail': 711,\n",
       "              'absurdist': 712,\n",
       "              'spider': 713,\n",
       "              'web': 714,\n",
       "              'happy': 715,\n",
       "              'listening': 716,\n",
       "              'watching': 717,\n",
       "              'them': 718,\n",
       "              'parade': 719,\n",
       "              'fascinates': 720,\n",
       "              'right': 721,\n",
       "              'moves': 722,\n",
       "              'beyond': 723,\n",
       "              'original': 724,\n",
       "              'nostalgia': 725,\n",
       "              'communal': 726,\n",
       "              'experiences': 727,\n",
       "              'yesteryear': 728,\n",
       "              'deeper': 729,\n",
       "              'realization': 730,\n",
       "              'inability': 731,\n",
       "              'stand': 732,\n",
       "              'true': 733,\n",
       "              'lived': 734,\n",
       "              'experience': 735,\n",
       "              'blend': 736,\n",
       "              'together': 737,\n",
       "              'distant': 738,\n",
       "              'memories': 739,\n",
       "              'mention': 740,\n",
       "              'solaris': 741,\n",
       "              'years': 742,\n",
       "              \"'m\": 743,\n",
       "              'sure': 744,\n",
       "              'saw': 745,\n",
       "              'opinion': 746,\n",
       "              'allen': 747,\n",
       "              'funniest': 748,\n",
       "              'likeable': 749,\n",
       "              'glorious': 750,\n",
       "              'spectacle': 751,\n",
       "              'd.w.': 752,\n",
       "              'griffith': 753,\n",
       "              'early': 754,\n",
       "              'days': 755,\n",
       "              'silent': 756,\n",
       "              'comic': 757,\n",
       "              'delightful': 758,\n",
       "              'derivative': 759,\n",
       "              'timely': 760,\n",
       "              'director': 761,\n",
       "              'could': 762,\n",
       "              'dreamed': 763,\n",
       "              'quietly': 764,\n",
       "              'lyrical': 765,\n",
       "              'tale': 766,\n",
       "              'probes': 767,\n",
       "              'ambiguous': 768,\n",
       "              'extended': 769,\n",
       "              'iran': 770,\n",
       "              'afghani': 771,\n",
       "              'streamed': 772,\n",
       "              'across': 773,\n",
       "              'borders': 774,\n",
       "              'desperate': 775,\n",
       "              'food': 776,\n",
       "              'leaping': 777,\n",
       "              'line': 778,\n",
       "              'shaped': 779,\n",
       "              'kosminsky': 780,\n",
       "              'sharp': 781,\n",
       "              'slivers': 782,\n",
       "              'cutting': 783,\n",
       "              'impressions': 784,\n",
       "              'shows': 785,\n",
       "              'signs': 786,\n",
       "              'detail': 787,\n",
       "              'condensed': 788,\n",
       "              'images': 789,\n",
       "              'striking': 790,\n",
       "              'traits': 791,\n",
       "              'three': 792,\n",
       "              'principal': 793,\n",
       "              'singers': 794,\n",
       "              'youthful': 795,\n",
       "              'good-looking': 796,\n",
       "              'diva': 797,\n",
       "              'tenor': 798,\n",
       "              'richly': 799,\n",
       "              'handsome': 800,\n",
       "              'locations': 801,\n",
       "              'wish': 802,\n",
       "              'jacquot': 803,\n",
       "              'left': 804,\n",
       "              'alone': 805,\n",
       "              'filmed': 806,\n",
       "              'opera': 807,\n",
       "              'distortions': 808,\n",
       "              'perspective': 809,\n",
       "              'production': 810,\n",
       "              'enormous': 811,\n",
       "              'amount': 812,\n",
       "              'affection': 813,\n",
       "              'we': 814,\n",
       "              'worthwhile': 815,\n",
       "              'winds': 816,\n",
       "              'both': 817,\n",
       "              'revelatory': 818,\n",
       "              'narcissistic': 819,\n",
       "              'achieving': 820,\n",
       "              'honest': 821,\n",
       "              'insight': 822,\n",
       "              'relationships': 823,\n",
       "              'high-concept': 824,\n",
       "              'candy-coat': 825,\n",
       "              'pat': 826,\n",
       "              'storylines': 827,\n",
       "              'precious': 828,\n",
       "              'circumstances': 829,\n",
       "              'stars': 830,\n",
       "              'inspiring': 831,\n",
       "              'joy': 832,\n",
       "              'spielberg': 833,\n",
       "              'brings': 834,\n",
       "              'another': 835,\n",
       "              'masterpiece': 836,\n",
       "              'finally': 837,\n",
       "              'french-produced': 838,\n",
       "              'read': 839,\n",
       "              'lips': 840,\n",
       "              'understands': 841,\n",
       "              'must': 842,\n",
       "              'ms.': 843,\n",
       "              'seigner': 844,\n",
       "              'serrault': 845,\n",
       "              'bring': 846,\n",
       "              'fresh': 847,\n",
       "              'naturalism': 848,\n",
       "              'outgag': 849,\n",
       "              'whippersnappers': 850,\n",
       "              'moving': 851,\n",
       "              'pictures': 852,\n",
       "              'today': 853,\n",
       "              'solid': 854,\n",
       "              'pedigree': 855,\n",
       "              'front': 856,\n",
       "              'specifically': 857,\n",
       "              'behind': 858,\n",
       "              'camera': 859,\n",
       "              'no': 860,\n",
       "              'slam-dunk': 861,\n",
       "              'disappoint': 862,\n",
       "              'moved': 863,\n",
       "              'edge': 864,\n",
       "              'seats': 865,\n",
       "              'dynamic': 866,\n",
       "              'touching': 867,\n",
       "              'transcendent': 868,\n",
       "              'encourage': 869,\n",
       "              'alike': 870,\n",
       "              'go': 871,\n",
       "              'see': 872,\n",
       "              'unique': 873,\n",
       "              'entertaining': 874,\n",
       "              'twist': 875,\n",
       "              'classic': 876,\n",
       "              'whale': 877,\n",
       "              'wo': 878,\n",
       "              'sorry': 879,\n",
       "              'literary': 880,\n",
       "              'detective': 881,\n",
       "              'aficionados': 882,\n",
       "              'whodunit': 883,\n",
       "              'disappointed': 884,\n",
       "              'high': 885,\n",
       "              'crimes': 886,\n",
       "              'steals': 887,\n",
       "              'freely': 888,\n",
       "              'combines': 889,\n",
       "              'disparate': 890,\n",
       "              'types': 891,\n",
       "              'ca': 892,\n",
       "              'help': 893,\n",
       "              'engage': 894,\n",
       "              'audience': 895,\n",
       "              'fan': 896,\n",
       "              'series': 897,\n",
       "              'twice': 898,\n",
       "              'celebrates': 899,\n",
       "              'group': 900,\n",
       "              'spark': 901,\n",
       "              'nonconformity': 902,\n",
       "              'glancing': 903,\n",
       "              'back': 904,\n",
       "              'hibiscus': 905,\n",
       "              'grandly': 906,\n",
       "              'angels': 907,\n",
       "              'ironic': 908,\n",
       "              'ridiculous': 909,\n",
       "              'money-oriented': 910,\n",
       "              'record': 911,\n",
       "              'industry': 912,\n",
       "              'also': 913,\n",
       "              'testament': 914,\n",
       "              'integrity': 915,\n",
       "              'band': 916,\n",
       "              'laced': 917,\n",
       "              'liberal': 918,\n",
       "              'doses': 919,\n",
       "              'dark': 920,\n",
       "              'gorgeous': 921,\n",
       "              'exterior': 922,\n",
       "              'photography': 923,\n",
       "              'stable-full': 924,\n",
       "              'such': 925,\n",
       "              'huppert': 926,\n",
       "              'show': 927,\n",
       "              'steal': 928,\n",
       "              'she': 929,\n",
       "              'meal': 930,\n",
       "              'channeling': 931,\n",
       "              'kathy': 932,\n",
       "              'baker': 933,\n",
       "              'creepy': 934,\n",
       "              'turn': 935,\n",
       "              'repressed': 936,\n",
       "              'mother': 937,\n",
       "              'boston': 938,\n",
       "              'public': 939,\n",
       "              '8': 940,\n",
       "              'women': 941,\n",
       "              'augustine': 942,\n",
       "              'nair': 943,\n",
       "              'does': 944,\n",
       "              'treat': 945,\n",
       "              'issues': 946,\n",
       "              'lightly': 947,\n",
       "              'allows': 948,\n",
       "              'confront': 949,\n",
       "              'problems': 950,\n",
       "              'openly': 951,\n",
       "              'horror': 952,\n",
       "              'real': 953,\n",
       "              'shocks': 954,\n",
       "              'store': 955,\n",
       "              'unwary': 956,\n",
       "              'viewers': 957,\n",
       "              'filmmaker': 958,\n",
       "              'secrets': 959,\n",
       "              'buried': 960,\n",
       "              'knows': 961,\n",
       "              'revealing': 962,\n",
       "              'strange': 963,\n",
       "              'occurrences': 964,\n",
       "              'build': 965,\n",
       "              'mind': 966,\n",
       "              'viewer': 967,\n",
       "              'urgency': 968,\n",
       "              'certain': 969,\n",
       "              'ghoulish': 970,\n",
       "              'fascination': 971,\n",
       "              'generates': 972,\n",
       "              'fair': 973,\n",
       "              'b-movie': 974,\n",
       "              'excitement': 975,\n",
       "              'familiar': 976,\n",
       "              'utterly': 977,\n",
       "              'keeps': 978,\n",
       "              'hooked': 979,\n",
       "              'delicious': 980,\n",
       "              'pulpiness': 981,\n",
       "              'lurid': 982,\n",
       "              'fiction': 983,\n",
       "              'aims': 984,\n",
       "              'funny': 985,\n",
       "              'uplifting': 986,\n",
       "              'once': 987,\n",
       "              'extent': 988,\n",
       "              'which': 989,\n",
       "              'succeeds': 990,\n",
       "              'impressive': 991,\n",
       "              'brilliantly': 992,\n",
       "              'shines': 993,\n",
       "              'direction': 994,\n",
       "              'intelligently': 995,\n",
       "              'accomplished': 996,\n",
       "              'while': 997,\n",
       "              'collegiate': 998,\n",
       "              'gross-out': 999,\n",
       "              ...}))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(w2i),w2i # words to indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "77214c5c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(<function __main__.<lambda>()>,\n",
       "            {'3': 0, '4': 1, '2': 2, '1': 3, '0': 4})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t2i # tags to indices"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4101d14c",
   "metadata": {},
   "source": [
    "How is the data organized?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1e8c2ea0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([1,\n",
       "  2,\n",
       "  3,\n",
       "  4,\n",
       "  5,\n",
       "  6,\n",
       "  1,\n",
       "  7,\n",
       "  8,\n",
       "  9,\n",
       "  10,\n",
       "  11,\n",
       "  12,\n",
       "  13,\n",
       "  14,\n",
       "  15,\n",
       "  16,\n",
       "  9,\n",
       "  17,\n",
       "  5,\n",
       "  18,\n",
       "  19,\n",
       "  20,\n",
       "  21,\n",
       "  22,\n",
       "  23,\n",
       "  24,\n",
       "  25,\n",
       "  26,\n",
       "  27,\n",
       "  28,\n",
       "  29,\n",
       "  30,\n",
       "  31,\n",
       "  32,\n",
       "  33],\n",
       " 0)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3cb99d1",
   "metadata": {},
   "source": [
    "So an array of word indices followed by a tag index.\n",
    "\n",
    "Let's create some ways to decode the indices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ce0bac78",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: '<unk>',\n",
       " 1: 'the',\n",
       " 2: 'rock',\n",
       " 3: 'is',\n",
       " 4: 'destined',\n",
       " 5: 'to',\n",
       " 6: 'be',\n",
       " 7: '21st',\n",
       " 8: 'century',\n",
       " 9: \"'s\",\n",
       " 10: 'new',\n",
       " 11: '``',\n",
       " 12: 'conan',\n",
       " 13: \"''\",\n",
       " 14: 'and',\n",
       " 15: 'that',\n",
       " 16: 'he',\n",
       " 17: 'going',\n",
       " 18: 'make',\n",
       " 19: 'a',\n",
       " 20: 'splash',\n",
       " 21: 'even',\n",
       " 22: 'greater',\n",
       " 23: 'than',\n",
       " 24: 'arnold',\n",
       " 25: 'schwarzenegger',\n",
       " 26: ',',\n",
       " 27: 'jean-claud',\n",
       " 28: 'van',\n",
       " 29: 'damme',\n",
       " 30: 'or',\n",
       " 31: 'steven',\n",
       " 32: 'segal',\n",
       " 33: '.',\n",
       " 34: 'gorgeously',\n",
       " 35: 'elaborate',\n",
       " 36: 'continuation',\n",
       " 37: 'of',\n",
       " 38: 'lord',\n",
       " 39: 'rings',\n",
       " 40: 'trilogy',\n",
       " 41: 'so',\n",
       " 42: 'huge',\n",
       " 43: 'column',\n",
       " 44: 'words',\n",
       " 45: 'can',\n",
       " 46: 'not',\n",
       " 47: 'adequately',\n",
       " 48: 'describe',\n",
       " 49: 'co-writer\\\\/director',\n",
       " 50: 'peter',\n",
       " 51: 'jackson',\n",
       " 52: 'expanded',\n",
       " 53: 'vision',\n",
       " 54: 'j.r.r.',\n",
       " 55: 'tolkien',\n",
       " 56: 'middle-earth',\n",
       " 57: 'singer\\\\/composer',\n",
       " 58: 'bryan',\n",
       " 59: 'adams',\n",
       " 60: 'contributes',\n",
       " 61: 'slew',\n",
       " 62: 'songs',\n",
       " 63: '--',\n",
       " 64: 'few',\n",
       " 65: 'potential',\n",
       " 66: 'hits',\n",
       " 67: 'more',\n",
       " 68: 'simply',\n",
       " 69: 'intrusive',\n",
       " 70: 'story',\n",
       " 71: 'but',\n",
       " 72: 'whole',\n",
       " 73: 'package',\n",
       " 74: 'certainly',\n",
       " 75: 'captures',\n",
       " 76: 'intended',\n",
       " 77: 'er',\n",
       " 78: 'spirit',\n",
       " 79: 'piece',\n",
       " 80: 'you',\n",
       " 81: \"'d\",\n",
       " 82: 'think',\n",
       " 83: 'by',\n",
       " 84: 'now',\n",
       " 85: 'america',\n",
       " 86: 'would',\n",
       " 87: 'have',\n",
       " 88: 'had',\n",
       " 89: 'enough',\n",
       " 90: 'plucky',\n",
       " 91: 'british',\n",
       " 92: 'eccentrics',\n",
       " 93: 'with',\n",
       " 94: 'hearts',\n",
       " 95: 'gold',\n",
       " 96: 'yet',\n",
       " 97: 'act',\n",
       " 98: 'still',\n",
       " 99: 'charming',\n",
       " 100: 'here',\n",
       " 101: 'whether',\n",
       " 102: \"'re\",\n",
       " 103: 'enlightened',\n",
       " 104: 'any',\n",
       " 105: 'derrida',\n",
       " 106: 'lectures',\n",
       " 107: 'on',\n",
       " 108: 'other',\n",
       " 109: 'self',\n",
       " 110: 'an',\n",
       " 111: 'undeniably',\n",
       " 112: 'fascinating',\n",
       " 113: 'playful',\n",
       " 114: 'fellow',\n",
       " 115: 'just',\n",
       " 116: 'labour',\n",
       " 117: 'involved',\n",
       " 118: 'in',\n",
       " 119: 'creating',\n",
       " 120: 'layered',\n",
       " 121: 'richness',\n",
       " 122: 'imagery',\n",
       " 123: 'this',\n",
       " 124: 'chiaroscuro',\n",
       " 125: 'madness',\n",
       " 126: 'light',\n",
       " 127: 'astonishing',\n",
       " 128: 'part',\n",
       " 129: 'charm',\n",
       " 130: 'satin',\n",
       " 131: 'rouge',\n",
       " 132: 'it',\n",
       " 133: 'avoids',\n",
       " 134: 'obvious',\n",
       " 135: 'humour',\n",
       " 136: 'lightness',\n",
       " 137: 'screenplay',\n",
       " 138: 'ingeniously',\n",
       " 139: 'constructed',\n",
       " 140: 'memento',\n",
       " 141: 'extreme',\n",
       " 142: 'ops',\n",
       " 143: 'exceeds',\n",
       " 144: 'expectations',\n",
       " 145: 'good',\n",
       " 146: 'fun',\n",
       " 147: 'action',\n",
       " 148: 'acting',\n",
       " 149: 'dialogue',\n",
       " 150: 'pace',\n",
       " 151: 'cinematography',\n",
       " 152: 'should',\n",
       " 153: 'pay',\n",
       " 154: 'nine',\n",
       " 155: 'bucks',\n",
       " 156: 'for',\n",
       " 157: ':',\n",
       " 158: 'because',\n",
       " 159: 'hear',\n",
       " 160: 'about',\n",
       " 161: 'suffering',\n",
       " 162: 'afghan',\n",
       " 163: 'refugees',\n",
       " 164: 'news',\n",
       " 165: 'unaffected',\n",
       " 166: 'dramas',\n",
       " 167: 'like',\n",
       " 168: 'human',\n",
       " 169: 'thunderous',\n",
       " 170: 'ride',\n",
       " 171: 'at',\n",
       " 172: 'first',\n",
       " 173: 'quiet',\n",
       " 174: 'cadences',\n",
       " 175: 'pure',\n",
       " 176: 'finesse',\n",
       " 177: 'are',\n",
       " 178: 'far',\n",
       " 179: 'between',\n",
       " 180: ';',\n",
       " 181: 'their',\n",
       " 182: 'shortage',\n",
       " 183: 'dilutes',\n",
       " 184: 'potency',\n",
       " 185: 'otherwise',\n",
       " 186: 'respectable',\n",
       " 187: 'flick',\n",
       " 188: 'host',\n",
       " 189: 'some',\n",
       " 190: 'truly',\n",
       " 191: 'excellent',\n",
       " 192: 'sequences',\n",
       " 193: 'australian',\n",
       " 194: 'actor\\\\/director',\n",
       " 195: 'john',\n",
       " 196: 'polson',\n",
       " 197: 'award-winning',\n",
       " 198: 'english',\n",
       " 199: 'cinematographer',\n",
       " 200: 'giles',\n",
       " 201: 'nuttgens',\n",
       " 202: 'terrific',\n",
       " 203: 'effort',\n",
       " 204: 'disguising',\n",
       " 205: 'energy',\n",
       " 206: 'innovation',\n",
       " 207: 'walk',\n",
       " 208: 'out',\n",
       " 209: 'girl',\n",
       " 210: 'mixed',\n",
       " 211: 'emotions',\n",
       " 212: 'disapproval',\n",
       " 213: 'justine',\n",
       " 214: 'combined',\n",
       " 215: 'tinge',\n",
       " 216: 'understanding',\n",
       " 217: 'her',\n",
       " 218: 'actions',\n",
       " 219: 'post',\n",
       " 220: '9\\\\/11',\n",
       " 221: 'philosophical',\n",
       " 222: 'message',\n",
       " 223: 'personal',\n",
       " 224: 'freedom',\n",
       " 225: 'might',\n",
       " 226: 'as',\n",
       " 227: 'palatable',\n",
       " 228: 'absorbing',\n",
       " 229: 'character',\n",
       " 230: 'study',\n",
       " 231: 'andré',\n",
       " 232: 'turpin',\n",
       " 233: 'if',\n",
       " 234: 'love',\n",
       " 235: 'reading',\n",
       " 236: 'and\\\\/or',\n",
       " 237: 'poetry',\n",
       " 238: 'then',\n",
       " 239: 'all',\n",
       " 240: 'means',\n",
       " 241: 'check',\n",
       " 242: \"'ll\",\n",
       " 243: 'probably',\n",
       " 244: 'frailty',\n",
       " 245: 'has',\n",
       " 246: 'been',\n",
       " 247: 'written',\n",
       " 248: 'well',\n",
       " 249: 'simple',\n",
       " 250: 'goddammit',\n",
       " 251: '!',\n",
       " 252: 'near',\n",
       " 253: 'end',\n",
       " 254: 'takes',\n",
       " 255: 'meaning',\n",
       " 256: 'grenier',\n",
       " 257: 'bringing',\n",
       " 258: 'unforced',\n",
       " 259: 'rapid-fire',\n",
       " 260: 'delivery',\n",
       " 261: 'toback',\n",
       " 262: 'heidegger',\n",
       " 263: '-',\n",
       " 264: 'nietzsche-referencing',\n",
       " 265: 'sundance',\n",
       " 266: 'film',\n",
       " 267: 'festival',\n",
       " 268: 'become',\n",
       " 269: 'buzz-obsessed',\n",
       " 270: 'fans',\n",
       " 271: 'producers',\n",
       " 272: 'descend',\n",
       " 273: 'upon',\n",
       " 274: 'utah',\n",
       " 275: 'each',\n",
       " 276: 'january',\n",
       " 277: 'ferret',\n",
       " 278: 'next',\n",
       " 279: 'great',\n",
       " 280: 'thing',\n",
       " 281: '`',\n",
       " 282: 'tadpole',\n",
       " 283: \"'\",\n",
       " 284: 'was',\n",
       " 285: 'one',\n",
       " 286: 'films',\n",
       " 287: 'declared',\n",
       " 288: 'year',\n",
       " 289: 'really',\n",
       " 290: 'pretty',\n",
       " 291: 'actors',\n",
       " 292: 'fantastic',\n",
       " 293: 'they',\n",
       " 294: 'what',\n",
       " 295: 'makes',\n",
       " 296: 'worth',\n",
       " 297: 'trip',\n",
       " 298: 'theatre',\n",
       " 299: '-lrb-',\n",
       " 300: 'taymor',\n",
       " 301: '-rrb-',\n",
       " 302: 'utilizes',\n",
       " 303: 'idea',\n",
       " 304: 'making',\n",
       " 305: 'kahlo',\n",
       " 306: 'art',\n",
       " 307: 'living',\n",
       " 308: 'breathing',\n",
       " 309: 'movie',\n",
       " 310: 'often',\n",
       " 311: 'catapulting',\n",
       " 312: 'artist',\n",
       " 313: 'into',\n",
       " 314: 'own',\n",
       " 315: 'work',\n",
       " 316: \"n't\",\n",
       " 317: 'done',\n",
       " 318: 'before',\n",
       " 319: 'never',\n",
       " 320: 'vividly',\n",
       " 321: 'much',\n",
       " 322: 'passion',\n",
       " 323: 'take',\n",
       " 324: 'care',\n",
       " 325: 'my',\n",
       " 326: 'cat',\n",
       " 327: 'honestly',\n",
       " 328: 'nice',\n",
       " 329: 'little',\n",
       " 330: 'us',\n",
       " 331: 'examination',\n",
       " 332: 'young',\n",
       " 333: 'adult',\n",
       " 334: 'life',\n",
       " 335: 'urban',\n",
       " 336: 'south',\n",
       " 337: 'korea',\n",
       " 338: 'through',\n",
       " 339: 'minds',\n",
       " 340: 'five',\n",
       " 341: 'principals',\n",
       " 342: 'life-affirming',\n",
       " 343: 'its',\n",
       " 344: 'vulgar',\n",
       " 345: 'mean',\n",
       " 346: 'i',\n",
       " 347: 'liked',\n",
       " 348: 'empire',\n",
       " 349: 'lacks',\n",
       " 350: 'depth',\n",
       " 351: 'up',\n",
       " 352: 'heart',\n",
       " 353: 'silly',\n",
       " 354: 'photographed',\n",
       " 355: 'colour',\n",
       " 356: 'rather',\n",
       " 357: 'time',\n",
       " 358: 'tongue-in-cheek',\n",
       " 359: 'preposterousness',\n",
       " 360: 'always',\n",
       " 361: 'most',\n",
       " 362: 'wilde',\n",
       " 363: 'droll',\n",
       " 364: 'whimsy',\n",
       " 365: 'helps',\n",
       " 366: 'being',\n",
       " 367: 'earnest',\n",
       " 368: 'overcome',\n",
       " 369: 'weaknesses',\n",
       " 370: 'parker',\n",
       " 371: 'creative',\n",
       " 372: 'interference',\n",
       " 373: '...',\n",
       " 374: 'lies',\n",
       " 375: 'utter',\n",
       " 376: 'cuteness',\n",
       " 377: 'stuart',\n",
       " 378: 'margolo',\n",
       " 379: 'computer-animated',\n",
       " 380: 'faces',\n",
       " 381: 'very',\n",
       " 382: 'expressive',\n",
       " 383: 'path',\n",
       " 384: 'ice',\n",
       " 385: 'age',\n",
       " 386: 'follows',\n",
       " 387: 'closely',\n",
       " 388: 'though',\n",
       " 389: 'established',\n",
       " 390: 'warner',\n",
       " 391: 'bros.',\n",
       " 392: 'giant',\n",
       " 393: 'chuck',\n",
       " 394: 'jones',\n",
       " 395: 'who',\n",
       " 396: 'died',\n",
       " 397: 'matter',\n",
       " 398: 'weeks',\n",
       " 399: 'release',\n",
       " 400: 'spiced',\n",
       " 401: 'humor',\n",
       " 402: 'speak',\n",
       " 403: 'fluent',\n",
       " 404: 'flatula',\n",
       " 405: 'advises',\n",
       " 406: 'denlopp',\n",
       " 407: 'after',\n",
       " 408: 'bubbly',\n",
       " 409: 'exchange',\n",
       " 410: 'alien',\n",
       " 411: 'deckhand',\n",
       " 412: 'witty',\n",
       " 413: 'updatings',\n",
       " 414: 'silver',\n",
       " 415: 'parrot',\n",
       " 416: 'replaced',\n",
       " 417: 'morph',\n",
       " 418: 'cute',\n",
       " 419: 'creature',\n",
       " 420: 'mimics',\n",
       " 421: 'everyone',\n",
       " 422: 'everything',\n",
       " 423: 'around',\n",
       " 424: 'there',\n",
       " 425: 'y',\n",
       " 426: 'tu',\n",
       " 427: 'mamá',\n",
       " 428: 'también',\n",
       " 429: 'comes',\n",
       " 430: 'from',\n",
       " 431: 'brave',\n",
       " 432: 'uninhibited',\n",
       " 433: 'performances',\n",
       " 434: 'lead',\n",
       " 435: '13',\n",
       " 436: 'conversations',\n",
       " 437: 'holds',\n",
       " 438: 'goodwill',\n",
       " 439: 'close',\n",
       " 440: 'relatively',\n",
       " 441: 'slow',\n",
       " 442: 'come',\n",
       " 443: 'point',\n",
       " 444: 'auto',\n",
       " 445: 'focus',\n",
       " 446: 'works',\n",
       " 447: 'unusual',\n",
       " 448: 'biopic',\n",
       " 449: 'document',\n",
       " 450: 'male',\n",
       " 451: 'swingers',\n",
       " 452: 'playboy',\n",
       " 453: 'era',\n",
       " 454: 'mr.',\n",
       " 455: 'zhang',\n",
       " 456: 'subject',\n",
       " 457: 'degree',\n",
       " 458: 'least',\n",
       " 459: 'quintessentially',\n",
       " 460: 'american',\n",
       " 461: 'his',\n",
       " 462: 'approach',\n",
       " 463: 'storytelling',\n",
       " 464: 'called',\n",
       " 465: 'iranian',\n",
       " 466: 'sour',\n",
       " 467: 'core',\n",
       " 468: 'exploration',\n",
       " 469: 'emptiness',\n",
       " 470: 'underlay',\n",
       " 471: 'relentless',\n",
       " 472: 'gaiety',\n",
       " 473: '1920',\n",
       " 474: 'ending',\n",
       " 475: '?',\n",
       " 476: 'feeling',\n",
       " 477: 'deal',\n",
       " 478: 'cremaster',\n",
       " 479: '3',\n",
       " 480: 'warning',\n",
       " 481: 'serious',\n",
       " 482: 'buffs',\n",
       " 483: 'only',\n",
       " 484: 'made',\n",
       " 485: 'me',\n",
       " 486: 'unintentionally',\n",
       " 487: 'famous',\n",
       " 488: 'queasy-stomached',\n",
       " 489: 'critic',\n",
       " 490: 'staggered',\n",
       " 491: 'theater',\n",
       " 492: 'blacked',\n",
       " 493: 'lobby',\n",
       " 494: 'believe',\n",
       " 495: 'beautiful',\n",
       " 496: 'evocative',\n",
       " 497: \"'ve\",\n",
       " 498: 'seen',\n",
       " 499: 'garcía',\n",
       " 500: 'bernal',\n",
       " 501: 'talancón',\n",
       " 502: 'immensely',\n",
       " 503: 'appealing',\n",
       " 504: 'couple',\n",
       " 505: 'predictable',\n",
       " 506: 'want',\n",
       " 507: 'things',\n",
       " 508: 'spoof',\n",
       " 509: 'comedy',\n",
       " 510: 'carries',\n",
       " 511: 'share',\n",
       " 512: 'laughs',\n",
       " 513: 'sometimes',\n",
       " 514: 'chuckle',\n",
       " 515: 'guffaw',\n",
       " 516: 'pleasure',\n",
       " 517: 'occasional',\n",
       " 518: 'belly',\n",
       " 519: 'laugh',\n",
       " 520: 'city',\n",
       " 521: 'reminds',\n",
       " 522: 'how',\n",
       " 523: 'realistically',\n",
       " 524: 'nuanced',\n",
       " 525: 'robert',\n",
       " 526: 'de',\n",
       " 527: 'niro',\n",
       " 528: 'performance',\n",
       " 529: 'when',\n",
       " 530: 'lucratively',\n",
       " 531: 'engaged',\n",
       " 532: 'shameless',\n",
       " 533: 'self-caricature',\n",
       " 534: 'analyze',\n",
       " 535: '1999',\n",
       " 536: 'promised',\n",
       " 537: 'threatened',\n",
       " 538: 'later',\n",
       " 539: 'wanton',\n",
       " 540: 'slipperiness',\n",
       " 541: '\\\\*',\n",
       " 542: 'corpus',\n",
       " 543: 'amiable',\n",
       " 544: 'jerking',\n",
       " 545: 'reshaping',\n",
       " 546: 'physical',\n",
       " 547: 'space',\n",
       " 548: 'watch',\n",
       " 549: 'kids',\n",
       " 550: 'use',\n",
       " 551: 'introduce',\n",
       " 552: 'video',\n",
       " 553: 'starts',\n",
       " 554: 'typical',\n",
       " 555: 'bible',\n",
       " 556: 'killer',\n",
       " 557: 'turns',\n",
       " 558: 'significantly',\n",
       " 559: 'different',\n",
       " 560: 'better',\n",
       " 561: 'theme',\n",
       " 562: 'those',\n",
       " 563: 'pride',\n",
       " 564: 'themselves',\n",
       " 565: 'sophisticated',\n",
       " 566: 'discerning',\n",
       " 567: 'taste',\n",
       " 568: 'seem',\n",
       " 569: 'proper',\n",
       " 570: 'cup',\n",
       " 571: 'tea',\n",
       " 572: 'however',\n",
       " 573: 'almost',\n",
       " 574: 'guaranteed',\n",
       " 575: 'stuffiest',\n",
       " 576: 'cinema',\n",
       " 577: 'goers',\n",
       " 578: 'will',\n",
       " 579: '\\\\*\\\\*\\\\*',\n",
       " 580: 'off',\n",
       " 581: 'hour-and-a-half',\n",
       " 582: 'cuts',\n",
       " 583: 'actually',\n",
       " 584: 'face',\n",
       " 585: 'your',\n",
       " 586: 'fears',\n",
       " 587: 'world',\n",
       " 588: 'boys',\n",
       " 589: 'boy',\n",
       " 590: 'big',\n",
       " 591: 'metaphorical',\n",
       " 592: 'wave',\n",
       " 593: 'wherever',\n",
       " 594: 'welcome',\n",
       " 595: 'relief',\n",
       " 596: 'baseball',\n",
       " 597: 'movies',\n",
       " 598: 'try',\n",
       " 599: 'too',\n",
       " 600: 'hard',\n",
       " 601: 'mythic',\n",
       " 602: 'sweet',\n",
       " 603: 'modest',\n",
       " 604: 'ultimately',\n",
       " 605: 'winning',\n",
       " 606: 'crisp',\n",
       " 607: 'psychological',\n",
       " 608: 'drama',\n",
       " 609: 'thriller',\n",
       " 610: 'perfect',\n",
       " 611: 'old',\n",
       " 612: 'twilight',\n",
       " 613: 'zone',\n",
       " 614: 'episode',\n",
       " 615: 'moments',\n",
       " 616: 'insightful',\n",
       " 617: 'fondly',\n",
       " 618: 'remembered',\n",
       " 619: 'endlessly',\n",
       " 620: 'challenging',\n",
       " 621: 'maze',\n",
       " 622: 'moviegoing',\n",
       " 623: 'opening',\n",
       " 624: 'contrived',\n",
       " 625: 'banter',\n",
       " 626: 'cliches',\n",
       " 627: 'loose',\n",
       " 628: 'ends',\n",
       " 629: 'second',\n",
       " 630: 'half',\n",
       " 631: 'uncluttered',\n",
       " 632: 'resonant',\n",
       " 633: 'gem',\n",
       " 634: 'relays',\n",
       " 635: 'universal',\n",
       " 636: 'points',\n",
       " 637: 'without',\n",
       " 638: 'confrontations',\n",
       " 639: 'cockettes',\n",
       " 640: 'provides',\n",
       " 641: 'window',\n",
       " 642: 'subculture',\n",
       " 643: 'hell-bent',\n",
       " 644: 'expressing',\n",
       " 645: 'itself',\n",
       " 646: 'every',\n",
       " 647: 'way',\n",
       " 648: 'imaginable',\n",
       " 649: 'smart',\n",
       " 650: 'steamy',\n",
       " 651: 'mix',\n",
       " 652: 'road',\n",
       " 653: 'coming-of-age',\n",
       " 654: 'political',\n",
       " 655: 'satire',\n",
       " 656: 'modern-day',\n",
       " 657: 'royals',\n",
       " 658: 'nothing',\n",
       " 659: 'these',\n",
       " 660: 'guys',\n",
       " 661: 'scandals',\n",
       " 662: 'fairy',\n",
       " 663: 'tales',\n",
       " 664: 'princesses',\n",
       " 665: 'married',\n",
       " 666: 'reason',\n",
       " 667: 'live',\n",
       " 668: 'happily',\n",
       " 669: 'ever',\n",
       " 670: 'b',\n",
       " 671: 'fact',\n",
       " 672: 'best',\n",
       " 673: 'recent',\n",
       " 674: 'memory',\n",
       " 675: 'birthday',\n",
       " 676: 'actor',\n",
       " 677: 'foremost',\n",
       " 678: 'walked',\n",
       " 679: 'away',\n",
       " 680: 'version',\n",
       " 681: 'e.t.',\n",
       " 682: 'hoped',\n",
       " 683: 'moist',\n",
       " 684: 'eyes',\n",
       " 685: 'devotees',\n",
       " 686: 'french',\n",
       " 687: 'safe',\n",
       " 688: 'conduct',\n",
       " 689: 'rich',\n",
       " 690: 'period',\n",
       " 691: 'minutiae',\n",
       " 692: 'dying',\n",
       " 693: 'celluloid',\n",
       " 694: 'heaven',\n",
       " 695: 'characters',\n",
       " 696: 'resemblance',\n",
       " 697: 'everyday',\n",
       " 698: 'children',\n",
       " 699: 'shamelessly',\n",
       " 700: 'resorting',\n",
       " 701: 'pee-related',\n",
       " 702: 'sight',\n",
       " 703: 'gags',\n",
       " 704: 'cause',\n",
       " 705: 'tom',\n",
       " 706: 'green',\n",
       " 707: 'grimace',\n",
       " 708: 'myer',\n",
       " 709: 'silliness',\n",
       " 710: 'eventually',\n",
       " 711: 'prevail',\n",
       " 712: 'absurdist',\n",
       " 713: 'spider',\n",
       " 714: 'web',\n",
       " 715: 'happy',\n",
       " 716: 'listening',\n",
       " 717: 'watching',\n",
       " 718: 'them',\n",
       " 719: 'parade',\n",
       " 720: 'fascinates',\n",
       " 721: 'right',\n",
       " 722: 'moves',\n",
       " 723: 'beyond',\n",
       " 724: 'original',\n",
       " 725: 'nostalgia',\n",
       " 726: 'communal',\n",
       " 727: 'experiences',\n",
       " 728: 'yesteryear',\n",
       " 729: 'deeper',\n",
       " 730: 'realization',\n",
       " 731: 'inability',\n",
       " 732: 'stand',\n",
       " 733: 'true',\n",
       " 734: 'lived',\n",
       " 735: 'experience',\n",
       " 736: 'blend',\n",
       " 737: 'together',\n",
       " 738: 'distant',\n",
       " 739: 'memories',\n",
       " 740: 'mention',\n",
       " 741: 'solaris',\n",
       " 742: 'years',\n",
       " 743: \"'m\",\n",
       " 744: 'sure',\n",
       " 745: 'saw',\n",
       " 746: 'opinion',\n",
       " 747: 'allen',\n",
       " 748: 'funniest',\n",
       " 749: 'likeable',\n",
       " 750: 'glorious',\n",
       " 751: 'spectacle',\n",
       " 752: 'd.w.',\n",
       " 753: 'griffith',\n",
       " 754: 'early',\n",
       " 755: 'days',\n",
       " 756: 'silent',\n",
       " 757: 'comic',\n",
       " 758: 'delightful',\n",
       " 759: 'derivative',\n",
       " 760: 'timely',\n",
       " 761: 'director',\n",
       " 762: 'could',\n",
       " 763: 'dreamed',\n",
       " 764: 'quietly',\n",
       " 765: 'lyrical',\n",
       " 766: 'tale',\n",
       " 767: 'probes',\n",
       " 768: 'ambiguous',\n",
       " 769: 'extended',\n",
       " 770: 'iran',\n",
       " 771: 'afghani',\n",
       " 772: 'streamed',\n",
       " 773: 'across',\n",
       " 774: 'borders',\n",
       " 775: 'desperate',\n",
       " 776: 'food',\n",
       " 777: 'leaping',\n",
       " 778: 'line',\n",
       " 779: 'shaped',\n",
       " 780: 'kosminsky',\n",
       " 781: 'sharp',\n",
       " 782: 'slivers',\n",
       " 783: 'cutting',\n",
       " 784: 'impressions',\n",
       " 785: 'shows',\n",
       " 786: 'signs',\n",
       " 787: 'detail',\n",
       " 788: 'condensed',\n",
       " 789: 'images',\n",
       " 790: 'striking',\n",
       " 791: 'traits',\n",
       " 792: 'three',\n",
       " 793: 'principal',\n",
       " 794: 'singers',\n",
       " 795: 'youthful',\n",
       " 796: 'good-looking',\n",
       " 797: 'diva',\n",
       " 798: 'tenor',\n",
       " 799: 'richly',\n",
       " 800: 'handsome',\n",
       " 801: 'locations',\n",
       " 802: 'wish',\n",
       " 803: 'jacquot',\n",
       " 804: 'left',\n",
       " 805: 'alone',\n",
       " 806: 'filmed',\n",
       " 807: 'opera',\n",
       " 808: 'distortions',\n",
       " 809: 'perspective',\n",
       " 810: 'production',\n",
       " 811: 'enormous',\n",
       " 812: 'amount',\n",
       " 813: 'affection',\n",
       " 814: 'we',\n",
       " 815: 'worthwhile',\n",
       " 816: 'winds',\n",
       " 817: 'both',\n",
       " 818: 'revelatory',\n",
       " 819: 'narcissistic',\n",
       " 820: 'achieving',\n",
       " 821: 'honest',\n",
       " 822: 'insight',\n",
       " 823: 'relationships',\n",
       " 824: 'high-concept',\n",
       " 825: 'candy-coat',\n",
       " 826: 'pat',\n",
       " 827: 'storylines',\n",
       " 828: 'precious',\n",
       " 829: 'circumstances',\n",
       " 830: 'stars',\n",
       " 831: 'inspiring',\n",
       " 832: 'joy',\n",
       " 833: 'spielberg',\n",
       " 834: 'brings',\n",
       " 835: 'another',\n",
       " 836: 'masterpiece',\n",
       " 837: 'finally',\n",
       " 838: 'french-produced',\n",
       " 839: 'read',\n",
       " 840: 'lips',\n",
       " 841: 'understands',\n",
       " 842: 'must',\n",
       " 843: 'ms.',\n",
       " 844: 'seigner',\n",
       " 845: 'serrault',\n",
       " 846: 'bring',\n",
       " 847: 'fresh',\n",
       " 848: 'naturalism',\n",
       " 849: 'outgag',\n",
       " 850: 'whippersnappers',\n",
       " 851: 'moving',\n",
       " 852: 'pictures',\n",
       " 853: 'today',\n",
       " 854: 'solid',\n",
       " 855: 'pedigree',\n",
       " 856: 'front',\n",
       " 857: 'specifically',\n",
       " 858: 'behind',\n",
       " 859: 'camera',\n",
       " 860: 'no',\n",
       " 861: 'slam-dunk',\n",
       " 862: 'disappoint',\n",
       " 863: 'moved',\n",
       " 864: 'edge',\n",
       " 865: 'seats',\n",
       " 866: 'dynamic',\n",
       " 867: 'touching',\n",
       " 868: 'transcendent',\n",
       " 869: 'encourage',\n",
       " 870: 'alike',\n",
       " 871: 'go',\n",
       " 872: 'see',\n",
       " 873: 'unique',\n",
       " 874: 'entertaining',\n",
       " 875: 'twist',\n",
       " 876: 'classic',\n",
       " 877: 'whale',\n",
       " 878: 'wo',\n",
       " 879: 'sorry',\n",
       " 880: 'literary',\n",
       " 881: 'detective',\n",
       " 882: 'aficionados',\n",
       " 883: 'whodunit',\n",
       " 884: 'disappointed',\n",
       " 885: 'high',\n",
       " 886: 'crimes',\n",
       " 887: 'steals',\n",
       " 888: 'freely',\n",
       " 889: 'combines',\n",
       " 890: 'disparate',\n",
       " 891: 'types',\n",
       " 892: 'ca',\n",
       " 893: 'help',\n",
       " 894: 'engage',\n",
       " 895: 'audience',\n",
       " 896: 'fan',\n",
       " 897: 'series',\n",
       " 898: 'twice',\n",
       " 899: 'celebrates',\n",
       " 900: 'group',\n",
       " 901: 'spark',\n",
       " 902: 'nonconformity',\n",
       " 903: 'glancing',\n",
       " 904: 'back',\n",
       " 905: 'hibiscus',\n",
       " 906: 'grandly',\n",
       " 907: 'angels',\n",
       " 908: 'ironic',\n",
       " 909: 'ridiculous',\n",
       " 910: 'money-oriented',\n",
       " 911: 'record',\n",
       " 912: 'industry',\n",
       " 913: 'also',\n",
       " 914: 'testament',\n",
       " 915: 'integrity',\n",
       " 916: 'band',\n",
       " 917: 'laced',\n",
       " 918: 'liberal',\n",
       " 919: 'doses',\n",
       " 920: 'dark',\n",
       " 921: 'gorgeous',\n",
       " 922: 'exterior',\n",
       " 923: 'photography',\n",
       " 924: 'stable-full',\n",
       " 925: 'such',\n",
       " 926: 'huppert',\n",
       " 927: 'show',\n",
       " 928: 'steal',\n",
       " 929: 'she',\n",
       " 930: 'meal',\n",
       " 931: 'channeling',\n",
       " 932: 'kathy',\n",
       " 933: 'baker',\n",
       " 934: 'creepy',\n",
       " 935: 'turn',\n",
       " 936: 'repressed',\n",
       " 937: 'mother',\n",
       " 938: 'boston',\n",
       " 939: 'public',\n",
       " 940: '8',\n",
       " 941: 'women',\n",
       " 942: 'augustine',\n",
       " 943: 'nair',\n",
       " 944: 'does',\n",
       " 945: 'treat',\n",
       " 946: 'issues',\n",
       " 947: 'lightly',\n",
       " 948: 'allows',\n",
       " 949: 'confront',\n",
       " 950: 'problems',\n",
       " 951: 'openly',\n",
       " 952: 'horror',\n",
       " 953: 'real',\n",
       " 954: 'shocks',\n",
       " 955: 'store',\n",
       " 956: 'unwary',\n",
       " 957: 'viewers',\n",
       " 958: 'filmmaker',\n",
       " 959: 'secrets',\n",
       " 960: 'buried',\n",
       " 961: 'knows',\n",
       " 962: 'revealing',\n",
       " 963: 'strange',\n",
       " 964: 'occurrences',\n",
       " 965: 'build',\n",
       " 966: 'mind',\n",
       " 967: 'viewer',\n",
       " 968: 'urgency',\n",
       " 969: 'certain',\n",
       " 970: 'ghoulish',\n",
       " 971: 'fascination',\n",
       " 972: 'generates',\n",
       " 973: 'fair',\n",
       " 974: 'b-movie',\n",
       " 975: 'excitement',\n",
       " 976: 'familiar',\n",
       " 977: 'utterly',\n",
       " 978: 'keeps',\n",
       " 979: 'hooked',\n",
       " 980: 'delicious',\n",
       " 981: 'pulpiness',\n",
       " 982: 'lurid',\n",
       " 983: 'fiction',\n",
       " 984: 'aims',\n",
       " 985: 'funny',\n",
       " 986: 'uplifting',\n",
       " 987: 'once',\n",
       " 988: 'extent',\n",
       " 989: 'which',\n",
       " 990: 'succeeds',\n",
       " 991: 'impressive',\n",
       " 992: 'brilliantly',\n",
       " 993: 'shines',\n",
       " 994: 'direction',\n",
       " 995: 'intelligently',\n",
       " 996: 'accomplished',\n",
       " 997: 'while',\n",
       " 998: 'collegiate',\n",
       " 999: 'gross-out',\n",
       " ...}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "i2w = {v:k for k,v in w2i.items()}\n",
    "i2w[0] = \"<unk>\"\n",
    "i2w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "28b045b2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: '3', 1: '4', 2: '2', 3: '1', 4: '0'}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "i2t = {v:k for k,v in t2i.items()}\n",
    "i2t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "56d20049",
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_sample(sample):\n",
    "    return [i2w[x] if x in i2w else \"<unk>\" for x in sample[0]], i2t[sample[1]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c3bbe81",
   "metadata": {},
   "source": [
    "Now it'll be easier to interpret a sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "790123a6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('good fun , good action , good acting , good dialogue , good pace , good cinematography .',\n",
       " '4')"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words, tag = decode_sample(train[10])\n",
    "\" \".join(words), tag"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "006104f2",
   "metadata": {},
   "source": [
    "## DyNet\n",
    "\n",
    "DyNet is a neural network toolkit produced by Carnegie Mellon University.  While not as well-known as, e.g., TensorFlow or PyTorch, it does most of the same things.\n",
    "\n",
    "Like 3D modeling software or carcinization ([the process by which nature evolves a crab](https://en.wikipedia.org/wiki/Carcinisation)), most neural network toolkits get you to the same place different ways."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d921671",
   "metadata": {},
   "source": [
    "DyNet also runs on a **computation graph** (like TF or PyTorch).\n",
    "\n",
    "A computation graph is a directed graph where the nodes correspond to operations or variables. Variables can feed their value into operations, and operations can feed their output into other operations. This way, every node in the graph defines a function of the variables.\n",
    "\n",
    "DyNet builds its computational graph on the fly. This makes variable-input and variable-output models simple to implement. It also makes defining models very compact."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a22f3aa8",
   "metadata": {},
   "source": [
    "Let's build a DyNet model and examine it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0fba2a9b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Start DyNet and define trainer\n",
    "model = dy.Model()\n",
    "trainer = dy.AdamTrainer(model)\n",
    "\n",
    "model.parameters_list()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "554cf695",
   "metadata": {},
   "source": [
    "Right now the model is empty."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71476f98",
   "metadata": {},
   "source": [
    "There are two types of parameters: **Lookup Parameters** and just plain **Parameters**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fae0eda",
   "metadata": {},
   "source": [
    "LookupParameters represents a table of parameters.\n",
    "\n",
    "They are used to embed a set of discrete objects (e.g. word embeddings). These are sparsely updated."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93b1921e",
   "metadata": {},
   "source": [
    "Parameters are things that are optimized. in contrast to a system like Torch where computational modules may have their own parameters, in DyNet parameters are just parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "009351ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the model\n",
    "W_sm = model.add_lookup_parameters((nwords, ntags)) # Word weights\n",
    "b_sm = model.add_parameters((ntags))                # Softmax bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0a857df8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nwords 18648\n",
      "ntags 5\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Parameter /_1]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"nwords\",nwords)\n",
    "print(\"ntags\",ntags)\n",
    "\n",
    "model.parameters_list()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f860c332",
   "metadata": {},
   "source": [
    "That's a 1-element list.  Let's see what's in it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4963d701",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.7385484 , -0.63852179,  0.11619706, -0.02760649, -0.71378034])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.parameters_list()[0].as_array()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7da696d7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.7385484 , -0.63852179,  0.11619706, -0.02760649, -0.71378034])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b_sm.as_array()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c783fb9c",
   "metadata": {},
   "source": [
    "Those are my softbax biases.  There are 5 values for the 5 tags, or 5 output values of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "68a04fc5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[LookupParameter /_0]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.lookup_parameters_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "bde65480",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.00920034, -0.00244009,  0.01366456, -0.01458473,  0.00298795],\n",
       "       [-0.01655894,  0.00119357,  0.01582853, -0.00662631, -0.00664929],\n",
       "       [-0.00122233,  0.00199532,  0.01677718, -0.00842867, -0.00135602],\n",
       "       ...,\n",
       "       [ 0.01266512,  0.01045685,  0.01523736,  0.01097631, -0.00882406],\n",
       "       [-0.0073431 , -0.00154144,  0.00119418,  0.00633218, -0.01244928],\n",
       "       [ 0.00456406,  0.00715536, -0.01556415, -0.01576499,  0.00834564]])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.lookup_parameters_list()[0].as_array()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f1382d8",
   "metadata": {},
   "source": [
    "That looks big."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b9d55082",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(18648, 5)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.lookup_parameters_list()[0].as_array().shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98b57f45",
   "metadata": {},
   "source": [
    "It is big."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c997de6a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.00920034, -0.00244009,  0.01366456, -0.01458473,  0.00298795],\n",
       "       [-0.01655894,  0.00119357,  0.01582853, -0.00662631, -0.00664929],\n",
       "       [-0.00122233,  0.00199532,  0.01677718, -0.00842867, -0.00135602],\n",
       "       ...,\n",
       "       [ 0.01266512,  0.01045685,  0.01523736,  0.01097631, -0.00882406],\n",
       "       [-0.0073431 , -0.00154144,  0.00119418,  0.00633218, -0.01244928],\n",
       "       [ 0.00456406,  0.00715536, -0.01556415, -0.01576499,  0.00834564]])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "W_sm.as_array()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e538f37",
   "metadata": {},
   "source": [
    "These are weights associated with every word (18,648 of them) and every tag or class (5 of them).  According to this, the first word in the vocabulary should have almost no association with any particular tag (all values close to zero)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd54a89a",
   "metadata": {},
   "source": [
    "What are the highest-weighted words for each category?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "06174959",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([17154, 18142, 10535,  8916, 18515])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.argmax(W_sm.as_array(), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a21734ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('<unk>', 0.002987946616485715)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "highest_weighted_word_idx = np.argmax(W_sm.as_array(), axis=0)[t2i['0']]\n",
    "highest_weighted_word_idx = highest_weighted_word_idx if highest_weighted_word_idx in i2w else 0 \n",
    "i2w[highest_weighted_word_idx], W_sm.as_array()[highest_weighted_word_idx][t2i['0']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "efa138b3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('frei', 0.017933908849954605)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "highest_weighted_word_idx = np.argmax(W_sm.as_array(), axis=0)[t2i['1']]\n",
    "highest_weighted_word_idx = highest_weighted_word_idx if highest_weighted_word_idx in i2w else 0 \n",
    "i2w[highest_weighted_word_idx], W_sm.as_array()[highest_weighted_word_idx][t2i['1']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c92b0486",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('drug-related', 0.017933925613760948)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "highest_weighted_word_idx = np.argmax(W_sm.as_array(), axis=0)[t2i['2']]\n",
    "highest_weighted_word_idx = highest_weighted_word_idx if highest_weighted_word_idx in i2w else 0 \n",
    "i2w[highest_weighted_word_idx], W_sm.as_array()[highest_weighted_word_idx][t2i['2']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a3de9586",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('<unk>', 0.009200339205563068)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "highest_weighted_word_idx = np.argmax(W_sm.as_array(), axis=0)[t2i['3']]\n",
    "highest_weighted_word_idx = highest_weighted_word_idx if highest_weighted_word_idx in i2w else 0 \n",
    "i2w[highest_weighted_word_idx], W_sm.as_array()[highest_weighted_word_idx][t2i['3']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ce6c01d5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('<unk>', -0.002440092386677861)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "highest_weighted_word_idx = np.argmax(W_sm.as_array(), axis=0)[t2i['4']] \n",
    "highest_weighted_word_idx = highest_weighted_word_idx if highest_weighted_word_idx in i2w else 0 \n",
    "i2w[highest_weighted_word_idx], W_sm.as_array()[highest_weighted_word_idx][t2i['4']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "742d8fa5",
   "metadata": {},
   "source": [
    "Weights are initialized uniformly, so the highest-weighted word for each category is pretty random, and also the weight associated with that word is almost identical for all of the highest-weighted words per tag."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf155839",
   "metadata": {},
   "source": [
    "### Expressions\n",
    "\n",
    "Expressions are used as an interface to the various functions that can be used to build computation graphs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "ceb65585",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "from __future__ import division\n",
    "from __future__ import absolute_import\n",
    "\n",
    "# Note: please import dynet_config before import dynet\n",
    "import dynet_config\n",
    "# set random seed to have the same result each time\n",
    "dynet_config.set(random_seed=0)\n",
    "\n",
    "## ==== Create a new computation graph\n",
    "# (There is a single global computation graph that is used at any point.\n",
    "# dy.renew_cg() clears the current one and starts a new one)\n",
    "dy.renew_cg();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "963a5018",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "expression 0/1"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create a scalar expression.\n",
    "value = 5.0\n",
    "x = dy.scalarInput(value)\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "34a227fe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "expression 1/1"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create a vector expression.\n",
    "dimension = 3\n",
    "v = dy.vecInput(dimension)\n",
    "v.set([1,2,3])\n",
    "v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "53e7d4dc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(expression 2/1, expression 3/1, expression 4/1)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create a matrix expression from a list\n",
    "mat1 = dy.inputTensor([[1,2], [3,4]]) # Row major\n",
    "\n",
    "# or, using a numpy array\n",
    "mat2 = dy.inputTensor(np.array([[1,2], [3,4]]))\n",
    "\n",
    "mat3 = dy.inputTensor(np.zeros((2,3)))\n",
    "mat1,mat2,mat3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "01bfb7a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1. 2.]\n",
      " [3. 4.]]\n",
      "[[1. 2.]\n",
      " [3. 4.]]\n",
      "[[1. 2.]\n",
      " [3. 4.]]\n",
      "[[0. 0. 0.]\n",
      " [0. 0. 0.]]\n",
      "[1.0, 2.0, 3.0]\n",
      "5.0\n",
      "5.0\n"
     ]
    }
   ],
   "source": [
    "## ==== Calculate the value of an expression.\n",
    "# This will run the forward step of the neural network.\n",
    "print(mat1.value())\n",
    "print(mat1.npvalue())    # as numpy array\n",
    "print(mat2.value())\n",
    "print(mat3.value())\n",
    "print(v.vec_value())     # as vector, if vector\n",
    "print(x.scalar_value())  # as scalar, if scalar\n",
    "print(x.value())         # choose the correct one"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2eb0d7c",
   "metadata": {},
   "source": [
    "### Some Function Definitions\n",
    "\n",
    "* **`dy.renew_cg()`**: Resets the computation graph\n",
    "* **`dy.esum(...)`**: Elementwise sum\n",
    "* **`dy.lookup(...)`**: Create an expression from lookup parameters\n",
    "* **`dy.parameter(...)`**: Add parameters to computation graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "20a8b684",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A function to calculate scores for one value\n",
    "def calc_scores(words):\n",
    "    dy.renew_cg()\n",
    "    score = dy.esum([dy.lookup(W_sm, x) for x in words])\n",
    "    b_sm_exp = dy.parameter(b_sm)\n",
    "    return score + b_sm_exp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5365934e",
   "metadata": {},
   "source": [
    "Now I'm ready to train and test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "56b36939",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The dy.parameter(...) call is now DEPRECATED.\n",
      "        There is no longer need to explicitly add parameters to the computation graph.\n",
      "        Any used parameter will be added automatically.\n",
      "iter 0: train loss/sent=1.5076, time=0.21s\n",
      "iter 0: test acc=0.3579\n",
      "iter 10: train loss/sent=0.8491, time=0.16s\n",
      "iter 10: test acc=0.4109\n",
      "iter 20: train loss/sent=0.6537, time=0.16s\n",
      "iter 20: test acc=0.4131\n",
      "iter 30: train loss/sent=0.5470, time=0.16s\n",
      "iter 30: test acc=0.4131\n",
      "iter 40: train loss/sent=0.4773, time=0.16s\n",
      "iter 40: test acc=0.4109\n",
      "iter 50: train loss/sent=0.4273, time=0.16s\n",
      "iter 50: test acc=0.4059\n",
      "iter 60: train loss/sent=0.3888, time=0.17s\n",
      "iter 60: test acc=0.4032\n",
      "iter 70: train loss/sent=0.3582, time=0.19s\n",
      "iter 70: test acc=0.4059\n",
      "iter 80: train loss/sent=0.3324, time=0.18s\n",
      "iter 80: test acc=0.4018\n",
      "iter 90: train loss/sent=0.3105, time=0.16s\n",
      "iter 90: test acc=0.3964\n"
     ]
    }
   ],
   "source": [
    "epochs = 100\n",
    "for ITER in range(epochs):\n",
    "    # Perform training\n",
    "    random.shuffle(train)\n",
    "    train_loss = 0.0\n",
    "    start = time.time()\n",
    "    for words, tag in train:\n",
    "        my_loss = dy.pickneglogsoftmax(calc_scores(words), tag) # negative softmax log likelihood\n",
    "        train_loss += my_loss.value()\n",
    "        my_loss.backward()\n",
    "        trainer.update()\n",
    "    if ITER % int(epochs/10) == 0:\n",
    "        print(\"iter %r: train loss/sent=%.4f, time=%.2fs\" % (ITER, train_loss/len(train), time.time()-start))\n",
    "    # Perform testing\n",
    "    test_correct = 0.0\n",
    "    for words, tag in dev:\n",
    "        scores = calc_scores(words).npvalue()\n",
    "        predict = np.argmax(scores)\n",
    "        if predict == tag:\n",
    "            test_correct += 1\n",
    "    if ITER % int(epochs/10) == 0:\n",
    "        print(\"iter %r: test acc=%.4f\" % (ITER, test_correct/len(dev)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "840ebf4f",
   "metadata": {},
   "source": [
    "Let's look at some of the samples, the first 20."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "36629d43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('effective but <unk> biopic', '2')\n",
      "('if you sometimes like to go to the movies to have fun , wasabi is a good place to start .', '3')\n",
      "(\"emerges as something rare , an issue movie that 's so honest and keenly observed that it does n't feel like one .\", '4')\n",
      "('the film provides some great insight into the neurotic mindset of all comics -- even those who have reached the absolute top of the game .', '2')\n",
      "('offers that rare combination of entertainment and education .', '4')\n",
      "('perhaps no picture ever made has more literally <unk> that the road to hell is paved with good intentions .', '3')\n",
      "(\"steers turns in a snappy screenplay that <unk> at the edges ; it 's so clever you want to hate it .\", '3')\n",
      "('but he somehow pulls it off .', '3')\n",
      "('take care of my cat offers a refreshingly different slice of asian cinema .', '3')\n",
      "('this is a film well worth seeing , talking and singing heads and all .', '4')\n",
      "('what really surprises about wisegirls is its low-key quality and genuine tenderness .', '3')\n",
      "('-lrb- wendigo is -rrb- why we go to the cinema : to be <unk> through the eye , the heart , the mind .', '3')\n",
      "('one of the greatest family-oriented , <unk> movies ever .', '4')\n",
      "('ultimately , it <unk> the reasons we need stories so much .', '2')\n",
      "(\"an utterly compelling ` who wrote it ' in which the reputation of the most famous author who ever lived comes into question .\", '3')\n",
      "('illuminating if overly talky documentary .', '2')\n",
      "('a masterpiece four years in the making .', '4')\n",
      "(\"the movie 's ripe , <unk> beauty will <unk> those willing to probe its <unk> mysteries .\", '3')\n",
      "('offers a breath of the fresh air of true sophistication .', '4')\n",
      "('a thoughtful , provocative , insistently humanizing film .', '4')\n"
     ]
    }
   ],
   "source": [
    "for i in range(20):\n",
    "    words, tag = decode_sample(dev[i])\n",
    "    print((\" \".join(words), tag))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5fa0676",
   "metadata": {},
   "source": [
    "And the results of those samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "f79414e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct\tPredicted\n",
      "2 \t 3\n",
      "3 \t 3\n",
      "4 \t 4\n",
      "2 \t 4\n",
      "4 \t 4\n",
      "3 \t 2\n",
      "3 \t 2\n",
      "3 \t 2\n",
      "3 \t 4\n",
      "4 \t 4\n",
      "3 \t 3\n",
      "3 \t 3\n",
      "4 \t 4\n",
      "2 \t 1\n",
      "3 \t 1\n",
      "2 \t 3\n",
      "4 \t 4\n",
      "3 \t 4\n",
      "4 \t 3\n",
      "4 \t 4\n"
     ]
    }
   ],
   "source": [
    "print(\"Correct\\tPredicted\")\n",
    "for i in range(20):\n",
    "    _, tag = decode_sample(dev[i])\n",
    "    scores = calc_scores(dev[i][0]).npvalue()\n",
    "    predict = np.argmax(scores)\n",
    "    print(tag,\"\\t\",i2t[predict])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4615346f",
   "metadata": {},
   "source": [
    "So... okay.  Lots of off-by-1 errors, but this is a subjective task.  What's a 3-star review vs. a 4-star review?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48ea8738",
   "metadata": {},
   "source": [
    "Test accuracy appears to be about 40%.  Is this good?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84d3bd7a",
   "metadata": {},
   "source": [
    "For a 5-class categorization task, it is about 20% better than chance.  That actually is good by most measures."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0067bec1",
   "metadata": {},
   "source": [
    "Let's examine the most predictive words for each class.  I'm going to do this by examining the trained weights.  We'll take the `argmax` across all columns.  We have 18,648 rows, each one representing a word, now weighted according to how strongly they predict each of the five labels.\n",
    "\n",
    "We should expect to see two things:\n",
    "\n",
    "1. The highest weighted word per category should be different from what we observed earlier.\n",
    "\n",
    "2. The weights associated with those words should also be different, and more obviously associated with a particular category."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "d66136bc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1691,   874,    65, 11285,  2603])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.argmax(W_sm.as_array(), axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "029b5a9c",
   "metadata": {},
   "source": [
    "Sure enough, the word indices have changed!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "c7d8b249",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: '3', 1: '4', 2: '2', 3: '1', 4: '0'}\n",
      "0 2603 worst [-1.20813549 -1.99004316 -0.54519242  0.02700192  1.31345379]\n",
      "1 11285 wannabe [-0.8572554  -0.86759192 -0.89146626  1.03033304 -0.50137949]\n",
      "2 65 potential [-0.31248748 -0.41788697  0.87740469 -0.7367509  -0.12538719]\n",
      "3 1691 old-fashioned [ 1.09442759 -0.56487101 -0.62937176 -0.51559848 -1.22022521]\n",
      "4 874 entertaining [ 0.34187451  1.30591071 -0.68952763 -0.88704032 -0.89838171]\n"
     ]
    }
   ],
   "source": [
    "print(i2t)\n",
    "for tag in sorted(t2i.keys()):\n",
    "    highest_weighted_word_idx = np.argmax(W_sm.as_array(), axis=0)[t2i[tag]]\n",
    "    print(tag,highest_weighted_word_idx,i2w[highest_weighted_word_idx], W_sm.as_array()[highest_weighted_word_idx])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bb190f8",
   "metadata": {},
   "source": [
    "\"worst\" is the word most associated with a 0-star review, while \"entertaining\" is most predictive of a 4-star review."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cace0d3",
   "metadata": {},
   "source": [
    "We can look at some of the reivews of a give rating that contain the most-predictive word.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "ede60207",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "jones has delivered a solidly entertaining and moving family drama . 4\n",
      "a wildly entertaining scan of evans ' career . 4\n",
      "manages to be both hugely entertaining and uplifting . 4\n",
      "a terrifically entertaining specimen of spielbergian sci-fi . 4\n",
      "while general audiences might not come away with a greater knowledge of the facts of cuban music , they 'll be treated to an impressive and highly entertaining celebration of its sounds . 4\n",
      "overall , it 's a very entertaining , thought-provoking film with a simple message : god is love . 4\n",
      "like the best 60 minutes exposé , the film -lrb- at 80 minutes -rrb- is actually quite entertaining . 4\n",
      "`` red dragon '' is entertaining . 4\n",
      "what a bewilderingly brilliant and entertaining movie this is . 4\n",
      "an enthralling , entertaining feature . 4\n",
      "richly entertaining and suggestive of any number of metaphorical readings . 4\n",
      "i encourage young and old alike to go see this unique and entertaining twist on the classic whale 's tale -- you wo n't be sorry ! 4\n",
      "this quiet , introspective and entertaining independent is worth seeking . 4\n",
      "a sexy , peculiar and always entertaining costume drama set in renaissance spain , and the fact that it 's based on true events somehow makes it all the more compelling . 4\n",
      "australian filmmaker david flatman uses the huge-screen format to make an old-fashioned nature film that educates viewers with words and pictures while entertaining them . 4\n",
      "a hugely rewarding experience that 's every bit as enlightening , insightful and entertaining as grant 's two best films -- four weddings and a funeral and bridget jones 's diary . 4\n",
      "a moving and solidly entertaining comedy\\/drama that should bolster director and co-writer juan josé campanella 's reputation in the united states . 4\n",
      "a beautiful , entertaining two hours . 4\n",
      "an entertaining british hybrid of comedy , caper thrills and quirky romance . 4\n",
      "a rare and lightly entertaining look behind the curtain that separates comics from the people laughing in the crowd . 4\n",
      "a fresh , entertaining comedy that looks at relationships minus traditional gender roles . 4\n",
      "a deftly entertaining film , smartly played and smartly directed . 4\n",
      "if you 're looking for something new and hoping for something entertaining , you 're in luck . 4\n",
      "hugely entertaining from start to finish , featuring a fall from grace that still leaves shockwaves , it will gratify anyone who has ever suspected hollywood of being overrun by corrupt and hedonistic weasels . 4\n",
      "a thoroughly entertaining comedy that uses grant 's own twist of acidity to prevent itself from succumbing to its own bathos . 4\n",
      "berry 's saucy , full-bodied performance gives this aging series a much needed kick , making `` die another day '' one of the most entertaining bonds in years 4\n",
      "marvelously entertaining and deliriously joyous documentary . 4\n",
      "entirely suspenseful , extremely well-paced and ultimately ... dare i say , entertaining ! 4\n",
      "an enormously entertaining movie , like nothing we 've ever seen before , and yet completely familiar . 4\n",
      "hartley adds enough quirky and satirical touches in the screenplay to keep the film entertaining . 4\n",
      "an immensely entertaining look at some of the unsung heroes of 20th century pop music . 4\n",
      "... one of the most entertaining monster movies in ages ... 4\n"
     ]
    }
   ],
   "source": [
    "t_idx = t2i['4']\n",
    "w_idx = np.argmax(W_sm.as_array(), axis=0)[t_idx]\n",
    "\n",
    "for (ws, t) in [(ws, t) for (ws, t) in train if t == t_idx and w_idx in ws]:\n",
    "    words, tag = decode_sample((ws,t))\n",
    "    print(\" \".join(words), tag)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da189cbf",
   "metadata": {},
   "source": [
    "We can do the same thing by looking at the *least* predictive words per category, which could also be pretty predictive of the *opposite* category(ies)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "8894bc0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: '3', 1: '4', 2: '2', 3: '1', 4: '0'}\n",
      "0 672 best [-0.09597449  1.12240946 -0.15426145 -0.53366077 -2.62302446]\n",
      "1 854 solid [ 0.60165358  0.66754663 -0.50576502 -2.3923192  -2.19553757]\n",
      "2 1332 thoroughly [ 0.43557012 -0.39067206 -1.82458651  0.07248002  0.24447037]\n",
      "3 4569 mess [-2.10412288 -1.9563787  -0.1827624   0.17761146  0.95808285]\n",
      "4 475 ? [-0.66050559 -2.39071202  0.72981125  0.5020591  -0.40074319]\n"
     ]
    }
   ],
   "source": [
    "print(i2t)\n",
    "for tag in sorted(t2i.keys()):\n",
    "    lowest_weighted_word_idx = np.argmin(W_sm.as_array(), axis=0)[t2i[tag]] # using argmin now\n",
    "    print(tag,lowest_weighted_word_idx,i2w[lowest_weighted_word_idx], W_sm.as_array()[lowest_weighted_word_idx])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b404f6e2",
   "metadata": {},
   "source": [
    "So \"best\" is least associated with a 0-star review, and the \"?\" is least associated with a 4-star review, with a weight of ~-2.4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "7e91a60a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a highly spirited , imaginative kid 's movie that broaches neo-augustinian theology : is god stuck in heaven because he 's afraid of his best-known creation ? 4\n",
      "could i have been more geeked when i heard that apollo 13 was going to be released in imax format ? 4\n"
     ]
    }
   ],
   "source": [
    "t_idx = t2i['4']\n",
    "w_idx = np.argmin(W_sm.as_array(), axis=0)[t_idx]\n",
    "\n",
    "for (ws, t) in [(ws, t) for (ws, t) in train if t == t_idx and w_idx in ws]:\n",
    "    words, tag = decode_sample((ws,t))\n",
    "    print(\" \".join(words), tag)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dac1972a",
   "metadata": {},
   "source": [
    "Only a very few 4-star reviews have question marks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "824d166d",
   "metadata": {},
   "outputs": [],
   "source": [
    "t = 2 # searching for the least predictive word for category t\n",
    "t_idx = t2i[str(-t+4)]\n",
    "w_idx = np.argmin(W_sm.as_array(), axis=0)[t2i[str(t)]] # looking for reviews of the opposite category that contain that word\n",
    "\n",
    "for (ws, t) in [(ws, t) for (ws, t) in train if t == t_idx and w_idx in ws]:\n",
    "    words, tag = decode_sample((ws,t))\n",
    "    print(\" \".join(words), tag)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91cdb8b1",
   "metadata": {},
   "source": [
    "A ton of 4-star reviews contain \"best.\"  More than contain \"entertaining,\" actually.  Why is \"entertaining\" more highly weighted for 4-stars?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd86368c",
   "metadata": {},
   "source": [
    "We can also try this for other ratings.  (Switch `t=4` above for another number)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00ebc840",
   "metadata": {},
   "source": [
    "Try `t=2`.  You'll find that few or no 2-star reviews contain the least associated word with category 2 (which may change as we train). Let's look for reviews that contain that word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "89771f9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "this loud and thoroughly obnoxious comedy about a pair of squabbling working-class spouses is a deeply unpleasant experience . 0\n",
      "it 's a hellish , numbing experience to watch , and it does n't offer any insights that have n't been thoroughly debated in the media already , back in the dahmer heyday of the mid - '90s . 1\n",
      "me no lika da accents so good , but i thoroughly enjoyed the love story . 3\n",
      "not only does the thoroughly formulaic film represent totally exemplify middle-of-the-road mainstream , it also represents glossy hollywood at its laziest . 0\n",
      "the film 's thoroughly recycled plot and tiresome jokes ... drag the movie down . 1\n",
      "despite besson 's high-profile name being wasabi 's big selling point , there is no doubt that krawczyk deserves a huge amount of the credit for the film 's thoroughly winning tone . 3\n",
      "a retread of material already thoroughly plumbed by martin scorsese . 1\n",
      "a first-class , thoroughly involving b movie that effectively combines two surefire , beloved genres -- the prison flick and the fight film . 4\n",
      "i ca n't begin to tell you how tedious , how resolutely unamusing , how thoroughly unrewarding all of this is , and what a reckless squandering of four fine acting talents ... 0\n",
      "a bright , inventive , thoroughly winning flight of revisionist fancy . 3\n",
      "cold , pretentious , thoroughly dislikable study in sociopathy . 1\n",
      "old-fashioned but thoroughly satisfying entertainment . 3\n",
      "a thoroughly engaging , surprisingly touching british comedy . 4\n",
      "a thoroughly enjoyable , heartfelt coming-of-age comedy . 3\n",
      "interesting and thoroughly unfaithful version of carmen 3\n",
      "this is standard crime drama fare ... instantly forgettable and thoroughly dull . 0\n",
      "it makes you believe the cast and crew thoroughly enjoyed themselves and believed in their small-budget film . 3\n",
      "thoroughly engrossing and ultimately tragic . 1\n",
      "an incredibly irritating comedy about thoroughly vacuous people ... manages to embody the worst excesses of nouvelle vague without any of its sense of fun or energy . 0\n",
      "thoroughly enjoyable . 3\n",
      "a thoroughly entertaining comedy that uses grant 's own twist of acidity to prevent itself from succumbing to its own bathos . 4\n",
      "director lee has a true cinematic knack , but it 's also nice to see a movie with its heart so thoroughly , unabashedly on its sleeve . 4\n",
      "a thoroughly awful movie -- dumb , narratively chaotic , visually sloppy ... a weird amalgam of ` the thing ' and a geriatric ` scream . ' 0\n",
      "the trouble with making this queen a thoroughly modern maiden is that it also makes her appear foolish and shallow rather than , as was more likely , a victim of mental illness . 1\n",
      "like many western action films , this thriller is too loud and thoroughly overbearing , but its heartfelt concern about north korea 's recent past and south korea 's future adds a much needed moral weight . 3\n",
      "thoroughly awful . 0\n"
     ]
    }
   ],
   "source": [
    "w_idx = np.argmin(W_sm.as_array(), axis=0)[2]\n",
    "\n",
    "for (ws, t) in [(ws, t) for (ws, t) in train if w_idx in ws]:\n",
    "    words, tag = decode_sample((ws,t))\n",
    "    print(\" \".join(words), tag)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89a4e665",
   "metadata": {},
   "source": [
    "So, we can see that this word doesn't appear in many or any) 2-star reviews (average), but appears all over the place in all of the other categories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "61a1053c",
   "metadata": {},
   "outputs": [],
   "source": [
    "lowest_word_2_idx = np.argmin(W_sm.as_array(), axis=0)[2]\n",
    "lowest_word_2 = i2w[np.argmin(W_sm.as_array(), axis=0)[t2i['2']]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de1a9223",
   "metadata": {},
   "source": [
    "So the meaning (or polarity, or sentiment) of this word will depend on what it modifies, or what it cooccurs with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "205cef32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"thoroughly\" + X\n",
      "thoroughly obnoxious 0\n",
      "thoroughly debated 1\n",
      "thoroughly enjoyed 3\n",
      "thoroughly formulaic 0\n",
      "thoroughly recycled 1\n",
      "thoroughly winning 3\n",
      "thoroughly plumbed 1\n",
      "thoroughly involving 4\n",
      "thoroughly unrewarding 0\n",
      "thoroughly winning 3\n",
      "thoroughly dislikable 1\n",
      "thoroughly satisfying 3\n",
      "thoroughly engaging 4\n",
      "thoroughly enjoyable 3\n",
      "thoroughly unfaithful 3\n",
      "thoroughly dull 0\n",
      "thoroughly enjoyed 3\n",
      "thoroughly engrossing 1\n",
      "thoroughly vacuous 0\n",
      "thoroughly enjoyable 3\n",
      "thoroughly entertaining 4\n",
      "thoroughly , 4\n",
      "thoroughly awful 0\n",
      "thoroughly modern 1\n",
      "thoroughly overbearing 3\n",
      "thoroughly awful 0\n"
     ]
    }
   ],
   "source": [
    "print(\"\\\"%s\\\" + X\" % lowest_word_2)\n",
    "for (ws, t) in [(ws, t) for (ws, t) in train if lowest_word_2_idx in ws]:\n",
    "    words, tag = decode_sample((ws,t))\n",
    "    print(\" \".join([words[words.index(lowest_word_2)],words[words.index(lowest_word_2)+1]]), tag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "8e884441",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X + \"thoroughly\"\n",
      "and thoroughly 0\n",
      "been thoroughly 1\n",
      "i thoroughly 3\n",
      "the thoroughly 0\n",
      "'s thoroughly 1\n",
      "'s thoroughly 3\n",
      "already thoroughly 1\n",
      ", thoroughly 4\n",
      "how thoroughly 0\n",
      ", thoroughly 3\n",
      ", thoroughly 1\n",
      "but thoroughly 3\n",
      "a thoroughly 4\n",
      "a thoroughly 3\n",
      "and thoroughly 3\n",
      "and thoroughly 0\n",
      "crew thoroughly 3\n",
      ". thoroughly 1\n",
      "about thoroughly 0\n",
      ". thoroughly 3\n",
      "a thoroughly 4\n",
      "so thoroughly 4\n",
      "a thoroughly 0\n",
      "a thoroughly 1\n",
      "and thoroughly 3\n",
      ". thoroughly 0\n"
     ]
    }
   ],
   "source": [
    "print(\"X + \\\"%s\\\"\" % lowest_word_2)\n",
    "for (ws, t) in [(ws, t) for (ws, t) in train if lowest_word_2_idx in ws]:\n",
    "    words, tag = decode_sample((ws,t))\n",
    "    print(\" \".join([words[words.index(lowest_word_2)-1],words[words.index(lowest_word_2)]]), tag)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24ef7b01",
   "metadata": {},
   "source": [
    "So, if I could capture something about the context a word occurs in, I might capture more about its meaning, and its predictive power."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2253ec54",
   "metadata": {},
   "source": [
    "## Continuous Bag of Words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "899ed76d",
   "metadata": {},
   "source": [
    "The CBOW model architecture tries to predict the current target word (the center word) based on the source context words (surrounding words).\n",
    "\n",
    "e.g., \"Mary had a little #### whose #### was white as snow.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "a0b41b35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the model\n",
    "EMB_SIZE = 64\n",
    "W_emb = model.add_lookup_parameters((nwords, EMB_SIZE)) # Word embeddings\n",
    "W_sm = model.add_parameters((ntags, EMB_SIZE))          # Softmax weights\n",
    "b_sm = model.add_parameters((ntags))                      # Softmax bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "506213d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A function to calculate scores for one value\n",
    "def calc_scores(words):\n",
    "    dy.renew_cg()\n",
    "    cbow = dy.esum([dy.lookup(W_emb, x) for x in words])\n",
    "    W_sm_exp = dy.parameter(W_sm)\n",
    "    b_sm_exp = dy.parameter(b_sm)\n",
    "    return W_sm_exp * cbow + b_sm_exp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "bfd27cf9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 0: train loss/sent=1.4639, time=0.23s\n",
      "iter 0: test acc=0.4059\n",
      "iter 10: train loss/sent=0.0549, time=0.20s\n",
      "iter 10: test acc=0.3751\n",
      "iter 20: train loss/sent=0.0130, time=0.21s\n",
      "iter 20: test acc=0.3756\n",
      "iter 30: train loss/sent=0.0118, time=0.20s\n",
      "iter 30: test acc=0.3629\n",
      "iter 40: train loss/sent=0.0073, time=0.20s\n",
      "iter 40: test acc=0.3602\n",
      "iter 50: train loss/sent=0.0061, time=0.20s\n",
      "iter 50: test acc=0.3674\n",
      "iter 60: train loss/sent=0.0109, time=0.20s\n",
      "iter 60: test acc=0.3525\n",
      "iter 70: train loss/sent=0.0064, time=0.20s\n",
      "iter 70: test acc=0.3697\n",
      "iter 80: train loss/sent=0.0105, time=0.20s\n",
      "iter 80: test acc=0.3579\n",
      "iter 90: train loss/sent=0.0084, time=0.20s\n",
      "iter 90: test acc=0.3606\n"
     ]
    }
   ],
   "source": [
    "epochs = 100\n",
    "for ITER in range(epochs):\n",
    "    # Perform training\n",
    "    random.shuffle(train)\n",
    "    train_loss = 0.0\n",
    "    start = time.time()\n",
    "    for words, tag in train:\n",
    "        my_loss = dy.pickneglogsoftmax(calc_scores(words), tag)\n",
    "        train_loss += my_loss.value()\n",
    "        my_loss.backward()\n",
    "        trainer.update()\n",
    "    if ITER % int(epochs/10) == 0:\n",
    "        print(\"iter %r: train loss/sent=%.4f, time=%.2fs\" % (ITER, train_loss/len(train), time.time()-start))\n",
    "    # Perform testing\n",
    "    test_correct = 0.0\n",
    "    for words, tag in dev:\n",
    "        scores = calc_scores(words).npvalue()\n",
    "        predict = np.argmax(scores)\n",
    "        if predict == tag:\n",
    "            test_correct += 1\n",
    "    if ITER % int(epochs/10) == 0:\n",
    "        print(\"iter %r: test acc=%.4f\" % (ITER, test_correct/len(dev)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "e33f6c68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('effective but <unk> biopic', '2')\n",
      "('if you sometimes like to go to the movies to have fun , wasabi is a good place to start .', '3')\n",
      "(\"emerges as something rare , an issue movie that 's so honest and keenly observed that it does n't feel like one .\", '4')\n",
      "('the film provides some great insight into the neurotic mindset of all comics -- even those who have reached the absolute top of the game .', '2')\n",
      "('offers that rare combination of entertainment and education .', '4')\n",
      "('perhaps no picture ever made has more literally <unk> that the road to hell is paved with good intentions .', '3')\n",
      "(\"steers turns in a snappy screenplay that <unk> at the edges ; it 's so clever you want to hate it .\", '3')\n",
      "('but he somehow pulls it off .', '3')\n",
      "('take care of my cat offers a refreshingly different slice of asian cinema .', '3')\n",
      "('this is a film well worth seeing , talking and singing heads and all .', '4')\n",
      "('what really surprises about wisegirls is its low-key quality and genuine tenderness .', '3')\n",
      "('-lrb- wendigo is -rrb- why we go to the cinema : to be <unk> through the eye , the heart , the mind .', '3')\n",
      "('one of the greatest family-oriented , <unk> movies ever .', '4')\n",
      "('ultimately , it <unk> the reasons we need stories so much .', '2')\n",
      "(\"an utterly compelling ` who wrote it ' in which the reputation of the most famous author who ever lived comes into question .\", '3')\n",
      "('illuminating if overly talky documentary .', '2')\n",
      "('a masterpiece four years in the making .', '4')\n",
      "(\"the movie 's ripe , <unk> beauty will <unk> those willing to probe its <unk> mysteries .\", '3')\n",
      "('offers a breath of the fresh air of true sophistication .', '4')\n",
      "('a thoughtful , provocative , insistently humanizing film .', '4')\n"
     ]
    }
   ],
   "source": [
    "# as a reminder, the first 20 reviews\n",
    "for i in range(20):\n",
    "    words, tag = decode_sample(dev[i])\n",
    "    print((\" \".join(words), tag))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "b1f445b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct\tPredicted\n",
      "2 \t 3\n",
      "3 \t 3\n",
      "4 \t 4\n",
      "2 \t 4\n",
      "4 \t 4\n",
      "3 \t 2\n",
      "3 \t 2\n",
      "3 \t 4\n",
      "3 \t 4\n",
      "4 \t 4\n",
      "3 \t 3\n",
      "3 \t 3\n",
      "4 \t 3\n",
      "2 \t 2\n",
      "3 \t 1\n",
      "2 \t 3\n",
      "4 \t 4\n",
      "3 \t 4\n",
      "4 \t 1\n",
      "4 \t 4\n"
     ]
    }
   ],
   "source": [
    "print(\"Correct\\tPredicted\")\n",
    "for i in range(20):\n",
    "    _, tag = decode_sample(dev[i])\n",
    "    scores = calc_scores(dev[i][0]).npvalue()\n",
    "    predict = np.argmax(scores)\n",
    "    print(tag,\"\\t\",i2t[predict])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ec33b04",
   "metadata": {},
   "source": [
    "Accuracy is about the same, or maybe even a little lower.  What's going on?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "bcf78e75",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5, 64)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "W_sm.as_array().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "2fa257d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([48, 25, 47, 17, 51])"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.argmax(W_sm.as_array(), axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c07a65fa",
   "metadata": {},
   "source": [
    "These numbers no longer refer to which word is most predictive of each class, but to an axis in 64-dimensional space.  According to this model, these *dimensions* are the ones most associated with each class, that capture the most information about that class in a set of **word embeddings**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24312e69",
   "metadata": {},
   "source": [
    "## Word Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b0938c3",
   "metadata": {},
   "source": [
    "Before we get into that, let's look at this thing:\n",
    "```\n",
    "EMB_SIZE = 64\n",
    "W_emb = model.add_lookup_parameters((nwords, EMB_SIZE)) # Word embeddings\n",
    "W_sm = model.add_parameters((ntags, EMB_SIZE))          # Softmax weights\n",
    "b_sm = model.add_parameters((ntags))                      # Softmax bias\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba2b6722",
   "metadata": {},
   "source": [
    "What is `EMB_SIZE`?  Let's take a look at the `W_emb` variable for a clue.  These are lookup parameters, so that means there's a lookup table of parameters associated with some key.  Also the first dimension of this is `nwords` so we can infer that whatever this is, there's one of it for each word."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aed8adc2",
   "metadata": {},
   "source": [
    "Previously, we looked at encoding words as single integers and as one-hot vectors or count vectors.\n",
    "\n",
    "* An issue with single integers is that you have decide what the fixed-size input to your network will be, and either truncate or pad out every input to that length.\n",
    "\n",
    "* An issue with count vectors is that your input has to be the same dimensionality as the number of words in your vocabulary (>45K in the previous notebook, even after removing stopwords!).\n",
    "\n",
    "Neither input format performed very well."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f560c658",
   "metadata": {},
   "source": [
    "This code suggests there's a way to encode something more than just the count of word frequences in a vector of a (relatively) modest size, in this case 64."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "471fd731",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(18648, 64)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "W_emb.as_array().shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99fc8a4e",
   "metadata": {},
   "source": [
    "HW0: Who said this?\n",
    "\n",
    "$$\\text{\"You shall know a word by the company it keeps.\"}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50cd84ef",
   "metadata": {},
   "source": [
    "<img src=\"images/firth.png\" width=300>\n",
    "$$\\text{John Rupert Firth}$$\n",
    "\n",
    "J.R. Firth was a leading British linguist during the 1950s. He is known for having drawn attention to the context-dependent nature of meaning with his notion of \"context of situation.\" His work on **collocational meaning** is widely acknowledged in the field of **distributional semantics**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6da07e30",
   "metadata": {},
   "source": [
    "Firth often doesn't get credit for the above quote, which is often associated with..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b7ca30c",
   "metadata": {},
   "source": [
    "<img src=\"images/wittgenstein.jpg\" width=300>\n",
    "$$\\text{Ludwig Wittgenstein}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fa845f9",
   "metadata": {},
   "source": [
    "Wittgenstein was an Austrian-British philosopher in in logic, philosophy of mathematics, philosophy of mind, and philosophy of language. He is considered to be one of the greatest philosophers of the modern era."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0154e46f",
   "metadata": {},
   "source": [
    "*His* quote is actually...\n",
    "\n",
    "$$\\text{\"Die Bedeutung eines Wortes ist sein Gebrauch in der Sprache.\"}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8b976ff",
   "metadata": {},
   "source": [
    "$$\\text{\"The meaning of a word is its use in the language.\"}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3022c0e2",
   "metadata": {},
   "source": [
    "That is, a speech community agrees on a *conventionalized* meaning of a word (goes back to Aristotle), and that meaning is communicated by how the word is used."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea9f9a85",
   "metadata": {},
   "source": [
    "Lecture 02: Rudolf Carnap (1934) - *Piroten karulieren elatisch* (pirots karulize elatically).\n",
    "\n",
    "These aren't real words, but we can still infer some characteristics of them by their use."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c0b5500",
   "metadata": {},
   "source": [
    "### Bardiwac\n",
    "\n",
    "Courtesy Stefan Evert.\n",
    "\n",
    "What is the meaning of \"bardiwac\"?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4796065",
   "metadata": {},
   "source": [
    "* He handed her a glass of **bardiwac**.\n",
    "* Beef dishes are made to complement the **bardiwacs**.\n",
    "* Nigel staggered to his feet, face flushed from too much **bardiwac**.\n",
    "* Malbec, one of the lesser-known **bardiwac** grapes, responds well to Argentina's sunshine.\n",
    "* I dined off bread and cheese and this excellent **bardiwac**\n",
    "* The drinks were delicious: blood-red **bardiwac** as well as light, sweet Rhenish."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5134ee9",
   "metadata": {},
   "source": [
    "It seems clear the \"bardiwac\" is a kind of red wine."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f584619",
   "metadata": {},
   "source": [
    "(Bardiwac is a made-up word)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2a26f3a",
   "metadata": {},
   "source": [
    "We know it is a kind of red wine from some particular cues: \"glass,\" \"flushed,\" \"grapes,\" \"drinks,\" \"blood-red.\"  Maybe \"beef,\" \"cheese.\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e93104ba",
   "metadata": {},
   "source": [
    "We can also do the reverse.  Let's switch \"bardiwac\" for \"red wine\" for simplicity:\n",
    "\n",
    "* He handed her a ___ of red wine.\n",
    "* ___ dishes are made to complement the red wines.\n",
    "* Nigel staggered to his feet, face ___ from too much red wine.\n",
    "* Malbec, one of the lesser-known red wine ___ , responds well to Argentina's sunshine.\n",
    "* I dined off bread and ___ and this excellent red wine.\n",
    "* The ___ were delicious: blood-red wine as well as light, sweet Rhenish."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07b295ef",
   "metadata": {},
   "source": [
    "This intuition underlies CBOW and other distributional semantic models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79d41d52",
   "metadata": {},
   "source": [
    "Humans do it naturally.  How does a computer do it?\n",
    "\n",
    "We keep a window size of $N$ where we look at $N$ words before and $N$ words after the target word while attempting to predict it."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe12c769",
   "metadata": {},
   "source": [
    "<img src=\"images/cbow.png\" width=400>\n",
    "\n",
    "[\"Distributed Representations of Words and Phrases and their Compositionality\" by Mikolov et al., 2013](https://arxiv.org/pdf/1310.4546.pdf)\n",
    "\n",
    "This is a deep neural architecture where the model tries to predict the target word using the context words as inputs times weights associated with each.\n",
    "\n",
    "After training, each word $w$ has weights with other words, and those weights are optimized so that $w$ times weights for target word $n$ should maximize the likelihood of $w \\cdot W_n$ producing a representation close to $n$.\n",
    "\n",
    "What this means is that words ($w$ and $n$) have to be represented as dense vectors, whose representations in high-dimensional space are optimized for the contexts in which they may appear.  Therefore this family of models is called **`word2vec`**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99f18338",
   "metadata": {},
   "source": [
    "Another model, `Skip-Gram`, is the inverse of CBOW, predicting the context words given a \"target\" word as input.\n",
    "\n",
    "<img src=\"images/skipgram.png\" width=400>\n",
    "\n",
    "Skip-Gram is slower but better for infrequent words, where as CBOW is faster."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a2820a0",
   "metadata": {},
   "source": [
    "Of course, multiple words could fill in a blank:\n",
    "\n",
    "* He handed her a **cup** of red wine.\n",
    "* **Lamb** dishes are made to complement the red wines.\n",
    "* Nigel staggered to his feet, face **hot** from too much red wine.\n",
    "* Malbec, one of the lesser-known red wine **cultivars** , responds well to Argentina's sunshine.\n",
    "* I dined off bread and **butter** and this excellent red wine.\n",
    "* The **libations** were delicious: blood-red wine as well as light, sweet Rhenish."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8686629f",
   "metadata": {},
   "source": [
    "So we should expect the set of words that could fill in a certain context to share some amount of similarity.  And in fact, this is what `word2vec` does."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6c129c9",
   "metadata": {},
   "source": [
    "How would we measure similarity?  Start with a low dimensional example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "325b3476",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define 2 vectors\n",
    "a = np.array([1,0,0])\n",
    "b = np.array([0,1,0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af41c13a",
   "metadata": {},
   "source": [
    "We know these two vectors are orthogonal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "9f29fd56",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-1,  0,  0])"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "-a # this vector is the opposite of a"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f82cad9b",
   "metadata": {},
   "source": [
    "We can measure this for arbitrary vectors using the dot or inner product."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "1e41d39a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0, -1)"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.inner(a, b), np.inner(a, -a) # a and b are orthogonal, a and -a are opposites"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "22de7562",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.707"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c = np.array([.707,.707,0])\n",
    "np.inner(b, c)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae63c8bc",
   "metadata": {},
   "source": [
    "Vectors have to be unit vectors for this to work globally."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "7e9cde7a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.414"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.inner(2*b, c) # can't be \"more aligned\" with c than 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "144b1de1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7071067811865476"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# normalize by the magnitude of the vectors\n",
    "def cos_sim(a,b):\n",
    "    return np.inner(a, b) / (np.linalg.norm(a) * np.linalg.norm(b))\n",
    "\n",
    "cos_sim(2*b,c)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f7e28c0",
   "metadata": {},
   "source": [
    "And we've arrived at cosine similarity:\n",
    "\n",
    "$$\\frac{A \\cdot B}{\\left\\Vert A \\right\\Vert \\left\\Vert B \\right\\Vert}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "28118aa2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(18648, 64)"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "W_emb.as_array().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "27a29526",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.08611389,  0.01851431, -0.05262605, -0.02099194, -0.04370634,\n",
       "       -0.14188324, -0.017655  , -0.04462953, -0.03319036, -0.02941604,\n",
       "        0.03692389, -0.12066798, -0.08824174, -0.03844653, -0.05290723,\n",
       "        0.01239145, -0.12430508, -0.00147147,  0.00381853,  0.0798252 ,\n",
       "        0.01569654, -0.01079613, -0.05492972, -0.01330107,  0.01594659,\n",
       "        0.02186976, -0.04040587, -0.01198792,  0.04421072, -0.04903821,\n",
       "        0.02082936, -0.00529031, -0.049401  , -0.08062241, -0.0752079 ,\n",
       "        0.04765494, -0.05476815, -0.06861457, -0.08632158, -0.04751514,\n",
       "        0.00879235, -0.03884557,  0.03562786, -0.0026878 ,  0.02592279,\n",
       "        0.01779602, -0.05618233,  0.08523858, -0.05233389,  0.07889603,\n",
       "        0.04556897,  0.01376447,  0.00500522,  0.01534181,  0.09122392,\n",
       "        0.09714206,  0.0429404 ,  0.01157522, -0.04845715,  0.05247163,\n",
       "       -0.03264216,  0.07944991, -0.03220316, -0.03381447])"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "W_emb.as_array()[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "5e26a55e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'the'"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "i2w[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "3e8b56ec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(114, 1258, 168, 157, 180)"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# let's look at the cosine similarity of some words\n",
    "w2i[\"fellow\"], w2i[\"man\"], w2i[\"human\"], w2i[\":\"], w2i[\";\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "dfb8dd88",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.09405337, -0.0068545 , -0.02815593,  0.02989283, -0.09907743,\n",
       "       -0.01665347, -0.01357323, -0.02419949,  0.11317193,  0.10726819,\n",
       "        0.08830141,  0.04747124,  0.05087541, -0.08019645,  0.01189666,\n",
       "       -0.09717955, -0.10588466, -0.08156066, -0.05297314,  0.08942267,\n",
       "        0.07363255, -0.09776401,  0.00053687,  0.00920651, -0.05599751,\n",
       "        0.07068352, -0.01897686,  0.10893773,  0.08013274, -0.02662371,\n",
       "       -0.07014231, -0.00766069, -0.00940143, -0.11138509, -0.09017604,\n",
       "       -0.08596055,  0.08659147, -0.03214842,  0.0703975 , -0.06838434,\n",
       "       -0.09638201,  0.08258951, -0.01230963,  0.10603916, -0.05529854,\n",
       "        0.07975018,  0.0421743 , -0.00755919,  0.00239073, -0.0587909 ,\n",
       "        0.01620165, -0.00227596, -0.01859931, -0.05402938, -0.05676281,\n",
       "       -0.01265562,  0.016468  ,  0.0759299 ,  0.08486662,  0.04877271,\n",
       "       -0.0563213 ,  0.06225343,  0.07030861, -0.07700639])"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "W_emb.as_array()[w2i[\"fellow\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "686f264c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-6.77911341e-02,  4.01769811e-03, -3.17093655e-02,  5.36661185e-02,\n",
       "       -4.52745110e-02, -1.84852686e-02,  5.73411435e-02, -7.08420109e-03,\n",
       "        5.40094338e-02,  1.02816783e-02,  7.18663260e-02, -4.82915714e-02,\n",
       "        7.84660205e-02, -5.87395802e-02, -3.15665267e-02, -2.91655883e-02,\n",
       "       -3.28136012e-02, -2.86480691e-02, -5.30125462e-02,  7.62354285e-02,\n",
       "        6.84506968e-02, -6.39826581e-02,  5.78714982e-02, -6.45848587e-02,\n",
       "       -8.84266049e-02,  2.12348010e-02, -5.59912361e-02,  8.21227133e-02,\n",
       "        4.40192688e-03,  8.87897611e-02, -1.76085681e-02, -3.59905176e-02,\n",
       "       -3.02739106e-02, -6.02865294e-02, -8.61913264e-02, -6.38790131e-02,\n",
       "       -1.67747997e-02, -1.17072269e-01,  3.01630050e-02, -3.68305892e-02,\n",
       "       -1.49115734e-03,  4.90807470e-05, -7.51123652e-02,  3.54792960e-02,\n",
       "       -9.10978690e-02,  5.89638874e-02,  3.10947094e-02, -6.40882850e-02,\n",
       "        1.42891416e-02, -6.53395504e-02, -1.40252933e-02, -5.64543009e-02,\n",
       "       -4.04255353e-02,  2.62616556e-02, -8.28849822e-02, -5.18106706e-02,\n",
       "        1.15472488e-02,  6.08056337e-02,  7.59436712e-02,  9.65518430e-02,\n",
       "       -1.68848317e-02,  6.43795282e-02,  4.72818837e-02, -2.17626058e-02])"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "W_emb.as_array()[w2i[\"man\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "e66fd022",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7337833848890636"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cos_sim(W_emb.as_array()[w2i[\"man\"]],W_emb.as_array()[w2i[\"human\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "6fde5884",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6706520930367287"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cos_sim(W_emb.as_array()[w2i[\"man\"]],W_emb.as_array()[w2i[\"fellow\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "38ca31be",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6639332333478192"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cos_sim(W_emb.as_array()[w2i[\"woman\"]],W_emb.as_array()[w2i[\"human\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "fba917c7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.44172229430404236"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cos_sim(W_emb.as_array()[w2i[\"human\"]],W_emb.as_array()[w2i[\"dog\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "967e21bd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.5836905371490316"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cos_sim(W_emb.as_array()[w2i[\"man\"]],W_emb.as_array()[w2i[\":\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "eb807ed4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.2106923557138087"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cos_sim(W_emb.as_array()[w2i[\";\"]],W_emb.as_array()[w2i[\":\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "42c91691",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.012906475215348689"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cos_sim(W_emb.as_array()[w2i[\"the\"]],W_emb.as_array()[w2i[\"<unk>\"]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a466c98",
   "metadata": {},
   "source": [
    "So words that denote humans are more similar to each other than they are to animals or punctuation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "9e1cdbc4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0467723579013554"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cos_sim(W_emb.as_array()[w2i[\"woman\"]],W_emb.as_array()[w2i[\"refrigerator\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "6dae40c5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0467723579013554"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cos_sim(W_emb.as_array()[w2i[\"woman\"]],W_emb.as_array()[w2i[\"argentina\"]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17ce7eb8",
   "metadata": {},
   "source": [
    "So a woman is as similar to a refrigerator as she is to Argentina?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "c49849a5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0, 0, 0)"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# actually, these words just aren't in the vocabulary\n",
    "w2i[\"refrigerator\"],w2i[\"argentina\"],w2i[\"<unk>\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa2434ef",
   "metadata": {},
   "source": [
    "If I maintain just one vector for unknown word, any out of vocabulary item will be treated the same way.  I really want a resource where I can get vectors for a larger vocabulary.  Luckily `word2vec` and other models can be downloaded using `gensim`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "8b9ac224",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gensim in /s/parsons/a/fac/nkrishna/anaconda3/lib/python3.9/site-packages (4.2.0)\n",
      "Requirement already satisfied: scipy>=0.18.1 in /s/parsons/a/fac/nkrishna/anaconda3/lib/python3.9/site-packages (from gensim) (1.7.1)\n",
      "Requirement already satisfied: smart-open>=1.8.1 in /s/parsons/a/fac/nkrishna/anaconda3/lib/python3.9/site-packages (from gensim) (6.2.0)\n",
      "Requirement already satisfied: numpy>=1.17.0 in /s/parsons/a/fac/nkrishna/anaconda3/lib/python3.9/site-packages (from gensim) (1.20.3)\n"
     ]
    }
   ],
   "source": [
    "!{sys.executable} -m pip install gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "7f6e3e1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data loaded in 41.8597 seconds\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "from gensim.models.keyedvectors import KeyedVectors\n",
    "model = KeyedVectors.load_word2vec_format(\"data/GoogleNews-vectors-negative300.bin\", binary=True)\n",
    "print(\"Data loaded in %.4f seconds\" % (time.time()-start_time,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "0aea6a8c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-1.76757812e-01, -2.11914062e-01,  2.01171875e-01,  5.58593750e-01,\n",
       "       -1.53320312e-01, -1.44531250e-01, -1.95312500e-02, -3.80859375e-01,\n",
       "       -7.12890625e-02, -1.92871094e-02, -1.95312500e-01, -2.79296875e-01,\n",
       "       -3.59375000e-01, -1.33789062e-01,  2.04101562e-01,  1.79443359e-02,\n",
       "       -1.27929688e-01,  1.10839844e-01, -8.05664062e-02, -7.51953125e-02,\n",
       "       -3.08837891e-02, -1.49414062e-01, -2.26562500e-01,  7.71484375e-02,\n",
       "       -4.17968750e-01,  3.34472656e-02, -1.88476562e-01,  4.83398438e-02,\n",
       "       -2.00195312e-01, -1.00097656e-01,  2.35351562e-01,  8.78906250e-02,\n",
       "        1.33789062e-01, -1.81640625e-01, -1.08398438e-01,  1.06445312e-01,\n",
       "       -2.50000000e-01, -6.88476562e-02,  2.81250000e-01,  3.32031250e-02,\n",
       "       -9.64355469e-03, -2.77343750e-01,  2.44140625e-01,  3.78417969e-02,\n",
       "        1.41601562e-01,  1.62109375e-01,  1.05468750e-01, -1.54296875e-01,\n",
       "       -2.49023438e-02,  2.12402344e-02, -1.86523438e-01,  1.09863281e-01,\n",
       "        3.02734375e-01, -1.05957031e-01, -6.10351562e-02, -7.42187500e-02,\n",
       "       -1.09863281e-01,  5.05371094e-02,  9.37500000e-02,  8.44726562e-02,\n",
       "       -4.72656250e-01, -5.22460938e-02, -4.98046875e-02, -1.68945312e-01,\n",
       "       -1.42822266e-02, -4.82421875e-01,  2.50000000e-01, -3.39843750e-01,\n",
       "       -2.59765625e-01,  2.10937500e-01,  3.02734375e-01,  1.31835938e-01,\n",
       "       -6.05468750e-02,  7.32421875e-02,  4.49218750e-02,  3.95507812e-02,\n",
       "        6.98242188e-02, -3.51562500e-02, -3.49609375e-01,  1.09375000e-01,\n",
       "       -3.16406250e-01, -5.49316406e-02,  7.86132812e-02, -5.46875000e-01,\n",
       "        2.18505859e-02,  1.19140625e-01, -1.25976562e-01,  1.84570312e-01,\n",
       "       -1.19140625e-01,  1.27929688e-01, -1.06811523e-02, -1.04492188e-01,\n",
       "        1.00097656e-01, -7.47680664e-04, -1.05957031e-01,  4.17968750e-01,\n",
       "        3.17382812e-02, -1.02539062e-01,  2.41210938e-01,  1.29882812e-01,\n",
       "       -1.32812500e-01,  3.97949219e-02, -2.28271484e-02,  1.19018555e-02,\n",
       "        1.02539062e-01,  1.01928711e-02, -1.43554688e-01,  1.52343750e-01,\n",
       "        4.32128906e-02, -2.94921875e-01, -3.80859375e-02, -1.63085938e-01,\n",
       "       -2.05993652e-03, -1.88476562e-01,  1.12792969e-01,  2.04101562e-01,\n",
       "        3.00292969e-02, -1.01074219e-01, -8.83789062e-02, -2.31933594e-03,\n",
       "       -4.39453125e-02,  8.36181641e-03, -5.44433594e-02,  2.63671875e-02,\n",
       "        5.66406250e-01,  1.86523438e-01,  6.22558594e-02, -1.85394287e-03,\n",
       "        3.39843750e-01,  1.65039062e-01,  3.80859375e-02, -1.98974609e-02,\n",
       "        1.30615234e-02,  2.10937500e-01, -8.74023438e-02, -2.42919922e-02,\n",
       "       -4.88281250e-01, -3.59375000e-01,  1.36718750e-01,  2.03125000e-01,\n",
       "        2.92968750e-01, -4.12109375e-01, -7.61718750e-02, -1.15722656e-01,\n",
       "        1.72851562e-01, -1.25000000e-01, -1.34765625e-01, -6.44531250e-02,\n",
       "        5.85937500e-02, -2.30468750e-01,  5.35156250e-01, -3.10546875e-01,\n",
       "       -3.00781250e-01, -1.22070312e-01, -1.47460938e-01, -3.24218750e-01,\n",
       "        1.89453125e-01,  5.54687500e-01, -3.65234375e-01,  1.23046875e-01,\n",
       "        2.36328125e-01,  1.00585938e-01, -3.61328125e-02,  1.69921875e-01,\n",
       "        1.10839844e-01, -1.56250000e-01,  2.00195312e-01,  6.88476562e-02,\n",
       "       -3.12500000e-01,  3.63769531e-02, -4.94140625e-01, -4.02832031e-02,\n",
       "        7.99560547e-03,  4.24804688e-02,  3.61328125e-02,  1.58203125e-01,\n",
       "        1.34765625e-01, -3.02734375e-01,  2.29492188e-01, -1.10351562e-01,\n",
       "        6.25000000e-02, -3.14453125e-01,  7.95898438e-02,  6.98242188e-02,\n",
       "       -4.85839844e-02,  1.77734375e-01, -2.16796875e-01,  9.96093750e-02,\n",
       "        9.76562500e-02,  1.67968750e-01,  1.20117188e-01,  2.53906250e-01,\n",
       "        8.39843750e-02, -2.84423828e-02,  1.64062500e-01,  2.48046875e-01,\n",
       "       -1.82617188e-01, -5.76171875e-02,  2.47802734e-02, -3.34472656e-02,\n",
       "       -2.87109375e-01,  1.93359375e-01,  8.74023438e-02, -2.37304688e-01,\n",
       "       -1.06445312e-01,  2.51953125e-01,  6.83593750e-02, -5.85937500e-02,\n",
       "       -7.37304688e-02,  4.66796875e-01,  1.39648438e-01,  2.38037109e-02,\n",
       "       -1.66015625e-01, -1.23535156e-01, -3.71093750e-01,  1.60156250e-01,\n",
       "        1.60156250e-01, -1.48437500e-01, -1.09375000e-01, -1.67236328e-02,\n",
       "        8.91113281e-03, -1.61132812e-01,  1.05468750e-01,  1.18652344e-01,\n",
       "        2.36328125e-01, -8.97216797e-03,  2.18750000e-01,  1.80664062e-01,\n",
       "       -1.62353516e-02,  5.93261719e-02, -2.33398438e-01, -4.05273438e-02,\n",
       "        4.83398438e-02, -1.40625000e-01, -3.02734375e-01,  3.04687500e-01,\n",
       "        9.42382812e-02, -1.30859375e-01,  1.45874023e-02,  3.73046875e-01,\n",
       "        2.90527344e-02,  1.08032227e-02, -4.24804688e-02,  3.30078125e-01,\n",
       "       -1.55273438e-01,  1.31835938e-01, -4.00390625e-02,  2.81250000e-01,\n",
       "       -2.92968750e-01, -1.60156250e-01,  4.08203125e-01,  1.74804688e-01,\n",
       "        4.10156250e-01,  7.03125000e-02,  7.32421875e-02, -4.98046875e-02,\n",
       "       -8.69140625e-02,  2.05078125e-02,  5.41992188e-02, -2.30468750e-01,\n",
       "        4.12597656e-02,  1.64062500e-01,  2.89062500e-01, -1.69921875e-01,\n",
       "        1.98242188e-01,  2.08740234e-02,  7.08007812e-02, -2.47802734e-02,\n",
       "        1.37939453e-02, -2.69531250e-01, -4.54101562e-02, -5.83496094e-02,\n",
       "        5.34667969e-02, -1.49414062e-01, -4.27246094e-02, -4.00390625e-02,\n",
       "        1.43051147e-04, -1.66015625e-01, -3.32031250e-01,  3.02734375e-01,\n",
       "       -8.11767578e-03, -2.65625000e-01,  2.22167969e-02,  2.36328125e-01,\n",
       "       -1.31835938e-01, -2.08984375e-01,  1.75781250e-01,  5.15747070e-03,\n",
       "       -1.27929688e-01,  5.81054688e-02,  2.37304688e-01,  2.71484375e-01,\n",
       "       -1.71875000e-01, -9.81445312e-02, -3.29589844e-02, -4.76074219e-02,\n",
       "       -1.35742188e-01, -2.29492188e-01, -2.77099609e-02,  6.98242188e-02],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model['colorado'] # get the word vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "1875858a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(300,)"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model[\"colorado\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "112505c0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('utah', 0.6840435862541199),\n",
       " ('minnesota', 0.6742897629737854),\n",
       " ('arkansas', 0.6606869697570801),\n",
       " ('tennessee', 0.6530125141143799),\n",
       " ('oklahoma', 0.6516874432563782),\n",
       " ('delaware', 0.6417563557624817),\n",
       " ('nevada', 0.6413941383361816),\n",
       " ('michigan', 0.6407091021537781),\n",
       " ('denver', 0.6375481486320496),\n",
       " ('texas', 0.6373345255851746)]"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.most_similar(\"colorado\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "1798c954",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6840437"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cos_sim(model[\"colorado\"],model[\"utah\"]) # w2v default similarity metric is cosine similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dad7a073",
   "metadata": {},
   "source": [
    "Let's do the canonical word2vec example: `king`-`man`+`woman` = `queen`.  Rather than try to parse a 300 dimensional vector, we'll use cosine similarity to assess the \"meaning\" of a vector relative to other vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "9e529fc3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.6510957, 0.22942673, 0.16658203, 0.3161814, 0.76640123)"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cos_sim(model[\"king\"],model[\"queen\"]), \\\n",
    "cos_sim(model[\"king\"],model[\"man\"]), \\\n",
    "cos_sim(model[\"queen\"],model[\"man\"]), \\\n",
    "cos_sim(model[\"queen\"],model[\"woman\"]), \\\n",
    "cos_sim(model[\"man\"],model[\"woman\"]) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15770956",
   "metadata": {},
   "source": [
    "So \"king\" and \"queen\" are about as similar to each other as \"man\" and \"woman\" are, and the royals and non-royals are about as dissimilar to each other regardless of gender (within a $cos\\theta$ of ~.1, which is a rotational distance of about 6 degrees)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "46e1f8ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.73005176"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cos_sim(model[\"king\"]-model[\"man\"]+model[\"woman\"],model[\"queen\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7cd7329",
   "metadata": {},
   "source": [
    "Ooh, so `king-man+woman` is closer to \"queen\" now.  About as close as \"man\" and \"woman\" were to begin with!\n",
    "\n",
    "`word2vec`, CBOW or Skip-Gram methods can ***analogize*** between words.  BOW or integer-based approaches can't possibly do this!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "9f78f298",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('king', 0.8449392318725586),\n",
       " ('queen', 0.7300517559051514),\n",
       " ('monarch', 0.645466148853302),\n",
       " ('princess', 0.6156251430511475),\n",
       " ('crown_prince', 0.5818676352500916),\n",
       " ('prince', 0.5777117609977722),\n",
       " ('kings', 0.5613664388656616),\n",
       " ('sultan', 0.5376776456832886),\n",
       " ('Queen_Consort', 0.5344247221946716),\n",
       " ('queens', 0.5289887189865112)]"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.most_similar(model[\"king\"]-model[\"man\"]+model[\"woman\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e23a2544",
   "metadata": {},
   "source": [
    "Dirty secret: these movements in high-dimensional space take a lot to move a vector very far from the original vector.  That is, \"king\", and \"man\"/\"woman\" were already more aligned than they were not (cosine similarity >0), so the amount of motion is overall not enormous, relatively speaking.  When processing word2vec outputs, an assumption is made of change, so that because \"king\" was in the input, it would be discarded from the output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "dbd3ffe7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('king', 0.9999999403953552),\n",
       " ('kings', 0.7138045430183411),\n",
       " ('queen', 0.6510956287384033),\n",
       " ('monarch', 0.6413194537162781),\n",
       " ('crown_prince', 0.6204219460487366),\n",
       " ('prince', 0.6159993410110474),\n",
       " ('sultan', 0.5864822864532471),\n",
       " ('ruler', 0.5797566771507263),\n",
       " ('princes', 0.5646552443504333),\n",
       " ('Prince_Paras', 0.5432944297790527)]"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.most_similar(model[\"king\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "674f7799",
   "metadata": {},
   "source": [
    "\"queen\" was already pretty darn close to \"king\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "ec95a962",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.7781848,\n",
       " [('cat', 0.8460758924484253),\n",
       "  ('puppy', 0.8246198296546936),\n",
       "  ('kitten', 0.7781848311424255),\n",
       "  ('puppies', 0.7319909930229187),\n",
       "  ('pup', 0.7162657976150513),\n",
       "  ('kittens', 0.700151801109314),\n",
       "  ('cats', 0.6890696287155151),\n",
       "  ('tabby', 0.6375119090080261),\n",
       "  ('kitties', 0.6353012323379517),\n",
       "  ('feline', 0.6320980191230774)])"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cos_sim(model[\"puppy\"]-model[\"dog\"]+model[\"cat\"],model[\"kitten\"]),\\\n",
    "model.most_similar(model[\"puppy\"]-model[\"dog\"]+model[\"cat\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b67c6445",
   "metadata": {},
   "source": [
    "Discard \"cat\" and \"puppy\" (both part of input), and the model can analogize `\"dog\" : \"puppy\" :: \"cat\" : \"kitten\"`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "2f25dfa7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('human', 0.7887089848518372),\n",
       " ('human_beings', 0.49892309308052063),\n",
       " ('employee_Laura_Althouse', 0.4492689371109009),\n",
       " ('humankind', 0.4466935098171234),\n",
       " ('impertinent_flamboyant_endearingly', 0.4449778199195862),\n",
       " ('humanity', 0.43886151909828186),\n",
       " ('humans', 0.4374164640903473),\n",
       " ('humanness', 0.431431382894516),\n",
       " ('Human', 0.4300132989883423),\n",
       " ('fertilized_embryo', 0.42177990078926086)]"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.most_similar(model[\"puppy\"]-model[\"dog\"]+model[\"human\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "146b0622",
   "metadata": {},
   "source": [
    "`\"dog\" : \"puppy\" :: \"human\" : \"employee Laura Althouse\"`.  It's not perfect."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc5f3a80",
   "metadata": {},
   "source": [
    "## Building Your Own Word Vectors\n",
    "\n",
    "a.k.a. How to get a jump on PA3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "aaaf42b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assume this small dataset:\n",
    "\n",
    "sim_data = [\"men feed dogs\",\n",
    "\"women feed dogs\",\n",
    "\"women feed men\",\n",
    "\"men feed men\",\n",
    "\"dogs bite men\",\n",
    "\"dogs bite women\",\n",
    "\"dogs bite dogs\",\n",
    "\"dogs like men\",\n",
    "\"men like women\",\n",
    "\"women like dogs\",\n",
    "\"men like dogs\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "4a11b84a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'men': 0, 'like': 1, 'women': 2, 'bite': 3, 'feed': 4, 'dogs': 5},\n",
       " {0: 'men', 1: 'like', 2: 'women', 3: 'bite', 4: 'feed', 5: 'dogs'})"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# count individual words\n",
    "\n",
    "voc = list(set([item for sublist in [s.split() for s in sim_data] for item in sublist]))\n",
    "voc_dict = {voc[i]:i for i in range(len(voc))}\n",
    "inv_voc_dict = {i:voc[i] for i in range(len(voc))}\n",
    "voc_dict, inv_voc_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "24b8fbbd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6, 6)"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# compute cooccurrence matrix\n",
    "\n",
    "cm = np.zeros((len(voc),len(voc)))\n",
    "cm.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "cfa8b46f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 3., 0., 1., 3., 0.],\n",
       "       [3., 0., 2., 0., 0., 3.],\n",
       "       [0., 2., 0., 1., 2., 0.],\n",
       "       [1., 0., 1., 0., 0., 3.],\n",
       "       [3., 0., 2., 0., 0., 2.],\n",
       "       [0., 3., 0., 3., 2., 0.]])"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for w in voc:\n",
    "    for c in voc:\n",
    "        if w != c:\n",
    "            for sent in sim_data:\n",
    "                if w in sent.split() and c in sent.split()\\\n",
    "                    and abs(sent.split().index(w)-sent.split().index(c)) == 1:\n",
    "                    cm[voc_dict[w],voc_dict[c]] += 1\n",
    "cm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8a50af5",
   "metadata": {},
   "source": [
    "Each row is now a vector representing its coocurrence with other vocabulary items."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "b52f7bb9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1., 31.,  1., 11., 31.,  1.],\n",
       "       [31.,  1., 21.,  1.,  1., 31.],\n",
       "       [ 1., 21.,  1., 11., 21.,  1.],\n",
       "       [11.,  1., 11.,  1.,  1., 31.],\n",
       "       [31.,  1., 21.,  1.,  1., 21.],\n",
       "       [ 1., 31.,  1., 31., 21.,  1.]])"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# inflate counts and smooth (you wouldn't need to inflate with appropriately-sized data)\n",
    "cm *= 10\n",
    "cm += 1\n",
    "cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "7de9c77c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([31.,  1., 21.,  1.,  1., 21.])"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get a vector\n",
    "\n",
    "cm[voc_dict[\"feed\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "b5f28f97",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9837843098879014"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cos_sim(cm[voc_dict[\"feed\"]],cm[voc_dict[\"like\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "b02126d1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.07512494944449179"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cos_sim(cm[voc_dict[\"feed\"]],cm[voc_dict[\"men\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "3063a923",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9939572834837749"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cos_sim(cm[voc_dict[\"women\"]],cm[voc_dict[\"men\"]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13eda14f",
   "metadata": {},
   "source": [
    "The verbs and the nouns are more similar to each other than they are to the other class!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa5ad1ab",
   "metadata": {},
   "source": [
    "With a non-toy vocabulary it's a little more involved than *just* creating a cooccurrence matrix, but that's the first step.  One technique is to use positive pointwise mutual information (PPMI):\n",
    "\n",
    "$$PPMI(w,c) = max\\left(log\\left(\\frac{P(w,c)}{P(w)P(c)}\\right),0\\right)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "016cf4dc",
   "metadata": {},
   "source": [
    "Once weighted by PPMI, your word vectors can be decomposed using SVD, and be projected into lower dimensionalities in abstract semantic space."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7aed2872",
   "metadata": {},
   "source": [
    "So deep learning is not the only way to train word vectors, but is perhaps the most eye-catching!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dd35899",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
