{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CS542 Fall 2021 Programming Assignment 2\n",
    "# Logistic Regression Classifier\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "from math import ceil\n",
    "from random import Random\n",
    "from collections import Counter\n",
    "\n",
    "'''\n",
    "Computes the logistic function.\n",
    "'''\n",
    "def sigma(z):\n",
    "    return 1 / (1 + np.exp(-z))\n",
    "\n",
    "class LogisticRegression():\n",
    "\n",
    "    def __init__(self, n_features=2):\n",
    "\n",
    "        # holds the class labels for each document\n",
    "        self.class_dict = {'neg': 0, 'pos': 1}\n",
    "\n",
    "        # Added this to hold the common words and bigrams for use in featurize\n",
    "        self.negatives, self.positives = self.generate_common_words(1000)\n",
    "        self.negative_bigrams, self.positive_bigrams = self.generate_common_bigrams(1000)\n",
    "\n",
    "        # Added this to hold words that may indicate the conclusion of a review (e.g. \"in conclusion\", \"to summarize\", etc.)\n",
    "        self.conclusive_words = []\n",
    "\n",
    "        # self.feature_dict holds the index of each feature in my feature vector\n",
    "        self.feature_dict = {'num_pos_words': 0, 'num_neg_words': 1, 'num_pos_words_conclusion': 2, 'num_neg_words_conclusion': 3, 'num_pos_bigrams': 4, 'num_neg_bigrams': 5, 'num_pos_bigrams_conclusion': 6, 'num_neg_bigrams_conclusion': 7}\n",
    "\n",
    "\n",
    "        self.n_features = n_features\n",
    "        self.theta = np.zeros(n_features + 1) # weights (and bias)\n",
    "\n",
    "    '''\n",
    "    Loads a dataset. Specifically, returns a list of filenames, and dictionaries\n",
    "    of classes and documents such that:\n",
    "    classes[filename] = class of the document\n",
    "    documents[filename] = feature vector for the document (use self.featurize)\n",
    "    '''\n",
    "    def load_data(self, data_set):\n",
    "        filenames = []\n",
    "        classes = dict()\n",
    "        documents = dict()\n",
    "\n",
    "        # iterate over documents\n",
    "        for root, dirs, files in os.walk(data_set):\n",
    "            for name in files:\n",
    "                with open(os.path.join(root, name)) as f:\n",
    "                    # your code here\n",
    "                    # BEGIN STUDENT CODE\n",
    "                    filenames.append(name) \n",
    "                    classes[name] = self.class_dict[os.path.basename(root)] # store class of document with filename as key and class as index\n",
    "                    document_words = f.read().split() # read in document into list of words\n",
    "                    documents[name] = self.featurize(document_words)  # send a doc as a list of words to be featurized\n",
    "                    # END STUDENT CODE\n",
    "        return filenames, classes, documents\n",
    "\n",
    "    # BEGIN STUDENT CODE\n",
    "    ''' \n",
    "    This function finds the most common words in the training set and returns them as a list of strings\n",
    "    It returns two lists, one for negative reviews and one for positive reviews\n",
    "    '''\n",
    "    def generate_common_words(self, top_n_words, path=\"movie_reviews/train\"):\n",
    "        # Initialize counters for negative and positive reviews to hold count of each word occurance\n",
    "        # I cuold have made my own counter but this is easier\n",
    "        negative_counter, positive_counter = Counter(), Counter()\n",
    "\n",
    "        # Open and iterate through negative reviews adding words to the counter\n",
    "        for root, dirs, files in os.walk(os.path.join(path, \"neg\")):\n",
    "            for name in files:\n",
    "                with open(os.path.join(root, name), 'r') as f:\n",
    "                    # Filter out words that are non-letter characters (this helped a lot)\n",
    "                    words = [word for word in f.read().split() if word.isalpha()] # this grabs the list of all words that are alphabetic\n",
    "                    negative_counter.update(words)  # this sums the count of each word in the list\n",
    "\n",
    "        # Does the same as above but for positive reviews\n",
    "        for root, dirs, files in os.walk(os.path.join(path, \"pos\")):\n",
    "            for name in files:\n",
    "                with open(os.path.join(root, name), 'r') as f:\n",
    "                    # Filter out words that are non-letter characters\n",
    "                    words = [word for word in f.read().split() if word.isalpha()]\n",
    "                    positive_counter.update(words)\n",
    "\n",
    "        # Identify words that appear in both positive and negative reviews\n",
    "        # I did this to remove words that are common to both positive and negative reviews\n",
    "        # This way the words in the lists should be indicative of pos or neg\n",
    "        common_words = set(negative_counter.keys()).intersection(set(positive_counter.keys()))\n",
    "\n",
    "        # Remove common words from the counters\n",
    "        for word in common_words:\n",
    "            del negative_counter[word]  \n",
    "            del positive_counter[word]  \n",
    "\n",
    "        # Get top N words from each counter\n",
    "        negatives = [item[0] for item in negative_counter.most_common(top_n_words)]\n",
    "        positives = [item[0] for item in positive_counter.most_common(top_n_words)]\n",
    "\n",
    "        return negatives, positives\n",
    "    # END STUDENT CODE\n",
    "\n",
    "    # BEGIN STUDENT CODE\n",
    "    ''' \n",
    "    This function finds the most common bigrams in the training set. I am trying to increase accuracy of the classifier.\n",
    "    This is pretty much the same as above except \n",
    "    '''\n",
    "\n",
    "    def generate_common_bigrams(self, top_n_bigrams, path=\"movie_reviews/train\"):\n",
    "        negative_counter, positive_counter = Counter(), Counter()\n",
    "\n",
    "        # Helper function to extract bigrams from a text\n",
    "        def extract_bigrams(text):\n",
    "            # Split the text into words\n",
    "            words = text.split()\n",
    "\n",
    "            # Only include bigrams where both words are alphabetic\n",
    "            bigrams = []\n",
    "            for i in range(len(words) - 1):\n",
    "                word1 = words[i]\n",
    "                word2 = words[i + 1]\n",
    "                if word1.isalpha() and word2.isalpha():\n",
    "                    bigrams.append((word1, word2))\n",
    "            return bigrams\n",
    "\n",
    "        # Iterate through negative reviews\n",
    "        for root, dirs, files in os.walk(os.path.join(path, \"neg\")):\n",
    "            for name in files:\n",
    "                with open(os.path.join(root, name), 'r') as f:\n",
    "                    bigrams = extract_bigrams(f.read())\n",
    "                    negative_counter.update(bigrams)\n",
    "\n",
    "        # Iterate through positive reviews\n",
    "        for root, dirs, files in os.walk(os.path.join(path, \"pos\")):\n",
    "            for name in files:\n",
    "                with open(os.path.join(root, name), 'r') as f:\n",
    "                    bigrams = extract_bigrams(f.read())\n",
    "                    positive_counter.update(bigrams)\n",
    "\n",
    "        # Identify bigrams that appear in both positive and negative reviews\n",
    "        common_bigrams = set(negative_counter.keys()).intersection(set(positive_counter.keys()))\n",
    "\n",
    "        # Remove common bigrams from the counters\n",
    "        for bigram in common_bigrams:\n",
    "            del negative_counter[bigram]\n",
    "            del positive_counter[bigram]\n",
    "\n",
    "        # Get top N bigrams from each counter\n",
    "        negative_bigrams = [item[0] for item in negative_counter.most_common(top_n_bigrams)]\n",
    "        positive_bigrams = [item[0] for item in positive_counter.most_common(top_n_bigrams)]\n",
    "\n",
    "        return negative_bigrams, positive_bigrams\n",
    "    # END STUDENT CODE\n",
    "\n",
    "\n",
    "    '''\n",
    "    Given a document (as a list of words), returns a feature vector.\n",
    "    Note that the last element of the vector, corresponding to the bias, is a\n",
    "    \"dummy feature\" with value 1.\n",
    "    '''\n",
    "    def featurize(self, document):\n",
    "        vector = np.zeros(self.n_features + 1)\n",
    "        # BEGIN STUDENT CODE\n",
    "        # count all neg and pos words in document\n",
    "        ''' \n",
    "        So the thought process here was originally to check if the last two sentences of each doc was reached\n",
    "        and if so, the check if any conclusive words were hit. If they were, then I would weight the sentiment hits\n",
    "        higher since they may be more indicative of the overall sentiment of the review.\n",
    "\n",
    "        I didn't want to add any return functions to the featurize function so I instead of checking for the last two\n",
    "        sentences, I just check for the conclusive words and then set a flag to true which prob isn't as good... but oh well.\n",
    "        '''\n",
    "        conclusive_word_reached = False\n",
    "        for word in document:\n",
    "            if word in self.conclusive_words and not conclusive_word_reached:\n",
    "                conclusive_word_reached = True\n",
    "                continue \n",
    "            if word in self.negatives:\n",
    "                if conclusive_word_reached:\n",
    "                    vector[self.feature_dict['num_neg_words_conclusion']] += 2  # weight words in conclusion higher\n",
    "                vector[self.feature_dict['num_neg_words']] += 1\n",
    "            elif word in self.positives:\n",
    "                if conclusive_word_reached:\n",
    "                    vector[self.feature_dict['num_pos_words_conclusion']] += 2  # weight words in conclusion higher\n",
    "                vector[self.feature_dict['num_pos_words']] += 1\n",
    "\n",
    "        # count all neg and pos bigrams in document\n",
    "        for i in range(len(document) - 1):\n",
    "            bigram = (document[i], document[i + 1])\n",
    "            if bigram in self.negative_bigrams:\n",
    "                if conclusive_word_reached:\n",
    "                    vector[self.feature_dict['num_neg_bigrams_conclusion']] += 10 # weight bigrams in conclusion much higher\n",
    "                vector[self.feature_dict['num_neg_bigrams']] += 5\n",
    "            elif bigram in self.positive_bigrams:\n",
    "                if conclusive_word_reached:\n",
    "                    vector[self.feature_dict['num_pos_bigrams_conclusion']] += 10 # weight bigrams in conclusion much higher\n",
    "                vector[self.feature_dict['num_pos_bigrams']] += 5\n",
    "\n",
    "        # NOTE: I would like to normalize this vector for good practice but I will keep it as is for this assignment.\n",
    "        # END STUDENT CODE\n",
    "        \n",
    "        vector[-1] = 1\n",
    "        return vector\n",
    "\n",
    "    '''\n",
    "    Trains a logistic regression classifier on a training set.\n",
    "    '''\n",
    "    def train(self, train_set, batch_size=3, n_epochs=1, eta=0.1):\n",
    "        filenames, classes, documents = self.load_data(train_set)\n",
    "        filenames = sorted(filenames)\n",
    "        n_minibatches = ceil(len(filenames) / batch_size)\n",
    "        for epoch in range(n_epochs):\n",
    "            print(\"Epoch {:} out of {:}\".format(epoch + 1, n_epochs))\n",
    "            loss = 0\n",
    "            for i in range(n_minibatches):\n",
    "                # list of filenames in minibatch\n",
    "                minibatch = filenames[i * batch_size: (i + 1) * batch_size]\n",
    "                # BEGIN STUDENT CODE\n",
    "                # create and fill in matrix x and vector y\n",
    "                # Initialize matrix X and vector Y for the minibatch\n",
    "                X = np.zeros((len(minibatch), self.n_features + 1))\n",
    "                Y = np.zeros(len(minibatch))\n",
    "                \n",
    "                # Fill in X and Y with each files vector and class info\n",
    "                for i, name in enumerate(minibatch):\n",
    "                    X[i] = documents[name]\n",
    "                    Y[i] = classes[name]\n",
    "\n",
    "                # compute y_hat\n",
    "\n",
    "                y_hat = sigma(np.dot(X, self.theta)) # order of X and theta matters here\n",
    "                # y_hat = sigma(X @ self.theta) # should be the same as above\n",
    "                # NOTE: calling the built in sigma function sometimes gives a division by zero warning\n",
    "                # print('y_hat calculated' + str(y_hat))\n",
    "\n",
    "                # update cross entropy loss\n",
    "                loss += -np.sum(Y * np.log(y_hat) + (1 - Y) * np.log(1 - y_hat))\n",
    "                # hmmm... should I use np.sum for this?\n",
    "                # print('loss calculated: ' + str(loss))\n",
    "\n",
    "                # compute gradient\n",
    "                gradient = np.dot(X.T, y_hat - Y) / len(minibatch)\n",
    "                # gradient = (X.T @ (y_hat - Y)) / len(minibatch) # should be the same as above\n",
    "                # print('gradient calculated' + str(gradient))\n",
    "\n",
    "                # update weights (and bias)\n",
    "                self.theta = self.theta - (eta * gradient)\n",
    "                # print('weights updated')\n",
    "\n",
    "                # END STUDENT CODE\n",
    "            # print(loss)\n",
    "            # print('len of filenames: ' + str(len(filenames)))\n",
    "            loss /= len(filenames)\n",
    "            print(\"Average Train Loss: {}\".format(loss))\n",
    "            # randomize order\n",
    "            Random(epoch).shuffle(filenames)\n",
    "\n",
    "    '''\n",
    "    Tests the classifier on a development or test set.\n",
    "    Returns a dictionary of filenames mapped to their correct and predicted\n",
    "    classes such that:\n",
    "    results[filename]['correct'] = correct class\n",
    "    results[filename]['predicted'] = predicted class\n",
    "    '''\n",
    "    def test(self, dev_set):\n",
    "        results = defaultdict(dict)\n",
    "        filenames, classes, documents = self.load_data(dev_set)\n",
    "        for name in filenames:\n",
    "            # BEGIN STUDENT CODE\n",
    "            # get most likely class (recall that P(y=1|x) = y_hat)\n",
    "            y_hat = sigma(np.dot(self.theta, documents[name]))\n",
    "            \n",
    "            # Determine the predicted class\n",
    "            if y_hat > 0.5:\n",
    "                predicted_class = 1    \n",
    "            else:\n",
    "                predicted_class = 0\n",
    "            \n",
    "            # Return a dictionary of filenames mapped to their correct and predicted\n",
    "            results[name]['correct'] = classes[name]\n",
    "            results[name]['predicted'] = predicted_class\n",
    "            # END STUDENT CODE\n",
    "        return results\n",
    "\n",
    "    '''\n",
    "    Given results, calculates the following:\n",
    "    Precision, Recall, F1 for each class\n",
    "    Accuracy overall\n",
    "    Also, prints evaluation metrics in readable format.\n",
    "    '''\n",
    "    def evaluate(self, results):\n",
    "        # Initialize counters for pos class\n",
    "        TP_pos = 0 \n",
    "        FP_pos = 0\n",
    "        TN_pos = 0\n",
    "        FN_pos = 0\n",
    "        \n",
    "        # Initialize counters for neg class\n",
    "        TP_neg = 0\n",
    "        FP_neg = 0\n",
    "        TN_neg = 0\n",
    "        FN_neg = 0\n",
    "\n",
    "        for name in results:\n",
    "            # True Positive for Positive class and True Negative for Negative class\n",
    "            if results[name]['correct'] == 1 and results[name]['predicted'] == 1:\n",
    "                TP_pos += 1\n",
    "                TN_neg += 1\n",
    "            # False Positive for Positive class and False Negative for Negative class\n",
    "            elif results[name]['correct'] == 0 and results[name]['predicted'] == 1:\n",
    "                FP_pos += 1\n",
    "                FN_neg += 1\n",
    "            # True Negative for Positive class and True Positive for Negative class\n",
    "            elif results[name]['correct'] == 0 and results[name]['predicted'] == 0:\n",
    "                TN_pos += 1\n",
    "                TP_neg += 1\n",
    "            # False Negative for Positive class and False Positive for Negative class\n",
    "            elif results[name]['correct'] == 1 and results[name]['predicted'] == 0:\n",
    "                FN_pos += 1\n",
    "                FP_neg += 1\n",
    "\n",
    "        # Calculate and print metrics for Positive class\n",
    "        precision_pos = TP_pos / (TP_pos + FP_pos)\n",
    "        recall_pos = TP_pos / (TP_pos + FN_pos)\n",
    "        F1_pos = 2 * (precision_pos * recall_pos) / (precision_pos + recall_pos)\n",
    "        print(\"Metrics for Positive Class:\")\n",
    "        print('Precision: ' + str(round(precision_pos, 4)))\n",
    "        print('Recall: ' + str(round(recall_pos, 4)))\n",
    "        print('F1: ' + str(round(F1_pos, 4)))\n",
    "\n",
    "        # Calculate and print metrics for Negative class\n",
    "        precision_neg = TP_neg / (TP_neg + FP_neg)\n",
    "        recall_neg = TP_neg / (TP_neg + FN_neg)\n",
    "        F1_neg = 2 * (precision_neg * recall_neg) / (precision_neg + recall_neg)\n",
    "        print(\"Metrics for Negative Class:\")\n",
    "        print('Precision: ' + str(round(precision_neg, 4)))\n",
    "        print('Recall: ' + str(round(recall_neg, 4)))\n",
    "        print('F1: ' + str(round(F1_neg, 4)))\n",
    "\n",
    "        # Overall accuracy\n",
    "        accuracy = (TP_pos + TN_neg) / (TP_pos + TN_neg + FP_pos + FN_pos)\n",
    "        print('Overall Accuracy of Model: ' + str(round(accuracy, 4)))\n",
    "\n",
    "        return {'pos': {'precision': precision_pos, 'recall': recall_pos, 'F1': F1_pos},\n",
    "                'neg': {'precision': precision_neg, 'recall': recall_neg, 'F1': F1_neg},\n",
    "                'accuracy': accuracy}\n",
    "        \n",
    "        # END STUDENT CODE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training with batch_size=1, n_epochs=1, eta=0.01...\n",
      "Epoch 1 out of 1\n",
      "Average Train Loss: 0.570660712002118\n",
      "Metrics for Positive Class:\n",
      "Precision: 0.6296\n",
      "Recall: 0.1717\n",
      "F1: 0.2698\n",
      "Metrics for Negative Class:\n",
      "Precision: 0.526\n",
      "Recall: 0.901\n",
      "F1: 0.6642\n",
      "Overall Accuracy of Model: 0.2698\n",
      "Training with batch_size=1, n_epochs=1, eta=0.05...\n",
      "Epoch 1 out of 1\n",
      "Average Train Loss: nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ericb\\anaconda3\\envs\\machinelearning_20220719\\lib\\site-packages\\ipykernel_launcher.py:239: RuntimeWarning: divide by zero encountered in log\n",
      "c:\\Users\\ericb\\anaconda3\\envs\\machinelearning_20220719\\lib\\site-packages\\ipykernel_launcher.py:239: RuntimeWarning: invalid value encountered in multiply\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metrics for Positive Class:\n",
      "Precision: 0.6296\n",
      "Recall: 0.1717\n",
      "F1: 0.2698\n",
      "Metrics for Negative Class:\n",
      "Precision: 0.526\n",
      "Recall: 0.901\n",
      "F1: 0.6642\n",
      "Overall Accuracy of Model: 0.2698\n",
      "Training with batch_size=1, n_epochs=1, eta=0.1...\n",
      "Epoch 1 out of 1\n",
      "Average Train Loss: nan\n",
      "Metrics for Positive Class:\n",
      "Precision: 0.6296\n",
      "Recall: 0.1717\n",
      "F1: 0.2698\n",
      "Metrics for Negative Class:\n",
      "Precision: 0.526\n",
      "Recall: 0.901\n",
      "F1: 0.6642\n",
      "Overall Accuracy of Model: 0.2698\n",
      "Training with batch_size=1, n_epochs=1, eta=0.15...\n",
      "Epoch 1 out of 1\n",
      "Average Train Loss: nan\n",
      "Metrics for Positive Class:\n",
      "Precision: 0.6296\n",
      "Recall: 0.1717\n",
      "F1: 0.2698\n",
      "Metrics for Negative Class:\n",
      "Precision: 0.526\n",
      "Recall: 0.901\n",
      "F1: 0.6642\n",
      "Overall Accuracy of Model: 0.2698\n",
      "Training with batch_size=1, n_epochs=1, eta=0.2...\n",
      "Epoch 1 out of 1\n",
      "Average Train Loss: nan\n",
      "Metrics for Positive Class:\n",
      "Precision: 0.5052\n",
      "Recall: 0.9798\n",
      "F1: 0.6667\n",
      "Metrics for Negative Class:\n",
      "Precision: 0.75\n",
      "Recall: 0.0594\n",
      "F1: 0.1101\n",
      "Overall Accuracy of Model: 0.6667\n",
      "Training with batch_size=1, n_epochs=2, eta=0.01...\n",
      "Epoch 1 out of 2\n",
      "Average Train Loss: 0.570660712002118\n",
      "Epoch 2 out of 2\n",
      "Average Train Loss: nan\n",
      "Metrics for Positive Class:\n",
      "Precision: 0.6296\n",
      "Recall: 0.1717\n",
      "F1: 0.2698\n",
      "Metrics for Negative Class:\n",
      "Precision: 0.526\n",
      "Recall: 0.901\n",
      "F1: 0.6642\n",
      "Overall Accuracy of Model: 0.2698\n",
      "Training with batch_size=1, n_epochs=2, eta=0.05...\n",
      "Epoch 1 out of 2\n",
      "Average Train Loss: nan\n",
      "Epoch 2 out of 2\n",
      "Average Train Loss: nan\n",
      "Metrics for Positive Class:\n",
      "Precision: 0.5052\n",
      "Recall: 0.9798\n",
      "F1: 0.6667\n",
      "Metrics for Negative Class:\n",
      "Precision: 0.75\n",
      "Recall: 0.0594\n",
      "F1: 0.1101\n",
      "Overall Accuracy of Model: 0.6667\n",
      "Training with batch_size=1, n_epochs=2, eta=0.1...\n",
      "Epoch 1 out of 2\n",
      "Average Train Loss: nan\n",
      "Epoch 2 out of 2\n",
      "Average Train Loss: nan\n",
      "Metrics for Positive Class:\n",
      "Precision: 0.5052\n",
      "Recall: 0.9798\n",
      "F1: 0.6667\n",
      "Metrics for Negative Class:\n",
      "Precision: 0.75\n",
      "Recall: 0.0594\n",
      "F1: 0.1101\n",
      "Overall Accuracy of Model: 0.6667\n",
      "Training with batch_size=1, n_epochs=2, eta=0.15...\n",
      "Epoch 1 out of 2\n",
      "Average Train Loss: nan\n",
      "Epoch 2 out of 2\n",
      "Average Train Loss: nan\n",
      "Metrics for Positive Class:\n",
      "Precision: 0.5052\n",
      "Recall: 0.9798\n",
      "F1: 0.6667\n",
      "Metrics for Negative Class:\n",
      "Precision: 0.75\n",
      "Recall: 0.0594\n",
      "F1: 0.1101\n",
      "Overall Accuracy of Model: 0.6667\n",
      "Training with batch_size=1, n_epochs=2, eta=0.2...\n",
      "Epoch 1 out of 2\n",
      "Average Train Loss: nan\n",
      "Epoch 2 out of 2\n",
      "Average Train Loss: nan\n",
      "Metrics for Positive Class:\n",
      "Precision: 0.6296\n",
      "Recall: 0.1717\n",
      "F1: 0.2698\n",
      "Metrics for Negative Class:\n",
      "Precision: 0.526\n",
      "Recall: 0.901\n",
      "F1: 0.6642\n",
      "Overall Accuracy of Model: 0.2698\n",
      "Training with batch_size=1, n_epochs=5, eta=0.01...\n",
      "Epoch 1 out of 5\n",
      "Average Train Loss: 0.570660712002118\n",
      "Epoch 2 out of 5\n",
      "Average Train Loss: nan\n",
      "Epoch 3 out of 5\n",
      "Average Train Loss: nan\n",
      "Epoch 4 out of 5\n",
      "Average Train Loss: nan\n",
      "Epoch 5 out of 5\n",
      "Average Train Loss: nan\n",
      "Metrics for Positive Class:\n",
      "Precision: 0.6296\n",
      "Recall: 0.1717\n",
      "F1: 0.2698\n",
      "Metrics for Negative Class:\n",
      "Precision: 0.526\n",
      "Recall: 0.901\n",
      "F1: 0.6642\n",
      "Overall Accuracy of Model: 0.2698\n",
      "Training with batch_size=1, n_epochs=5, eta=0.05...\n",
      "Epoch 1 out of 5\n",
      "Average Train Loss: nan\n",
      "Epoch 2 out of 5\n",
      "Average Train Loss: nan\n",
      "Epoch 3 out of 5\n",
      "Average Train Loss: nan\n",
      "Epoch 4 out of 5\n",
      "Average Train Loss: nan\n",
      "Epoch 5 out of 5\n",
      "Average Train Loss: nan\n",
      "Metrics for Positive Class:\n",
      "Precision: 0.5052\n",
      "Recall: 0.9798\n",
      "F1: 0.6667\n",
      "Metrics for Negative Class:\n",
      "Precision: 0.75\n",
      "Recall: 0.0594\n",
      "F1: 0.1101\n",
      "Overall Accuracy of Model: 0.6667\n",
      "Training with batch_size=1, n_epochs=5, eta=0.1...\n",
      "Epoch 1 out of 5\n",
      "Average Train Loss: nan\n",
      "Epoch 2 out of 5\n",
      "Average Train Loss: nan\n",
      "Epoch 3 out of 5\n",
      "Average Train Loss: nan\n",
      "Epoch 4 out of 5\n",
      "Average Train Loss: nan\n",
      "Epoch 5 out of 5\n",
      "Average Train Loss: nan\n",
      "Metrics for Positive Class:\n",
      "Precision: 0.5052\n",
      "Recall: 0.9798\n",
      "F1: 0.6667\n",
      "Metrics for Negative Class:\n",
      "Precision: 0.75\n",
      "Recall: 0.0594\n",
      "F1: 0.1101\n",
      "Overall Accuracy of Model: 0.6667\n",
      "Training with batch_size=1, n_epochs=5, eta=0.15...\n",
      "Epoch 1 out of 5\n",
      "Average Train Loss: nan\n",
      "Epoch 2 out of 5\n",
      "Average Train Loss: nan\n",
      "Epoch 3 out of 5\n",
      "Average Train Loss: nan\n",
      "Epoch 4 out of 5\n",
      "Average Train Loss: nan\n",
      "Epoch 5 out of 5\n",
      "Average Train Loss: nan\n",
      "Metrics for Positive Class:\n",
      "Precision: 0.5052\n",
      "Recall: 0.9798\n",
      "F1: 0.6667\n",
      "Metrics for Negative Class:\n",
      "Precision: 0.75\n",
      "Recall: 0.0594\n",
      "F1: 0.1101\n",
      "Overall Accuracy of Model: 0.6667\n",
      "Training with batch_size=1, n_epochs=5, eta=0.2...\n",
      "Epoch 1 out of 5\n",
      "Average Train Loss: nan\n",
      "Epoch 2 out of 5\n",
      "Average Train Loss: nan\n",
      "Epoch 3 out of 5\n",
      "Average Train Loss: nan\n",
      "Epoch 4 out of 5\n",
      "Average Train Loss: nan\n",
      "Epoch 5 out of 5\n",
      "Average Train Loss: nan\n",
      "Metrics for Positive Class:\n",
      "Precision: 0.5052\n",
      "Recall: 0.9798\n",
      "F1: 0.6667\n",
      "Metrics for Negative Class:\n",
      "Precision: 0.75\n",
      "Recall: 0.0594\n",
      "F1: 0.1101\n",
      "Overall Accuracy of Model: 0.6667\n",
      "Training with batch_size=1, n_epochs=10, eta=0.01...\n",
      "Epoch 1 out of 10\n",
      "Average Train Loss: 0.570660712002118\n",
      "Epoch 2 out of 10\n",
      "Average Train Loss: nan\n",
      "Epoch 3 out of 10\n",
      "Average Train Loss: nan\n",
      "Epoch 4 out of 10\n",
      "Average Train Loss: nan\n",
      "Epoch 5 out of 10\n",
      "Average Train Loss: nan\n",
      "Epoch 6 out of 10\n",
      "Average Train Loss: nan\n",
      "Epoch 7 out of 10\n",
      "Average Train Loss: nan\n",
      "Epoch 8 out of 10\n",
      "Average Train Loss: nan\n",
      "Epoch 9 out of 10\n",
      "Average Train Loss: nan\n",
      "Epoch 10 out of 10\n",
      "Average Train Loss: nan\n",
      "Metrics for Positive Class:\n",
      "Precision: 0.6296\n",
      "Recall: 0.1717\n",
      "F1: 0.2698\n",
      "Metrics for Negative Class:\n",
      "Precision: 0.526\n",
      "Recall: 0.901\n",
      "F1: 0.6642\n",
      "Overall Accuracy of Model: 0.2698\n",
      "Training with batch_size=1, n_epochs=10, eta=0.05...\n",
      "Epoch 1 out of 10\n",
      "Average Train Loss: nan\n",
      "Epoch 2 out of 10\n",
      "Average Train Loss: nan\n",
      "Epoch 3 out of 10\n",
      "Average Train Loss: nan\n",
      "Epoch 4 out of 10\n",
      "Average Train Loss: nan\n",
      "Epoch 5 out of 10\n",
      "Average Train Loss: nan\n",
      "Epoch 6 out of 10\n",
      "Average Train Loss: nan\n",
      "Epoch 7 out of 10\n",
      "Average Train Loss: nan\n",
      "Epoch 8 out of 10\n",
      "Average Train Loss: nan\n",
      "Epoch 9 out of 10\n",
      "Average Train Loss: nan\n",
      "Epoch 10 out of 10\n",
      "Average Train Loss: nan\n",
      "Metrics for Positive Class:\n",
      "Precision: 0.6296\n",
      "Recall: 0.1717\n",
      "F1: 0.2698\n",
      "Metrics for Negative Class:\n",
      "Precision: 0.526\n",
      "Recall: 0.901\n",
      "F1: 0.6642\n",
      "Overall Accuracy of Model: 0.2698\n",
      "Training with batch_size=1, n_epochs=10, eta=0.1...\n",
      "Epoch 1 out of 10\n",
      "Average Train Loss: nan\n",
      "Epoch 2 out of 10\n",
      "Average Train Loss: nan\n",
      "Epoch 3 out of 10\n",
      "Average Train Loss: nan\n",
      "Epoch 4 out of 10\n",
      "Average Train Loss: nan\n",
      "Epoch 5 out of 10\n",
      "Average Train Loss: nan\n",
      "Epoch 6 out of 10\n",
      "Average Train Loss: nan\n",
      "Epoch 7 out of 10\n",
      "Average Train Loss: nan\n",
      "Epoch 8 out of 10\n",
      "Average Train Loss: nan\n",
      "Epoch 9 out of 10\n",
      "Average Train Loss: nan\n",
      "Epoch 10 out of 10\n",
      "Average Train Loss: nan\n",
      "Metrics for Positive Class:\n",
      "Precision: 0.6296\n",
      "Recall: 0.1717\n",
      "F1: 0.2698\n",
      "Metrics for Negative Class:\n",
      "Precision: 0.526\n",
      "Recall: 0.901\n",
      "F1: 0.6642\n",
      "Overall Accuracy of Model: 0.2698\n",
      "Training with batch_size=1, n_epochs=10, eta=0.15...\n",
      "Epoch 1 out of 10\n",
      "Average Train Loss: nan\n",
      "Epoch 2 out of 10\n",
      "Average Train Loss: nan\n",
      "Epoch 3 out of 10\n",
      "Average Train Loss: nan\n",
      "Epoch 4 out of 10\n",
      "Average Train Loss: nan\n",
      "Epoch 5 out of 10\n",
      "Average Train Loss: nan\n",
      "Epoch 6 out of 10\n",
      "Average Train Loss: nan\n",
      "Epoch 7 out of 10\n",
      "Average Train Loss: nan\n",
      "Epoch 8 out of 10\n",
      "Average Train Loss: nan\n",
      "Epoch 9 out of 10\n",
      "Average Train Loss: nan\n",
      "Epoch 10 out of 10\n",
      "Average Train Loss: nan\n",
      "Metrics for Positive Class:\n",
      "Precision: 0.6296\n",
      "Recall: 0.1717\n",
      "F1: 0.2698\n",
      "Metrics for Negative Class:\n",
      "Precision: 0.526\n",
      "Recall: 0.901\n",
      "F1: 0.6642\n",
      "Overall Accuracy of Model: 0.2698\n",
      "Training with batch_size=1, n_epochs=10, eta=0.2...\n",
      "Epoch 1 out of 10\n",
      "Average Train Loss: nan\n",
      "Epoch 2 out of 10\n",
      "Average Train Loss: nan\n",
      "Epoch 3 out of 10\n",
      "Average Train Loss: nan\n",
      "Epoch 4 out of 10\n",
      "Average Train Loss: nan\n",
      "Epoch 5 out of 10\n",
      "Average Train Loss: nan\n",
      "Epoch 6 out of 10\n",
      "Average Train Loss: nan\n",
      "Epoch 7 out of 10\n",
      "Average Train Loss: nan\n",
      "Epoch 8 out of 10\n",
      "Average Train Loss: nan\n",
      "Epoch 9 out of 10\n",
      "Average Train Loss: nan\n",
      "Epoch 10 out of 10\n",
      "Average Train Loss: nan\n",
      "Metrics for Positive Class:\n",
      "Precision: 0.6296\n",
      "Recall: 0.1717\n",
      "F1: 0.2698\n",
      "Metrics for Negative Class:\n",
      "Precision: 0.526\n",
      "Recall: 0.901\n",
      "F1: 0.6642\n",
      "Overall Accuracy of Model: 0.2698\n",
      "Training with batch_size=2, n_epochs=1, eta=0.01...\n",
      "Epoch 1 out of 1\n",
      "Average Train Loss: 0.5809293255844817\n",
      "Metrics for Positive Class:\n",
      "Precision: 0.6296\n",
      "Recall: 0.1717\n",
      "F1: 0.2698\n",
      "Metrics for Negative Class:\n",
      "Precision: 0.526\n",
      "Recall: 0.901\n",
      "F1: 0.6642\n",
      "Overall Accuracy of Model: 0.2698\n",
      "Training with batch_size=2, n_epochs=1, eta=0.05...\n",
      "Epoch 1 out of 1\n",
      "Average Train Loss: nan\n",
      "Metrics for Positive Class:\n",
      "Precision: 0.6296\n",
      "Recall: 0.1717\n",
      "F1: 0.2698\n",
      "Metrics for Negative Class:\n",
      "Precision: 0.526\n",
      "Recall: 0.901\n",
      "F1: 0.6642\n",
      "Overall Accuracy of Model: 0.2698\n",
      "Training with batch_size=2, n_epochs=1, eta=0.1...\n",
      "Epoch 1 out of 1\n",
      "Average Train Loss: nan\n",
      "Metrics for Positive Class:\n",
      "Precision: 0.6296\n",
      "Recall: 0.1717\n",
      "F1: 0.2698\n",
      "Metrics for Negative Class:\n",
      "Precision: 0.526\n",
      "Recall: 0.901\n",
      "F1: 0.6642\n",
      "Overall Accuracy of Model: 0.2698\n",
      "Training with batch_size=2, n_epochs=1, eta=0.15...\n",
      "Epoch 1 out of 1\n",
      "Average Train Loss: nan\n",
      "Metrics for Positive Class:\n",
      "Precision: 0.6296\n",
      "Recall: 0.1717\n",
      "F1: 0.2698\n",
      "Metrics for Negative Class:\n",
      "Precision: 0.526\n",
      "Recall: 0.901\n",
      "F1: 0.6642\n",
      "Overall Accuracy of Model: 0.2698\n",
      "Training with batch_size=2, n_epochs=1, eta=0.2...\n",
      "Epoch 1 out of 1\n",
      "Average Train Loss: nan\n",
      "Metrics for Positive Class:\n",
      "Precision: 0.6296\n",
      "Recall: 0.1717\n",
      "F1: 0.2698\n",
      "Metrics for Negative Class:\n",
      "Precision: 0.526\n",
      "Recall: 0.901\n",
      "F1: 0.6642\n",
      "Overall Accuracy of Model: 0.2698\n",
      "Training with batch_size=2, n_epochs=2, eta=0.01...\n",
      "Epoch 1 out of 2\n",
      "Average Train Loss: 0.5809293255844817\n",
      "Epoch 2 out of 2\n",
      "Average Train Loss: 0.5594562270769666\n",
      "Metrics for Positive Class:\n",
      "Precision: 0.6296\n",
      "Recall: 0.1717\n",
      "F1: 0.2698\n",
      "Metrics for Negative Class:\n",
      "Precision: 0.526\n",
      "Recall: 0.901\n",
      "F1: 0.6642\n",
      "Overall Accuracy of Model: 0.2698\n",
      "Training with batch_size=2, n_epochs=2, eta=0.05...\n",
      "Epoch 1 out of 2\n",
      "Average Train Loss: nan\n",
      "Epoch 2 out of 2\n",
      "Average Train Loss: nan\n",
      "Metrics for Positive Class:\n",
      "Precision: 0.5052\n",
      "Recall: 0.9798\n",
      "F1: 0.6667\n",
      "Metrics for Negative Class:\n",
      "Precision: 0.75\n",
      "Recall: 0.0594\n",
      "F1: 0.1101\n",
      "Overall Accuracy of Model: 0.6667\n",
      "Training with batch_size=2, n_epochs=2, eta=0.1...\n",
      "Epoch 1 out of 2\n",
      "Average Train Loss: nan\n",
      "Epoch 2 out of 2\n",
      "Average Train Loss: nan\n",
      "Metrics for Positive Class:\n",
      "Precision: 0.5052\n",
      "Recall: 0.9798\n",
      "F1: 0.6667\n",
      "Metrics for Negative Class:\n",
      "Precision: 0.75\n",
      "Recall: 0.0594\n",
      "F1: 0.1101\n",
      "Overall Accuracy of Model: 0.6667\n",
      "Training with batch_size=2, n_epochs=2, eta=0.15...\n",
      "Epoch 1 out of 2\n",
      "Average Train Loss: nan\n",
      "Epoch 2 out of 2\n",
      "Average Train Loss: nan\n",
      "Metrics for Positive Class:\n",
      "Precision: 0.5052\n",
      "Recall: 0.9798\n",
      "F1: 0.6667\n",
      "Metrics for Negative Class:\n",
      "Precision: 0.75\n",
      "Recall: 0.0594\n",
      "F1: 0.1101\n",
      "Overall Accuracy of Model: 0.6667\n",
      "Training with batch_size=2, n_epochs=2, eta=0.2...\n",
      "Epoch 1 out of 2\n",
      "Average Train Loss: nan\n",
      "Epoch 2 out of 2\n",
      "Average Train Loss: nan\n",
      "Metrics for Positive Class:\n",
      "Precision: 0.5052\n",
      "Recall: 0.9798\n",
      "F1: 0.6667\n",
      "Metrics for Negative Class:\n",
      "Precision: 0.75\n",
      "Recall: 0.0594\n",
      "F1: 0.1101\n",
      "Overall Accuracy of Model: 0.6667\n",
      "Training with batch_size=2, n_epochs=5, eta=0.01...\n",
      "Epoch 1 out of 5\n",
      "Average Train Loss: 0.5809293255844817\n",
      "Epoch 2 out of 5\n",
      "Average Train Loss: 0.5594562270769666\n",
      "Epoch 3 out of 5\n",
      "Average Train Loss: nan\n",
      "Epoch 4 out of 5\n",
      "Average Train Loss: nan\n",
      "Epoch 5 out of 5\n",
      "Average Train Loss: nan\n",
      "Metrics for Positive Class:\n",
      "Precision: 0.6296\n",
      "Recall: 0.1717\n",
      "F1: 0.2698\n",
      "Metrics for Negative Class:\n",
      "Precision: 0.526\n",
      "Recall: 0.901\n",
      "F1: 0.6642\n",
      "Overall Accuracy of Model: 0.2698\n",
      "Training with batch_size=2, n_epochs=5, eta=0.05...\n",
      "Epoch 1 out of 5\n",
      "Average Train Loss: nan\n",
      "Epoch 2 out of 5\n",
      "Average Train Loss: nan\n",
      "Epoch 3 out of 5\n",
      "Average Train Loss: nan\n",
      "Epoch 4 out of 5\n",
      "Average Train Loss: nan\n",
      "Epoch 5 out of 5\n",
      "Average Train Loss: nan\n",
      "Metrics for Positive Class:\n",
      "Precision: 0.6296\n",
      "Recall: 0.1717\n",
      "F1: 0.2698\n",
      "Metrics for Negative Class:\n",
      "Precision: 0.526\n",
      "Recall: 0.901\n",
      "F1: 0.6642\n",
      "Overall Accuracy of Model: 0.2698\n",
      "Training with batch_size=2, n_epochs=5, eta=0.1...\n",
      "Epoch 1 out of 5\n",
      "Average Train Loss: nan\n",
      "Epoch 2 out of 5\n",
      "Average Train Loss: nan\n",
      "Epoch 3 out of 5\n",
      "Average Train Loss: nan\n",
      "Epoch 4 out of 5\n",
      "Average Train Loss: nan\n",
      "Epoch 5 out of 5\n",
      "Average Train Loss: nan\n",
      "Metrics for Positive Class:\n",
      "Precision: 0.5052\n",
      "Recall: 0.9798\n",
      "F1: 0.6667\n",
      "Metrics for Negative Class:\n",
      "Precision: 0.75\n",
      "Recall: 0.0594\n",
      "F1: 0.1101\n",
      "Overall Accuracy of Model: 0.6667\n",
      "Training with batch_size=2, n_epochs=5, eta=0.15...\n",
      "Epoch 1 out of 5\n",
      "Average Train Loss: nan\n",
      "Epoch 2 out of 5\n",
      "Average Train Loss: nan\n",
      "Epoch 3 out of 5\n",
      "Average Train Loss: nan\n",
      "Epoch 4 out of 5\n",
      "Average Train Loss: nan\n",
      "Epoch 5 out of 5\n",
      "Average Train Loss: nan\n",
      "Metrics for Positive Class:\n",
      "Precision: 0.5052\n",
      "Recall: 0.9798\n",
      "F1: 0.6667\n",
      "Metrics for Negative Class:\n",
      "Precision: 0.75\n",
      "Recall: 0.0594\n",
      "F1: 0.1101\n",
      "Overall Accuracy of Model: 0.6667\n",
      "Training with batch_size=2, n_epochs=5, eta=0.2...\n",
      "Epoch 1 out of 5\n",
      "Average Train Loss: nan\n",
      "Epoch 2 out of 5\n",
      "Average Train Loss: nan\n",
      "Epoch 3 out of 5\n",
      "Average Train Loss: nan\n",
      "Epoch 4 out of 5\n",
      "Average Train Loss: nan\n",
      "Epoch 5 out of 5\n",
      "Average Train Loss: nan\n",
      "Metrics for Positive Class:\n",
      "Precision: 0.5052\n",
      "Recall: 0.9798\n",
      "F1: 0.6667\n",
      "Metrics for Negative Class:\n",
      "Precision: 0.75\n",
      "Recall: 0.0594\n",
      "F1: 0.1101\n",
      "Overall Accuracy of Model: 0.6667\n",
      "Training with batch_size=2, n_epochs=10, eta=0.01...\n",
      "Epoch 1 out of 10\n",
      "Average Train Loss: 0.5809293255844817\n",
      "Epoch 2 out of 10\n",
      "Average Train Loss: 0.5594562270769666\n",
      "Epoch 3 out of 10\n",
      "Average Train Loss: nan\n",
      "Epoch 4 out of 10\n",
      "Average Train Loss: nan\n",
      "Epoch 5 out of 10\n",
      "Average Train Loss: nan\n",
      "Epoch 6 out of 10\n",
      "Average Train Loss: nan\n",
      "Epoch 7 out of 10\n",
      "Average Train Loss: nan\n",
      "Epoch 8 out of 10\n",
      "Average Train Loss: nan\n",
      "Epoch 9 out of 10\n",
      "Average Train Loss: nan\n",
      "Epoch 10 out of 10\n",
      "Average Train Loss: nan\n",
      "Metrics for Positive Class:\n",
      "Precision: 0.6296\n",
      "Recall: 0.1717\n",
      "F1: 0.2698\n",
      "Metrics for Negative Class:\n",
      "Precision: 0.526\n",
      "Recall: 0.901\n",
      "F1: 0.6642\n",
      "Overall Accuracy of Model: 0.2698\n",
      "Training with batch_size=2, n_epochs=10, eta=0.05...\n",
      "Epoch 1 out of 10\n",
      "Average Train Loss: nan\n",
      "Epoch 2 out of 10\n",
      "Average Train Loss: nan\n",
      "Epoch 3 out of 10\n",
      "Average Train Loss: nan\n",
      "Epoch 4 out of 10\n",
      "Average Train Loss: nan\n",
      "Epoch 5 out of 10\n",
      "Average Train Loss: nan\n",
      "Epoch 6 out of 10\n",
      "Average Train Loss: nan\n",
      "Epoch 7 out of 10\n",
      "Average Train Loss: nan\n",
      "Epoch 8 out of 10\n",
      "Average Train Loss: nan\n",
      "Epoch 9 out of 10\n",
      "Average Train Loss: nan\n",
      "Epoch 10 out of 10\n",
      "Average Train Loss: nan\n",
      "Metrics for Positive Class:\n",
      "Precision: 0.6296\n",
      "Recall: 0.1717\n",
      "F1: 0.2698\n",
      "Metrics for Negative Class:\n",
      "Precision: 0.526\n",
      "Recall: 0.901\n",
      "F1: 0.6642\n",
      "Overall Accuracy of Model: 0.2698\n",
      "Training with batch_size=2, n_epochs=10, eta=0.1...\n",
      "Epoch 1 out of 10\n",
      "Average Train Loss: nan\n",
      "Epoch 2 out of 10\n",
      "Average Train Loss: nan\n",
      "Epoch 3 out of 10\n",
      "Average Train Loss: nan\n",
      "Epoch 4 out of 10\n",
      "Average Train Loss: nan\n",
      "Epoch 5 out of 10\n",
      "Average Train Loss: nan\n",
      "Epoch 6 out of 10\n",
      "Average Train Loss: nan\n",
      "Epoch 7 out of 10\n",
      "Average Train Loss: nan\n",
      "Epoch 8 out of 10\n",
      "Average Train Loss: nan\n",
      "Epoch 9 out of 10\n",
      "Average Train Loss: nan\n",
      "Epoch 10 out of 10\n",
      "Average Train Loss: nan\n",
      "Metrics for Positive Class:\n",
      "Precision: 0.6296\n",
      "Recall: 0.1717\n",
      "F1: 0.2698\n",
      "Metrics for Negative Class:\n",
      "Precision: 0.526\n",
      "Recall: 0.901\n",
      "F1: 0.6642\n",
      "Overall Accuracy of Model: 0.2698\n",
      "Training with batch_size=2, n_epochs=10, eta=0.15...\n",
      "Epoch 1 out of 10\n",
      "Average Train Loss: nan\n",
      "Epoch 2 out of 10\n",
      "Average Train Loss: nan\n",
      "Epoch 3 out of 10\n",
      "Average Train Loss: nan\n",
      "Epoch 4 out of 10\n",
      "Average Train Loss: nan\n",
      "Epoch 5 out of 10\n",
      "Average Train Loss: nan\n",
      "Epoch 6 out of 10\n",
      "Average Train Loss: nan\n",
      "Epoch 7 out of 10\n",
      "Average Train Loss: nan\n",
      "Epoch 8 out of 10\n",
      "Average Train Loss: nan\n",
      "Epoch 9 out of 10\n",
      "Average Train Loss: nan\n",
      "Epoch 10 out of 10\n",
      "Average Train Loss: nan\n",
      "Metrics for Positive Class:\n",
      "Precision: 0.6296\n",
      "Recall: 0.1717\n",
      "F1: 0.2698\n",
      "Metrics for Negative Class:\n",
      "Precision: 0.526\n",
      "Recall: 0.901\n",
      "F1: 0.6642\n",
      "Overall Accuracy of Model: 0.2698\n",
      "Training with batch_size=2, n_epochs=10, eta=0.2...\n",
      "Epoch 1 out of 10\n",
      "Average Train Loss: nan\n",
      "Epoch 2 out of 10\n",
      "Average Train Loss: nan\n",
      "Epoch 3 out of 10\n",
      "Average Train Loss: nan\n",
      "Epoch 4 out of 10\n",
      "Average Train Loss: nan\n",
      "Epoch 5 out of 10\n",
      "Average Train Loss: nan\n",
      "Epoch 6 out of 10\n",
      "Average Train Loss: nan\n",
      "Epoch 7 out of 10\n",
      "Average Train Loss: nan\n",
      "Epoch 8 out of 10\n",
      "Average Train Loss: nan\n",
      "Epoch 9 out of 10\n",
      "Average Train Loss: nan\n",
      "Epoch 10 out of 10\n",
      "Average Train Loss: nan\n",
      "Metrics for Positive Class:\n",
      "Precision: 0.6296\n",
      "Recall: 0.1717\n",
      "F1: 0.2698\n",
      "Metrics for Negative Class:\n",
      "Precision: 0.526\n",
      "Recall: 0.901\n",
      "F1: 0.6642\n",
      "Overall Accuracy of Model: 0.2698\n",
      "Training with batch_size=3, n_epochs=1, eta=0.01...\n",
      "Epoch 1 out of 1\n",
      "Average Train Loss: 0.5895439647426236\n",
      "Metrics for Positive Class:\n",
      "Precision: 0.6296\n",
      "Recall: 0.1717\n",
      "F1: 0.2698\n",
      "Metrics for Negative Class:\n",
      "Precision: 0.526\n",
      "Recall: 0.901\n",
      "F1: 0.6642\n",
      "Overall Accuracy of Model: 0.2698\n",
      "Training with batch_size=3, n_epochs=1, eta=0.05...\n",
      "Epoch 1 out of 1\n",
      "Average Train Loss: nan\n",
      "Metrics for Positive Class:\n",
      "Precision: 0.6296\n",
      "Recall: 0.1717\n",
      "F1: 0.2698\n",
      "Metrics for Negative Class:\n",
      "Precision: 0.526\n",
      "Recall: 0.901\n",
      "F1: 0.6642\n",
      "Overall Accuracy of Model: 0.2698\n",
      "Training with batch_size=3, n_epochs=1, eta=0.1...\n",
      "Epoch 1 out of 1\n",
      "Average Train Loss: nan\n",
      "Metrics for Positive Class:\n",
      "Precision: 0.6296\n",
      "Recall: 0.1717\n",
      "F1: 0.2698\n",
      "Metrics for Negative Class:\n",
      "Precision: 0.526\n",
      "Recall: 0.901\n",
      "F1: 0.6642\n",
      "Overall Accuracy of Model: 0.2698\n",
      "Training with batch_size=3, n_epochs=1, eta=0.15...\n",
      "Epoch 1 out of 1\n",
      "Average Train Loss: nan\n",
      "Metrics for Positive Class:\n",
      "Precision: 0.6296\n",
      "Recall: 0.1717\n",
      "F1: 0.2698\n",
      "Metrics for Negative Class:\n",
      "Precision: 0.526\n",
      "Recall: 0.901\n",
      "F1: 0.6642\n",
      "Overall Accuracy of Model: 0.2698\n",
      "Training with batch_size=3, n_epochs=1, eta=0.2...\n",
      "Epoch 1 out of 1\n",
      "Average Train Loss: nan\n",
      "Metrics for Positive Class:\n",
      "Precision: 0.5052\n",
      "Recall: 0.9798\n",
      "F1: 0.6667\n",
      "Metrics for Negative Class:\n",
      "Precision: 0.75\n",
      "Recall: 0.0594\n",
      "F1: 0.1101\n",
      "Overall Accuracy of Model: 0.6667\n",
      "Training with batch_size=3, n_epochs=2, eta=0.01...\n",
      "Epoch 1 out of 2\n",
      "Average Train Loss: 0.5895439647426236\n",
      "Epoch 2 out of 2\n",
      "Average Train Loss: 0.5626598515897673\n",
      "Metrics for Positive Class:\n",
      "Precision: 0.6296\n",
      "Recall: 0.1717\n",
      "F1: 0.2698\n",
      "Metrics for Negative Class:\n",
      "Precision: 0.526\n",
      "Recall: 0.901\n",
      "F1: 0.6642\n",
      "Overall Accuracy of Model: 0.2698\n",
      "Training with batch_size=3, n_epochs=2, eta=0.05...\n",
      "Epoch 1 out of 2\n",
      "Average Train Loss: nan\n",
      "Epoch 2 out of 2\n",
      "Average Train Loss: nan\n",
      "Metrics for Positive Class:\n",
      "Precision: 0.6296\n",
      "Recall: 0.1717\n",
      "F1: 0.2698\n",
      "Metrics for Negative Class:\n",
      "Precision: 0.526\n",
      "Recall: 0.901\n",
      "F1: 0.6642\n",
      "Overall Accuracy of Model: 0.2698\n",
      "Training with batch_size=3, n_epochs=2, eta=0.1...\n",
      "Epoch 1 out of 2\n",
      "Average Train Loss: nan\n",
      "Epoch 2 out of 2\n",
      "Average Train Loss: nan\n",
      "Metrics for Positive Class:\n",
      "Precision: 0.5052\n",
      "Recall: 0.9798\n",
      "F1: 0.6667\n",
      "Metrics for Negative Class:\n",
      "Precision: 0.75\n",
      "Recall: 0.0594\n",
      "F1: 0.1101\n",
      "Overall Accuracy of Model: 0.6667\n",
      "Training with batch_size=3, n_epochs=2, eta=0.15...\n",
      "Epoch 1 out of 2\n",
      "Average Train Loss: nan\n",
      "Epoch 2 out of 2\n",
      "Average Train Loss: nan\n",
      "Metrics for Positive Class:\n",
      "Precision: 0.5052\n",
      "Recall: 0.9798\n",
      "F1: 0.6667\n",
      "Metrics for Negative Class:\n",
      "Precision: 0.75\n",
      "Recall: 0.0594\n",
      "F1: 0.1101\n",
      "Overall Accuracy of Model: 0.6667\n",
      "Training with batch_size=3, n_epochs=2, eta=0.2...\n",
      "Epoch 1 out of 2\n",
      "Average Train Loss: nan\n",
      "Epoch 2 out of 2\n",
      "Average Train Loss: nan\n",
      "Metrics for Positive Class:\n",
      "Precision: 0.5052\n",
      "Recall: 0.9798\n",
      "F1: 0.6667\n",
      "Metrics for Negative Class:\n",
      "Precision: 0.75\n",
      "Recall: 0.0594\n",
      "F1: 0.1101\n",
      "Overall Accuracy of Model: 0.6667\n",
      "Training with batch_size=3, n_epochs=5, eta=0.01...\n",
      "Epoch 1 out of 5\n",
      "Average Train Loss: 0.5895439647426236\n",
      "Epoch 2 out of 5\n",
      "Average Train Loss: 0.5626598515897673\n",
      "Epoch 3 out of 5\n",
      "Average Train Loss: 0.5584644339953792\n",
      "Epoch 4 out of 5\n",
      "Average Train Loss: 0.5567623899258405\n",
      "Epoch 5 out of 5\n",
      "Average Train Loss: nan\n",
      "Metrics for Positive Class:\n",
      "Precision: 0.6296\n",
      "Recall: 0.1717\n",
      "F1: 0.2698\n",
      "Metrics for Negative Class:\n",
      "Precision: 0.526\n",
      "Recall: 0.901\n",
      "F1: 0.6642\n",
      "Overall Accuracy of Model: 0.2698\n",
      "Training with batch_size=3, n_epochs=5, eta=0.05...\n",
      "Epoch 1 out of 5\n",
      "Average Train Loss: nan\n",
      "Epoch 2 out of 5\n",
      "Average Train Loss: nan\n",
      "Epoch 3 out of 5\n",
      "Average Train Loss: nan\n",
      "Epoch 4 out of 5\n",
      "Average Train Loss: nan\n",
      "Epoch 5 out of 5\n",
      "Average Train Loss: nan\n",
      "Metrics for Positive Class:\n",
      "Precision: 0.6296\n",
      "Recall: 0.1717\n",
      "F1: 0.2698\n",
      "Metrics for Negative Class:\n",
      "Precision: 0.526\n",
      "Recall: 0.901\n",
      "F1: 0.6642\n",
      "Overall Accuracy of Model: 0.2698\n",
      "Training with batch_size=3, n_epochs=5, eta=0.1...\n",
      "Epoch 1 out of 5\n",
      "Average Train Loss: nan\n",
      "Epoch 2 out of 5\n",
      "Average Train Loss: nan\n",
      "Epoch 3 out of 5\n",
      "Average Train Loss: nan\n",
      "Epoch 4 out of 5\n",
      "Average Train Loss: nan\n",
      "Epoch 5 out of 5\n",
      "Average Train Loss: nan\n",
      "Metrics for Positive Class:\n",
      "Precision: 0.5052\n",
      "Recall: 0.9798\n",
      "F1: 0.6667\n",
      "Metrics for Negative Class:\n",
      "Precision: 0.75\n",
      "Recall: 0.0594\n",
      "F1: 0.1101\n",
      "Overall Accuracy of Model: 0.6667\n",
      "Training with batch_size=3, n_epochs=5, eta=0.15...\n",
      "Epoch 1 out of 5\n",
      "Average Train Loss: nan\n",
      "Epoch 2 out of 5\n",
      "Average Train Loss: nan\n",
      "Epoch 3 out of 5\n",
      "Average Train Loss: nan\n",
      "Epoch 4 out of 5\n",
      "Average Train Loss: nan\n",
      "Epoch 5 out of 5\n",
      "Average Train Loss: nan\n",
      "Metrics for Positive Class:\n",
      "Precision: 0.5052\n",
      "Recall: 0.9798\n",
      "F1: 0.6667\n",
      "Metrics for Negative Class:\n",
      "Precision: 0.75\n",
      "Recall: 0.0594\n",
      "F1: 0.1101\n",
      "Overall Accuracy of Model: 0.6667\n",
      "Training with batch_size=3, n_epochs=5, eta=0.2...\n",
      "Epoch 1 out of 5\n",
      "Average Train Loss: nan\n",
      "Epoch 2 out of 5\n",
      "Average Train Loss: nan\n",
      "Epoch 3 out of 5\n",
      "Average Train Loss: nan\n",
      "Epoch 4 out of 5\n",
      "Average Train Loss: nan\n",
      "Epoch 5 out of 5\n",
      "Average Train Loss: nan\n",
      "Metrics for Positive Class:\n",
      "Precision: 0.5052\n",
      "Recall: 0.9798\n",
      "F1: 0.6667\n",
      "Metrics for Negative Class:\n",
      "Precision: 0.75\n",
      "Recall: 0.0594\n",
      "F1: 0.1101\n",
      "Overall Accuracy of Model: 0.6667\n",
      "Training with batch_size=3, n_epochs=10, eta=0.01...\n",
      "Epoch 1 out of 10\n",
      "Average Train Loss: 0.5895439647426236\n",
      "Epoch 2 out of 10\n",
      "Average Train Loss: 0.5626598515897673\n",
      "Epoch 3 out of 10\n",
      "Average Train Loss: 0.5584644339953792\n",
      "Epoch 4 out of 10\n",
      "Average Train Loss: 0.5567623899258405\n",
      "Epoch 5 out of 10\n",
      "Average Train Loss: nan\n",
      "Epoch 6 out of 10\n",
      "Average Train Loss: nan\n",
      "Epoch 7 out of 10\n",
      "Average Train Loss: nan\n",
      "Epoch 8 out of 10\n",
      "Average Train Loss: nan\n",
      "Epoch 9 out of 10\n",
      "Average Train Loss: nan\n",
      "Epoch 10 out of 10\n",
      "Average Train Loss: nan\n",
      "Metrics for Positive Class:\n",
      "Precision: 0.6296\n",
      "Recall: 0.1717\n",
      "F1: 0.2698\n",
      "Metrics for Negative Class:\n",
      "Precision: 0.526\n",
      "Recall: 0.901\n",
      "F1: 0.6642\n",
      "Overall Accuracy of Model: 0.2698\n",
      "Training with batch_size=3, n_epochs=10, eta=0.05...\n",
      "Epoch 1 out of 10\n",
      "Average Train Loss: nan\n",
      "Epoch 2 out of 10\n",
      "Average Train Loss: nan\n",
      "Epoch 3 out of 10\n",
      "Average Train Loss: nan\n",
      "Epoch 4 out of 10\n",
      "Average Train Loss: nan\n",
      "Epoch 5 out of 10\n",
      "Average Train Loss: nan\n",
      "Epoch 6 out of 10\n",
      "Average Train Loss: nan\n",
      "Epoch 7 out of 10\n",
      "Average Train Loss: nan\n",
      "Epoch 8 out of 10\n",
      "Average Train Loss: nan\n",
      "Epoch 9 out of 10\n",
      "Average Train Loss: nan\n",
      "Epoch 10 out of 10\n",
      "Average Train Loss: nan\n",
      "Metrics for Positive Class:\n",
      "Precision: 0.6296\n",
      "Recall: 0.1717\n",
      "F1: 0.2698\n",
      "Metrics for Negative Class:\n",
      "Precision: 0.526\n",
      "Recall: 0.901\n",
      "F1: 0.6642\n",
      "Overall Accuracy of Model: 0.2698\n",
      "Training with batch_size=3, n_epochs=10, eta=0.1...\n",
      "Epoch 1 out of 10\n",
      "Average Train Loss: nan\n",
      "Epoch 2 out of 10\n",
      "Average Train Loss: nan\n",
      "Epoch 3 out of 10\n",
      "Average Train Loss: nan\n",
      "Epoch 4 out of 10\n",
      "Average Train Loss: nan\n",
      "Epoch 5 out of 10\n",
      "Average Train Loss: nan\n",
      "Epoch 6 out of 10\n",
      "Average Train Loss: nan\n",
      "Epoch 7 out of 10\n",
      "Average Train Loss: nan\n",
      "Epoch 8 out of 10\n",
      "Average Train Loss: nan\n",
      "Epoch 9 out of 10\n",
      "Average Train Loss: nan\n",
      "Epoch 10 out of 10\n",
      "Average Train Loss: nan\n",
      "Metrics for Positive Class:\n",
      "Precision: 0.6296\n",
      "Recall: 0.1717\n",
      "F1: 0.2698\n",
      "Metrics for Negative Class:\n",
      "Precision: 0.526\n",
      "Recall: 0.901\n",
      "F1: 0.6642\n",
      "Overall Accuracy of Model: 0.2698\n",
      "Training with batch_size=3, n_epochs=10, eta=0.15...\n",
      "Epoch 1 out of 10\n",
      "Average Train Loss: nan\n",
      "Epoch 2 out of 10\n",
      "Average Train Loss: nan\n",
      "Epoch 3 out of 10\n",
      "Average Train Loss: nan\n",
      "Epoch 4 out of 10\n",
      "Average Train Loss: nan\n",
      "Epoch 5 out of 10\n",
      "Average Train Loss: nan\n",
      "Epoch 6 out of 10\n",
      "Average Train Loss: nan\n",
      "Epoch 7 out of 10\n",
      "Average Train Loss: nan\n",
      "Epoch 8 out of 10\n",
      "Average Train Loss: nan\n",
      "Epoch 9 out of 10\n",
      "Average Train Loss: nan\n",
      "Epoch 10 out of 10\n",
      "Average Train Loss: nan\n",
      "Metrics for Positive Class:\n",
      "Precision: 0.6296\n",
      "Recall: 0.1717\n",
      "F1: 0.2698\n",
      "Metrics for Negative Class:\n",
      "Precision: 0.526\n",
      "Recall: 0.901\n",
      "F1: 0.6642\n",
      "Overall Accuracy of Model: 0.2698\n",
      "Training with batch_size=3, n_epochs=10, eta=0.2...\n",
      "Epoch 1 out of 10\n",
      "Average Train Loss: nan\n",
      "Epoch 2 out of 10\n",
      "Average Train Loss: nan\n",
      "Epoch 3 out of 10\n",
      "Average Train Loss: nan\n",
      "Epoch 4 out of 10\n",
      "Average Train Loss: nan\n",
      "Epoch 5 out of 10\n",
      "Average Train Loss: nan\n",
      "Epoch 6 out of 10\n",
      "Average Train Loss: nan\n",
      "Epoch 7 out of 10\n",
      "Average Train Loss: nan\n",
      "Epoch 8 out of 10\n",
      "Average Train Loss: nan\n",
      "Epoch 9 out of 10\n",
      "Average Train Loss: nan\n",
      "Epoch 10 out of 10\n",
      "Average Train Loss: nan\n",
      "Metrics for Positive Class:\n",
      "Precision: 0.6296\n",
      "Recall: 0.1717\n",
      "F1: 0.2698\n",
      "Metrics for Negative Class:\n",
      "Precision: 0.526\n",
      "Recall: 0.901\n",
      "F1: 0.6642\n",
      "Overall Accuracy of Model: 0.2698\n",
      "Training with batch_size=10, n_epochs=1, eta=0.01...\n",
      "Epoch 1 out of 1\n",
      "Average Train Loss: 0.6222810281451383\n",
      "Metrics for Positive Class:\n",
      "Precision: 0.6296\n",
      "Recall: 0.1717\n",
      "F1: 0.2698\n",
      "Metrics for Negative Class:\n",
      "Precision: 0.526\n",
      "Recall: 0.901\n",
      "F1: 0.6642\n",
      "Overall Accuracy of Model: 0.2698\n",
      "Training with batch_size=10, n_epochs=1, eta=0.05...\n",
      "Epoch 1 out of 1\n",
      "Average Train Loss: 0.5809519856774679\n",
      "Metrics for Positive Class:\n",
      "Precision: 0.6296\n",
      "Recall: 0.1717\n",
      "F1: 0.2698\n",
      "Metrics for Negative Class:\n",
      "Precision: 0.526\n",
      "Recall: 0.901\n",
      "F1: 0.6642\n",
      "Overall Accuracy of Model: 0.2698\n",
      "Training with batch_size=10, n_epochs=1, eta=0.1...\n",
      "Epoch 1 out of 1\n",
      "Average Train Loss: 0.5697970105074588\n",
      "Metrics for Positive Class:\n",
      "Precision: 0.6296\n",
      "Recall: 0.1717\n",
      "F1: 0.2698\n",
      "Metrics for Negative Class:\n",
      "Precision: 0.526\n",
      "Recall: 0.901\n",
      "F1: 0.6642\n",
      "Overall Accuracy of Model: 0.2698\n",
      "Training with batch_size=10, n_epochs=1, eta=0.15...\n",
      "Epoch 1 out of 1\n",
      "Average Train Loss: 0.5651368800528354\n",
      "Metrics for Positive Class:\n",
      "Precision: 0.6296\n",
      "Recall: 0.1717\n",
      "F1: 0.2698\n",
      "Metrics for Negative Class:\n",
      "Precision: 0.526\n",
      "Recall: 0.901\n",
      "F1: 0.6642\n",
      "Overall Accuracy of Model: 0.2698\n",
      "Training with batch_size=10, n_epochs=1, eta=0.2...\n",
      "Epoch 1 out of 1\n",
      "Average Train Loss: nan\n",
      "Metrics for Positive Class:\n",
      "Precision: 0.6296\n",
      "Recall: 0.1717\n",
      "F1: 0.2698\n",
      "Metrics for Negative Class:\n",
      "Precision: 0.526\n",
      "Recall: 0.901\n",
      "F1: 0.6642\n",
      "Overall Accuracy of Model: 0.2698\n",
      "Training with batch_size=10, n_epochs=2, eta=0.01...\n",
      "Epoch 1 out of 2\n",
      "Average Train Loss: 0.6222810281451383\n",
      "Epoch 2 out of 2\n",
      "Average Train Loss: 0.5821085977511739\n",
      "Metrics for Positive Class:\n",
      "Precision: 0.6296\n",
      "Recall: 0.1717\n",
      "F1: 0.2698\n",
      "Metrics for Negative Class:\n",
      "Precision: 0.526\n",
      "Recall: 0.901\n",
      "F1: 0.6642\n",
      "Overall Accuracy of Model: 0.2698\n",
      "Training with batch_size=10, n_epochs=2, eta=0.05...\n",
      "Epoch 1 out of 2\n",
      "Average Train Loss: 0.5809519856774679\n",
      "Epoch 2 out of 2\n",
      "Average Train Loss: 0.5594860204164481\n",
      "Metrics for Positive Class:\n",
      "Precision: 0.6296\n",
      "Recall: 0.1717\n",
      "F1: 0.2698\n",
      "Metrics for Negative Class:\n",
      "Precision: 0.526\n",
      "Recall: 0.901\n",
      "F1: 0.6642\n",
      "Overall Accuracy of Model: 0.2698\n",
      "Training with batch_size=10, n_epochs=2, eta=0.1...\n",
      "Epoch 1 out of 2\n",
      "Average Train Loss: 0.5697970105074588\n",
      "Epoch 2 out of 2\n",
      "Average Train Loss: nan\n",
      "Metrics for Positive Class:\n",
      "Precision: 0.6296\n",
      "Recall: 0.1717\n",
      "F1: 0.2698\n",
      "Metrics for Negative Class:\n",
      "Precision: 0.526\n",
      "Recall: 0.901\n",
      "F1: 0.6642\n",
      "Overall Accuracy of Model: 0.2698\n",
      "Training with batch_size=10, n_epochs=2, eta=0.15...\n",
      "Epoch 1 out of 2\n",
      "Average Train Loss: 0.5651368800528354\n",
      "Epoch 2 out of 2\n",
      "Average Train Loss: nan\n",
      "Metrics for Positive Class:\n",
      "Precision: 0.6296\n",
      "Recall: 0.1717\n",
      "F1: 0.2698\n",
      "Metrics for Negative Class:\n",
      "Precision: 0.526\n",
      "Recall: 0.901\n",
      "F1: 0.6642\n",
      "Overall Accuracy of Model: 0.2698\n",
      "Training with batch_size=10, n_epochs=2, eta=0.2...\n",
      "Epoch 1 out of 2\n",
      "Average Train Loss: nan\n",
      "Epoch 2 out of 2\n",
      "Average Train Loss: nan\n",
      "Metrics for Positive Class:\n",
      "Precision: 0.6296\n",
      "Recall: 0.1717\n",
      "F1: 0.2698\n",
      "Metrics for Negative Class:\n",
      "Precision: 0.526\n",
      "Recall: 0.901\n",
      "F1: 0.6642\n",
      "Overall Accuracy of Model: 0.2698\n",
      "Training with batch_size=10, n_epochs=5, eta=0.01...\n",
      "Epoch 1 out of 5\n",
      "Average Train Loss: 0.6222810281451383\n",
      "Epoch 2 out of 5\n",
      "Average Train Loss: 0.5821085977511739\n",
      "Epoch 3 out of 5\n",
      "Average Train Loss: 0.5713020175767556\n",
      "Epoch 4 out of 5\n",
      "Average Train Loss: 0.5661713378461429\n",
      "Epoch 5 out of 5\n",
      "Average Train Loss: 0.5631945470052842\n",
      "Metrics for Positive Class:\n",
      "Precision: 0.6296\n",
      "Recall: 0.1717\n",
      "F1: 0.2698\n",
      "Metrics for Negative Class:\n",
      "Precision: 0.526\n",
      "Recall: 0.901\n",
      "F1: 0.6642\n",
      "Overall Accuracy of Model: 0.2698\n",
      "Training with batch_size=10, n_epochs=5, eta=0.05...\n",
      "Epoch 1 out of 5\n",
      "Average Train Loss: 0.5809519856774679\n",
      "Epoch 2 out of 5\n",
      "Average Train Loss: 0.5594860204164481\n",
      "Epoch 3 out of 5\n",
      "Average Train Loss: nan\n",
      "Epoch 4 out of 5\n",
      "Average Train Loss: nan\n",
      "Epoch 5 out of 5\n",
      "Average Train Loss: nan\n",
      "Metrics for Positive Class:\n",
      "Precision: 0.6296\n",
      "Recall: 0.1717\n",
      "F1: 0.2698\n",
      "Metrics for Negative Class:\n",
      "Precision: 0.526\n",
      "Recall: 0.901\n",
      "F1: 0.6642\n",
      "Overall Accuracy of Model: 0.2698\n",
      "Training with batch_size=10, n_epochs=5, eta=0.1...\n",
      "Epoch 1 out of 5\n",
      "Average Train Loss: 0.5697970105074588\n",
      "Epoch 2 out of 5\n",
      "Average Train Loss: nan\n",
      "Epoch 3 out of 5\n",
      "Average Train Loss: nan\n",
      "Epoch 4 out of 5\n",
      "Average Train Loss: nan\n",
      "Epoch 5 out of 5\n",
      "Average Train Loss: nan\n",
      "Metrics for Positive Class:\n",
      "Precision: 0.6296\n",
      "Recall: 0.1717\n",
      "F1: 0.2698\n",
      "Metrics for Negative Class:\n",
      "Precision: 0.526\n",
      "Recall: 0.901\n",
      "F1: 0.6642\n",
      "Overall Accuracy of Model: 0.2698\n",
      "Training with batch_size=10, n_epochs=5, eta=0.15...\n",
      "Epoch 1 out of 5\n",
      "Average Train Loss: 0.5651368800528354\n",
      "Epoch 2 out of 5\n",
      "Average Train Loss: nan\n",
      "Epoch 3 out of 5\n",
      "Average Train Loss: nan\n",
      "Epoch 4 out of 5\n",
      "Average Train Loss: nan\n",
      "Epoch 5 out of 5\n",
      "Average Train Loss: nan\n",
      "Metrics for Positive Class:\n",
      "Precision: 0.6296\n",
      "Recall: 0.1717\n",
      "F1: 0.2698\n",
      "Metrics for Negative Class:\n",
      "Precision: 0.526\n",
      "Recall: 0.901\n",
      "F1: 0.6642\n",
      "Overall Accuracy of Model: 0.2698\n",
      "Training with batch_size=10, n_epochs=5, eta=0.2...\n",
      "Epoch 1 out of 5\n",
      "Average Train Loss: nan\n",
      "Epoch 2 out of 5\n",
      "Average Train Loss: nan\n",
      "Epoch 3 out of 5\n",
      "Average Train Loss: nan\n",
      "Epoch 4 out of 5\n",
      "Average Train Loss: nan\n",
      "Epoch 5 out of 5\n",
      "Average Train Loss: nan\n",
      "Metrics for Positive Class:\n",
      "Precision: 0.6296\n",
      "Recall: 0.1717\n",
      "F1: 0.2698\n",
      "Metrics for Negative Class:\n",
      "Precision: 0.526\n",
      "Recall: 0.901\n",
      "F1: 0.6642\n",
      "Overall Accuracy of Model: 0.2698\n",
      "Training with batch_size=10, n_epochs=10, eta=0.01...\n",
      "Epoch 1 out of 10\n",
      "Average Train Loss: 0.6222810281451383\n",
      "Epoch 2 out of 10\n",
      "Average Train Loss: 0.5821085977511739\n",
      "Epoch 3 out of 10\n",
      "Average Train Loss: 0.5713020175767556\n",
      "Epoch 4 out of 10\n",
      "Average Train Loss: 0.5661713378461429\n",
      "Epoch 5 out of 10\n",
      "Average Train Loss: 0.5631945470052842\n",
      "Epoch 6 out of 10\n",
      "Average Train Loss: 0.5612536285704062\n",
      "Epoch 7 out of 10\n",
      "Average Train Loss: 0.559886797837459\n",
      "Epoch 8 out of 10\n",
      "Average Train Loss: 0.5588765630569362\n",
      "Epoch 9 out of 10\n",
      "Average Train Loss: 0.5581120907775402\n",
      "Epoch 10 out of 10\n",
      "Average Train Loss: 0.5575085542724765\n",
      "Metrics for Positive Class:\n",
      "Precision: 0.6296\n",
      "Recall: 0.1717\n",
      "F1: 0.2698\n",
      "Metrics for Negative Class:\n",
      "Precision: 0.526\n",
      "Recall: 0.901\n",
      "F1: 0.6642\n",
      "Overall Accuracy of Model: 0.2698\n",
      "Training with batch_size=10, n_epochs=10, eta=0.05...\n",
      "Epoch 1 out of 10\n",
      "Average Train Loss: 0.5809519856774679\n",
      "Epoch 2 out of 10\n",
      "Average Train Loss: 0.5594860204164481\n",
      "Epoch 3 out of 10\n",
      "Average Train Loss: nan\n",
      "Epoch 4 out of 10\n",
      "Average Train Loss: nan\n",
      "Epoch 5 out of 10\n",
      "Average Train Loss: nan\n",
      "Epoch 6 out of 10\n",
      "Average Train Loss: nan\n",
      "Epoch 7 out of 10\n",
      "Average Train Loss: nan\n",
      "Epoch 8 out of 10\n",
      "Average Train Loss: nan\n",
      "Epoch 9 out of 10\n",
      "Average Train Loss: nan\n",
      "Epoch 10 out of 10\n",
      "Average Train Loss: nan\n",
      "Metrics for Positive Class:\n",
      "Precision: 0.6296\n",
      "Recall: 0.1717\n",
      "F1: 0.2698\n",
      "Metrics for Negative Class:\n",
      "Precision: 0.526\n",
      "Recall: 0.901\n",
      "F1: 0.6642\n",
      "Overall Accuracy of Model: 0.2698\n",
      "Training with batch_size=10, n_epochs=10, eta=0.1...\n",
      "Epoch 1 out of 10\n",
      "Average Train Loss: 0.5697970105074588\n",
      "Epoch 2 out of 10\n",
      "Average Train Loss: nan\n",
      "Epoch 3 out of 10\n",
      "Average Train Loss: nan\n",
      "Epoch 4 out of 10\n",
      "Average Train Loss: nan\n",
      "Epoch 5 out of 10\n",
      "Average Train Loss: nan\n",
      "Epoch 6 out of 10\n",
      "Average Train Loss: nan\n",
      "Epoch 7 out of 10\n",
      "Average Train Loss: nan\n",
      "Epoch 8 out of 10\n",
      "Average Train Loss: nan\n",
      "Epoch 9 out of 10\n",
      "Average Train Loss: nan\n",
      "Epoch 10 out of 10\n",
      "Average Train Loss: nan\n",
      "Metrics for Positive Class:\n",
      "Precision: 0.6296\n",
      "Recall: 0.1717\n",
      "F1: 0.2698\n",
      "Metrics for Negative Class:\n",
      "Precision: 0.526\n",
      "Recall: 0.901\n",
      "F1: 0.6642\n",
      "Overall Accuracy of Model: 0.2698\n",
      "Training with batch_size=10, n_epochs=10, eta=0.15...\n",
      "Epoch 1 out of 10\n",
      "Average Train Loss: 0.5651368800528354\n",
      "Epoch 2 out of 10\n",
      "Average Train Loss: nan\n",
      "Epoch 3 out of 10\n",
      "Average Train Loss: nan\n",
      "Epoch 4 out of 10\n",
      "Average Train Loss: nan\n",
      "Epoch 5 out of 10\n",
      "Average Train Loss: nan\n",
      "Epoch 6 out of 10\n",
      "Average Train Loss: nan\n",
      "Epoch 7 out of 10\n",
      "Average Train Loss: nan\n",
      "Epoch 8 out of 10\n",
      "Average Train Loss: nan\n",
      "Epoch 9 out of 10\n",
      "Average Train Loss: nan\n",
      "Epoch 10 out of 10\n",
      "Average Train Loss: nan\n",
      "Metrics for Positive Class:\n",
      "Precision: 0.6296\n",
      "Recall: 0.1717\n",
      "F1: 0.2698\n",
      "Metrics for Negative Class:\n",
      "Precision: 0.526\n",
      "Recall: 0.901\n",
      "F1: 0.6642\n",
      "Overall Accuracy of Model: 0.2698\n",
      "Training with batch_size=10, n_epochs=10, eta=0.2...\n",
      "Epoch 1 out of 10\n",
      "Average Train Loss: nan\n",
      "Epoch 2 out of 10\n",
      "Average Train Loss: nan\n",
      "Epoch 3 out of 10\n",
      "Average Train Loss: nan\n",
      "Epoch 4 out of 10\n",
      "Average Train Loss: nan\n",
      "Epoch 5 out of 10\n",
      "Average Train Loss: nan\n",
      "Epoch 6 out of 10\n",
      "Average Train Loss: nan\n",
      "Epoch 7 out of 10\n",
      "Average Train Loss: nan\n",
      "Epoch 8 out of 10\n",
      "Average Train Loss: nan\n",
      "Epoch 9 out of 10\n",
      "Average Train Loss: nan\n",
      "Epoch 10 out of 10\n",
      "Average Train Loss: nan\n",
      "Metrics for Positive Class:\n",
      "Precision: 0.6296\n",
      "Recall: 0.1717\n",
      "F1: 0.2698\n",
      "Metrics for Negative Class:\n",
      "Precision: 0.526\n",
      "Recall: 0.901\n",
      "F1: 0.6642\n",
      "Overall Accuracy of Model: 0.2698\n",
      "Best Accuracy: 0.6666666666666666\n",
      "Best Hyperparameters: {'batch_size': 1, 'n_epochs': 1, 'eta': 0.2}\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    # Define lists of values for each hyperparameter\n",
    "    batch_sizes = [1, 2, 3, 10]\n",
    "    n_epochs_list = [1, 2, 5, 10]\n",
    "    etas = [0.01, 0.05, 0.1, 0.15, 0.2]\n",
    "\n",
    "    # Initialize the best accuracy to a very low value\n",
    "    best_accuracy = 0.0\n",
    "    best_params = {}\n",
    "\n",
    "    # Loop over all combinations of hyperparameters\n",
    "    for batch_size in batch_sizes:\n",
    "        for n_epochs in n_epochs_list:\n",
    "            for eta in etas:\n",
    "                print(f\"Training with batch_size={batch_size}, n_epochs={n_epochs}, eta={eta}...\")\n",
    "                \n",
    "                # Initialize a new LogisticRegression object for each run\n",
    "                lr = LogisticRegression(n_features=8)\n",
    "\n",
    "                lr.negatives = [\n",
    "                                \"atrocious\", \"atrociously\",\n",
    "                                \"incoherent\", \"incoherently\",\n",
    "                                \"dud\",\n",
    "                                \"horrid\", \"horridly\",\n",
    "                                \"shoddy\",\n",
    "                                \"overwrought\",\n",
    "                                \"feeble\", \"feebly\",\n",
    "                                \"horrendous\", \"horrendously\",\n",
    "                                \"ineffectual\",\n",
    "                                \"pathetic\", \"pathetically\",\n",
    "                                \"reject\", \"rejected\",\n",
    "                                \"lame\", \"lamest\",\n",
    "                                \"leaden\",\n",
    "                                \"incompetence\", \"incompetent\",\n",
    "                                \"abysmal\", \"abysmally\",\n",
    "                                \"unamusing\",\n",
    "                                \"travesty\",\n",
    "                                \"putrid\",\n",
    "                                \"absurd\", \"absurdly\", \"absurdity\",\n",
    "                                \"muck\",\n",
    "                                \"moron\", \"moronic\",\n",
    "                                \"plod\", \"plodding\",\n",
    "                                \"stupid\", \"stupidest\", \"stupidity\",\n",
    "                                \"nonsensical\", \"nonsense\",\n",
    "                                \"unimpressive\",\n",
    "                                \"irritate\", \"irritating\", \"irritatingly\",\n",
    "                                \"unentertaining\",\n",
    "                                \"clunker\",\n",
    "                                \"ill-advised\",\n",
    "                                \"insipid\", \"insipidity\",\n",
    "                                \"woeful\", \"woefully\",\n",
    "                                \"unacceptable\", \"unacceptably\",\n",
    "                                \"terrible\", \"terribly\",\n",
    "                                \"vomit\",\n",
    "                                \"rot\", \"rotting\",\n",
    "                                \"inept\", \"ineptitude\",\n",
    "                                \"uninterested\", \"disinterested\",\n",
    "                                \"embarrass\", \"embarrassing\", \"embarrassingly\",\n",
    "                                \"unwatchable\",\n",
    "                                \"unbearable\", \"unbearably\",\n",
    "                                \"unlikable\",\n",
    "                                \"unsatisfying\", \"unsatisfied\",\n",
    "                                \"unbelievable\", \"unbelievably\",\n",
    "                                \"tedious\", \"tediously\",\n",
    "                                \"sloppy\", \"sloppiness\",\n",
    "                                \"sketch\", \"sketchy\",\n",
    "                                \"repetitive\", \"repetition\",\n",
    "                                \"regret\", \"regrettable\", \"regrettably\",\n",
    "                                \"offensive\", \"offensively\",\n",
    "                                \"ineffective\", \"ineffectively\",\n",
    "                                \"dreadful\", \"dreadfully\",\n",
    "                                \"disastrous\", \"disastrously\",\n",
    "                                \"disappoint\", \"disappointing\", \"disappointingly\",\n",
    "                                \"dismal\", \"dismally\",\n",
    "                                \"clumsy\", \"clumsily\",\n",
    "                                \"chaos\", \"chaotic\",\n",
    "                                \"bore\", \"boring\", \"boringly\",\n",
    "                                \"awful\", \"awfully\",\n",
    "                                \"appall\", \"appalling\", \"appallingly\",\n",
    "                                \"annoy\", \"annoying\", \"annoyingly\",\n",
    "                                \"aggravate\", \"aggravating\"\n",
    "                            ]\n",
    "                lr.positives = [\n",
    "                                \"ideal\", \"ideals\",\n",
    "                                \"love\", \"loving\", \"lovingly\",\n",
    "                                \"masterful\", \"masterfully\",\n",
    "                                \"exhilarate\", \"exhilarating\",\n",
    "                                \"steady\",\n",
    "                                \"must-see\",\n",
    "                                \"symbol\", \"symbols\",\n",
    "                                \"introspect\", \"introspective\",\n",
    "                                \"divine\",\n",
    "                                \"powerful\", \"powerfully\",\n",
    "                                \"vivid\", \"vividly\",\n",
    "                                \"audacious\",\n",
    "                                \"harmonize\", \"harmony\",\n",
    "                                \"foundation\",\n",
    "                                \"uncompromising\",\n",
    "                                \"deft\", \"deftly\",\n",
    "                                \"affection\", \"affectionate\",\n",
    "                                \"sensitive\", \"sensitivity\",\n",
    "                                \"remark\", \"remarkable\",\n",
    "                                \"admire\", \"admiration\",\n",
    "                                \"comfort\", \"comforts\",\n",
    "                                \"passion\", \"passionate\",\n",
    "                                \"cherish\", \"cherished\",\n",
    "                                \"work\", \"workings\",\n",
    "                                \"meticulous\", \"meticulously\",\n",
    "                                \"stand-out\",\n",
    "                                \"honor\", \"honour\",\n",
    "                                \"droll\",\n",
    "                                \"brisk\",\n",
    "                                \"notion\", \"notions\",\n",
    "                                \"authentic\", \"authenticity\",\n",
    "                                \"unwavering\",\n",
    "                                \"respect\", \"respectful\",\n",
    "                                \"elegant\", \"elegantly\",\n",
    "                                \"purpose\", \"purposeful\",\n",
    "                                \"resolve\", \"resolves\",\n",
    "                                \"immerse\", \"immersive\",\n",
    "                                \"embrace\", \"embraces\",\n",
    "                                \"resilience\", \"resilient\",\n",
    "                                \"enthusiasm\", \"enthusiastic\",\n",
    "                                \"profound\",\n",
    "                                \"captivate\", \"captivating\",\n",
    "                                \"inspire\", \"inspiring\",\n",
    "                                \"compassion\", \"compassionate\",\n",
    "                                \"dedicate\", \"dedication\",\n",
    "                                \"praise\",\n",
    "                                \"commend\", \"commendable\",\n",
    "                                \"endear\", \"endearing\",\n",
    "                                \"integrity\",\n",
    "                                \"impress\", \"impressive\",\n",
    "                                \"enchant\", \"enchanting\",\n",
    "                                \"revelation\",\n",
    "                                \"satisfy\", \"satisfying\", \"satisfied\", \"satisfactory\", \"satisfyingly\",\n",
    "                                \"tender\", \"tenderness\",\n",
    "                                \"heartfelt\",\n",
    "                                \"exquisite\",\n",
    "                                \"joy\", \"joyful\",\n",
    "                                \"nurture\", \"nurturing\",\n",
    "                                \"refresh\", \"refreshing\",\n",
    "                                \"invigorate\", \"invigorating\",\n",
    "                                \"outstanding\",\n",
    "                                \"exception\", \"exceptional\",\n",
    "                                \"celebrate\", \"celebration\",\n",
    "                                \"uplift\", \"uplifting\",\n",
    "                                \"pleasant\",\n",
    "                                \"grace\", \"graceful\",\n",
    "                                \"heartwarming\",\n",
    "                                \"charm\", \"charming\",\n",
    "                                \"delight\", \"delightful\",\n",
    "                                \"admirable\",\n",
    "                                \"reassure\", \"reassuring\",\n",
    "                                \"astound\", \"astounding\",\n",
    "                                \"awe\", \"awe-inspiring\",\n",
    "                                \"allure\", \"alluring\",\n",
    "                                \"appreciate\", \"appreciation\",\n",
    "                                \"breathtaking\",\n",
    "                                \"vibrate\", \"vibrant\",\n",
    "                                \"enrich\", \"enriching\",\n",
    "                                \"encouraging\",\n",
    "                                \"magnificence\", \"magnificent\",\n",
    "                                \"radiance\", \"radiant\",\n",
    "                                \"phenomenon\", \"phenomenal\",\n",
    "                                \"stunning\", \"stunned\", \"stunningly\"\n",
    "                                \"brilliance\", \"brilliant\",\n",
    "                                \"value\", \"valuable\",\n",
    "                                \"reward\", \"rewarding\",\n",
    "                                \"treasure\", \"treasured\",\n",
    "                                \"superb\",\n",
    "                                \"splendid\",\n",
    "                                \"superior\",\n",
    "                                \"noteworthy\",\n",
    "                                \"noble\",\n",
    "                                \"nourish\", \"nourishing\",\n",
    "                                \"positive\",\n",
    "                                \"precious\",\n",
    "                                \"prosper\", \"prosperous\",\n",
    "                                \"rejuvenate\", \"rejuvenating\",\n",
    "                                \"robust\",\n",
    "                                \"sturdy\",\n",
    "                                \"sunny\",\n",
    "                                \"life\", \"lively\",\n",
    "                                \"trust\", \"trustworthy\",\n",
    "                                \"venerate\", \"venerable\",\n",
    "                                \"victory\", \"victorious\",\n",
    "                                \"wholesome\",\n",
    "                                \"wonderful\",\n",
    "                                \"worthy\",\n",
    "                                \"zealous\",\n",
    "                                \"zest\", \"zestful\"\n",
    "                            ]\n",
    "\n",
    "\n",
    "                # conclusive words that may be symbollic of the end of a review\n",
    "                # I thought this can help identify sentiment if we weight the positive and negative words that appear after these words\n",
    "                # I should have done bigrams but I did this prior to that\n",
    "                lr.conclusive_words = [\n",
    "                                \"however\",\n",
    "                                \"conclusion\",\n",
    "                                \"opinion\",\n",
    "                                \"final\",\n",
    "                                \"synopsis\",\n",
    "                                \"ultimately\",\n",
    "                                \"overall\",\n",
    "                                \"summary\",\n",
    "                                \"end\",\n",
    "                                \"conclusively\",\n",
    "                                \"lastly\",\n",
    "                                \"thus\",\n",
    "                                \"therefore\",\n",
    "                                \"hence\",\n",
    "                                \"nutshell\",\n",
    "                                \"essence\",\n",
    "                                \"verdict\",\n",
    "                                \"recap\",\n",
    "                                \"retrospect\",\n",
    "                                \"simply\",\n",
    "                                \"brief\",\n",
    "                                \"conclude\",\n",
    "                                \"wrap\",\n",
    "                                \"bottom\",\n",
    "                                \"line\",\n",
    "                                \"closing\",\n",
    "                                \"parting\",\n",
    "                                \"endnote\",\n",
    "                                \"mark\",\n",
    "                                \"give\",\n",
    "                                \"rate\",\n",
    "                                \"rating\",\n",
    "                                \"score\",\n",
    "                                \"star\",\n",
    "                                \"stars\",\n",
    "                                \"film\",\n",
    "                                \"movie\",\n",
    "                                \"review\",\n",
    "                                \"reviews\",\n",
    "                                \"critic\",\n",
    "                                \"critics\",\n",
    "                                \"critique\",\n",
    "                                \"critiques\",\n",
    "                                \"criticism\",\n",
    "                            ]\n",
    "\n",
    "                # lr.conclusive_words = []\n",
    "\n",
    "                # print('conclusive words: ' + str(lr.conclusive_words))\n",
    "                '''\n",
    "                Ok so here I am trying to remove the bigrams that have no words in common\n",
    "                with the union of the negative top N and positive top N\n",
    "                This should help remove bigrams that are not indicative of sentiment\n",
    "                '''\n",
    "\n",
    "                # union of negative and positive words for use in filtering bigrams\n",
    "                pos_neg_union = set(lr.negatives).union(set(lr.positives))\n",
    "                        \n",
    "                # remove negative bigrams that contain two words that are not in the union set\n",
    "                for bigram in lr.negative_bigrams[:]:  # Iterating over a copy using slicing\n",
    "                    if bigram[0] not in pos_neg_union and bigram[1] not in pos_neg_union:\n",
    "                        lr.negative_bigrams.remove(bigram)\n",
    "                            \n",
    "                # remove positive bigrams that contain two words that are not in the union set\n",
    "                for bigram in lr.positive_bigrams[:]:  # Iterating over a copy using slicing\n",
    "                    if bigram[0] not in pos_neg_union and bigram[1] not in pos_neg_union:\n",
    "                        lr.positive_bigrams.remove(bigram)\n",
    "\n",
    "                            \n",
    "                # Train the model with the current combination of hyperparameters\n",
    "                lr.train('movie_reviews/train', batch_size=batch_size, n_epochs=n_epochs, eta=eta)\n",
    "                \n",
    "                # Test the model\n",
    "                results = lr.test('movie_reviews/dev')\n",
    "                \n",
    "                # Evaluate the model and get the accuracy\n",
    "                metrics = lr.evaluate(results)\n",
    "                accuracy = metrics['accuracy']  # Assuming 'accuracy' is a key in the returned metrics dictionary\n",
    "                \n",
    "                # Check if this accuracy is the best\n",
    "                if accuracy > best_accuracy:\n",
    "                    best_accuracy = accuracy\n",
    "                    best_params = {'batch_size': batch_size, 'n_epochs': n_epochs, 'eta': eta}\n",
    "\n",
    "    print(f\"Best Accuracy: {best_accuracy}\")\n",
    "    print(f\"Best Hyperparameters: {best_params}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 out of 1\n",
      "Average Train Loss: nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ericb\\anaconda3\\envs\\machinelearning_20220719\\lib\\site-packages\\ipykernel_launcher.py:239: RuntimeWarning: divide by zero encountered in log\n",
      "c:\\Users\\ericb\\anaconda3\\envs\\machinelearning_20220719\\lib\\site-packages\\ipykernel_launcher.py:239: RuntimeWarning: invalid value encountered in multiply\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metrics for Positive Class:\n",
      "Precision: 0.5052\n",
      "Recall: 0.9798\n",
      "F1: 0.6667\n",
      "Metrics for Negative Class:\n",
      "Precision: 0.75\n",
      "Recall: 0.0594\n",
      "F1: 0.1101\n",
      "Overall Accuracy of Model: 0.6667\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    lr = LogisticRegression(n_features=8)\n",
    "\n",
    "    # print('common negative words: ' + str(lr.negatives))\n",
    "    # print('common positive words: ' + str(lr.positives))\n",
    "\n",
    "    # print('common negative bigrams: ' + str(lr.negative_bigrams))\n",
    "    # print('common positive bigrams: ' + str(lr.positive_bigrams))\n",
    "\n",
    "    # Painstakingly manually choose words from the top 500 that aren't proper nouns or super movie specific\n",
    "    # I also added similiar wordss for example, 'lame' was in the list so I added 'lamest'\n",
    "    # My positives list is longer...not sure if this matters yet\n",
    "    lr.negatives = [\n",
    "                    \"atrocious\", \"atrociously\",\n",
    "                    \"incoherent\", \"incoherently\",\n",
    "                    \"dud\",\n",
    "                    \"horrid\", \"horridly\",\n",
    "                    \"shoddy\",\n",
    "                    \"overwrought\",\n",
    "                    \"feeble\", \"feebly\",\n",
    "                    \"horrendous\", \"horrendously\",\n",
    "                    \"ineffectual\",\n",
    "                    \"pathetic\", \"pathetically\",\n",
    "                    \"reject\", \"rejected\",\n",
    "                    \"lame\", \"lamest\",\n",
    "                    \"leaden\",\n",
    "                    \"incompetence\", \"incompetent\",\n",
    "                    \"abysmal\", \"abysmally\",\n",
    "                    \"unamusing\",\n",
    "                    \"travesty\",\n",
    "                    \"putrid\",\n",
    "                    \"absurd\", \"absurdly\", \"absurdity\",\n",
    "                    \"muck\",\n",
    "                    \"moron\", \"moronic\",\n",
    "                    \"plod\", \"plodding\",\n",
    "                    \"stupid\", \"stupidest\", \"stupidity\",\n",
    "                    \"nonsensical\", \"nonsense\",\n",
    "                    \"unimpressive\",\n",
    "                    \"irritate\", \"irritating\", \"irritatingly\",\n",
    "                    \"unentertaining\",\n",
    "                    \"clunker\",\n",
    "                    \"ill-advised\",\n",
    "                    \"insipid\", \"insipidity\",\n",
    "                    \"woeful\", \"woefully\",\n",
    "                    \"unacceptable\", \"unacceptably\",\n",
    "                    \"terrible\", \"terribly\",\n",
    "                    \"vomit\",\n",
    "                    \"rot\", \"rotting\",\n",
    "                    \"inept\", \"ineptitude\",\n",
    "                    \"uninterested\", \"disinterested\",\n",
    "                    \"embarrass\", \"embarrassing\", \"embarrassingly\",\n",
    "                    \"unwatchable\",\n",
    "                    \"unbearable\", \"unbearably\",\n",
    "                    \"unlikable\",\n",
    "                    \"unsatisfying\", \"unsatisfied\",\n",
    "                    \"unbelievable\", \"unbelievably\",\n",
    "                    \"tedious\", \"tediously\",\n",
    "                    \"sloppy\", \"sloppiness\",\n",
    "                    \"sketch\", \"sketchy\",\n",
    "                    \"repetitive\", \"repetition\",\n",
    "                    \"regret\", \"regrettable\", \"regrettably\",\n",
    "                    \"offensive\", \"offensively\",\n",
    "                    \"ineffective\", \"ineffectively\",\n",
    "                    \"dreadful\", \"dreadfully\",\n",
    "                    \"disastrous\", \"disastrously\",\n",
    "                    \"disappoint\", \"disappointing\", \"disappointingly\",\n",
    "                    \"dismal\", \"dismally\",\n",
    "                    \"clumsy\", \"clumsily\",\n",
    "                    \"chaos\", \"chaotic\",\n",
    "                    \"bore\", \"boring\", \"boringly\",\n",
    "                    \"awful\", \"awfully\",\n",
    "                    \"appall\", \"appalling\", \"appallingly\",\n",
    "                    \"annoy\", \"annoying\", \"annoyingly\",\n",
    "                    \"aggravate\", \"aggravating\"\n",
    "                ]\n",
    "    lr.positives = [\n",
    "                    \"ideal\", \"ideals\",\n",
    "                    \"love\", \"loving\", \"lovingly\",\n",
    "                    \"masterful\", \"masterfully\",\n",
    "                    \"exhilarate\", \"exhilarating\",\n",
    "                    \"steady\",\n",
    "                    \"must-see\",\n",
    "                    \"symbol\", \"symbols\",\n",
    "                    \"introspect\", \"introspective\",\n",
    "                    \"divine\",\n",
    "                    \"powerful\", \"powerfully\",\n",
    "                    \"vivid\", \"vividly\",\n",
    "                    \"audacious\",\n",
    "                    \"harmonize\", \"harmony\",\n",
    "                    \"foundation\",\n",
    "                    \"uncompromising\",\n",
    "                    \"deft\", \"deftly\",\n",
    "                    \"affection\", \"affectionate\",\n",
    "                    \"sensitive\", \"sensitivity\",\n",
    "                    \"remark\", \"remarkable\",\n",
    "                    \"admire\", \"admiration\",\n",
    "                    \"comfort\", \"comforts\",\n",
    "                    \"passion\", \"passionate\",\n",
    "                    \"cherish\", \"cherished\",\n",
    "                    \"work\", \"workings\",\n",
    "                    \"meticulous\", \"meticulously\",\n",
    "                    \"stand-out\",\n",
    "                    \"honor\", \"honour\",\n",
    "                    \"droll\",\n",
    "                    \"brisk\",\n",
    "                    \"notion\", \"notions\",\n",
    "                    \"authentic\", \"authenticity\",\n",
    "                    \"unwavering\",\n",
    "                    \"respect\", \"respectful\",\n",
    "                    \"elegant\", \"elegantly\",\n",
    "                    \"purpose\", \"purposeful\",\n",
    "                    \"resolve\", \"resolves\",\n",
    "                    \"immerse\", \"immersive\",\n",
    "                    \"embrace\", \"embraces\",\n",
    "                    \"resilience\", \"resilient\",\n",
    "                    \"enthusiasm\", \"enthusiastic\",\n",
    "                    \"profound\",\n",
    "                    \"captivate\", \"captivating\",\n",
    "                    \"inspire\", \"inspiring\",\n",
    "                    \"compassion\", \"compassionate\",\n",
    "                    \"dedicate\", \"dedication\",\n",
    "                    \"praise\",\n",
    "                    \"commend\", \"commendable\",\n",
    "                    \"endear\", \"endearing\",\n",
    "                    \"integrity\",\n",
    "                    \"impress\", \"impressive\",\n",
    "                    \"enchant\", \"enchanting\",\n",
    "                    \"revelation\",\n",
    "                    \"satisfy\", \"satisfying\", \"satisfied\", \"satisfactory\", \"satisfyingly\",\n",
    "                    \"tender\", \"tenderness\",\n",
    "                    \"heartfelt\",\n",
    "                    \"exquisite\",\n",
    "                    \"joy\", \"joyful\",\n",
    "                    \"nurture\", \"nurturing\",\n",
    "                    \"refresh\", \"refreshing\",\n",
    "                    \"invigorate\", \"invigorating\",\n",
    "                    \"outstanding\",\n",
    "                    \"exception\", \"exceptional\",\n",
    "                    \"celebrate\", \"celebration\",\n",
    "                    \"uplift\", \"uplifting\",\n",
    "                    \"pleasant\",\n",
    "                    \"grace\", \"graceful\",\n",
    "                    \"heartwarming\",\n",
    "                    \"charm\", \"charming\",\n",
    "                    \"delight\", \"delightful\",\n",
    "                    \"admirable\",\n",
    "                    \"reassure\", \"reassuring\",\n",
    "                    \"astound\", \"astounding\",\n",
    "                    \"awe\", \"awe-inspiring\",\n",
    "                    \"allure\", \"alluring\",\n",
    "                    \"appreciate\", \"appreciation\",\n",
    "                    \"breathtaking\",\n",
    "                    \"vibrate\", \"vibrant\",\n",
    "                    \"enrich\", \"enriching\",\n",
    "                    \"encouraging\",\n",
    "                    \"magnificence\", \"magnificent\",\n",
    "                    \"radiance\", \"radiant\",\n",
    "                    \"phenomenon\", \"phenomenal\",\n",
    "                    \"stunning\", \"stunned\", \"stunningly\"\n",
    "                    \"brilliance\", \"brilliant\",\n",
    "                    \"value\", \"valuable\",\n",
    "                    \"reward\", \"rewarding\",\n",
    "                    \"treasure\", \"treasured\",\n",
    "                    \"superb\",\n",
    "                    \"splendid\",\n",
    "                    \"superior\",\n",
    "                    \"noteworthy\",\n",
    "                    \"noble\",\n",
    "                    \"nourish\", \"nourishing\",\n",
    "                    \"positive\",\n",
    "                    \"precious\",\n",
    "                    \"prosper\", \"prosperous\",\n",
    "                    \"rejuvenate\", \"rejuvenating\",\n",
    "                    \"robust\",\n",
    "                    \"sturdy\",\n",
    "                    \"sunny\",\n",
    "                    \"life\", \"lively\",\n",
    "                    \"trust\", \"trustworthy\",\n",
    "                    \"venerate\", \"venerable\",\n",
    "                    \"victory\", \"victorious\",\n",
    "                    \"wholesome\",\n",
    "                    \"wonderful\",\n",
    "                    \"worthy\",\n",
    "                    \"zealous\",\n",
    "                    \"zest\", \"zestful\"\n",
    "                ]\n",
    "\n",
    "\n",
    "    # conclusive words that may be symbollic of the end of a review\n",
    "    # I thought this can help identify sentiment if we weight the positive and negative words that appear after these words\n",
    "    # I should have done bigrams but I did this prior to that\n",
    "    lr.conclusive_words = [\n",
    "                    \"however\",\n",
    "                    \"conclusion\",\n",
    "                    \"opinion\",\n",
    "                    \"final\",\n",
    "                    \"synopsis\",\n",
    "                    \"ultimately\",\n",
    "                    \"overall\",\n",
    "                    \"summary\",\n",
    "                    \"end\",\n",
    "                    \"conclusively\",\n",
    "                    \"lastly\",\n",
    "                    \"thus\",\n",
    "                    \"therefore\",\n",
    "                    \"hence\",\n",
    "                    \"nutshell\",\n",
    "                    \"essence\",\n",
    "                    \"verdict\",\n",
    "                    \"recap\",\n",
    "                    \"retrospect\",\n",
    "                    \"simply\",\n",
    "                    \"brief\",\n",
    "                    \"conclude\",\n",
    "                    \"wrap\",\n",
    "                    \"bottom\",\n",
    "                    \"line\",\n",
    "                    \"closing\",\n",
    "                    \"parting\",\n",
    "                    \"endnote\",\n",
    "                    \"mark\",\n",
    "                    \"give\",\n",
    "                    \"rate\",\n",
    "                    \"rating\",\n",
    "                    \"score\",\n",
    "                    \"star\",\n",
    "                    \"stars\",\n",
    "                    \"film\",\n",
    "                    \"movie\",\n",
    "                    \"review\",\n",
    "                    \"reviews\",\n",
    "                    \"critic\",\n",
    "                    \"critics\",\n",
    "                    \"critique\",\n",
    "                    \"critiques\",\n",
    "                    \"criticism\",\n",
    "                ]\n",
    "\n",
    "    lr.conclusive_words = []\n",
    "\n",
    "    # print('conclusive words: ' + str(lr.conclusive_words))\n",
    "    '''\n",
    "    Ok so here I am trying to remove the bigrams that have no words in common\n",
    "    with the union of the negative top N and positive top N\n",
    "    This should help remove bigrams that are not indicative of sentiment\n",
    "    '''\n",
    "\n",
    "    # union of negative and positive words for use in filtering bigrams\n",
    "    pos_neg_union = set(lr.negatives).union(set(lr.positives))\n",
    "            \n",
    "    # remove negative bigrams that contain two words that are not in the union set\n",
    "    for bigram in lr.negative_bigrams[:]:  # Iterating over a copy using slicing\n",
    "        if bigram[0] not in pos_neg_union and bigram[1] not in pos_neg_union:\n",
    "            lr.negative_bigrams.remove(bigram)\n",
    "                \n",
    "    # remove positive bigrams that contain two words that are not in the union set\n",
    "    for bigram in lr.positive_bigrams[:]:  # Iterating over a copy using slicing\n",
    "        if bigram[0] not in pos_neg_union and bigram[1] not in pos_neg_union:\n",
    "            lr.positive_bigrams.remove(bigram)\n",
    "\n",
    "    # print('revised negative bigrams: ' + str(lr.negative_bigrams))\n",
    "    # print('revised positive bigrams: ' + str(lr.positive_bigrams))\n",
    "\n",
    "    # make sure these point to the right directories\n",
    "    lr.train('movie_reviews/train', batch_size=1, n_epochs=1, eta=0.2)\n",
    "    # lr.train('movie_reviews_small/train', batch_size=3, n_epochs=1, eta=0.1)\n",
    "    results = lr.test('movie_reviews/dev')\n",
    "    # results = lr.test('movie_reviews_small/test')\n",
    "    lr.evaluate(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "words + bigram + conclusive words\n",
    "\n",
    "Precision: 0.71\n",
    "Recall: 0.73\n",
    "F1: 0.72\n",
    "Accuracy: 0.71\n",
    "\n",
    "\n",
    "words + bigram + no conclusive words\n",
    "batch_size=1, n_epochs=1, eta=0.01\n",
    "\n",
    "Epoch 1 out of 1\n",
    "Average Train Loss: 0.47955403043614564\n",
    "Precision: 0.7\n",
    "Recall: 0.74\n",
    "F1: 0.72\n",
    "Accuracy: 0.71\n",
    "\n",
    "\n",
    "Bigram only + conclusion\n",
    "\n",
    "Epoch 1 out of 1\n",
    "Average Train Loss: 0.649795892498662\n",
    "Precision: 0.63\n",
    "Recall: 0.17\n",
    "F1: 0.27\n",
    "Accuracy: 0.54\n",
    "\n",
    "words only\n",
    "\n",
    "Epoch 1 out of 1\n",
    "Average Train Loss: 0.5990533887000353\n",
    "Precision: 0.73\n",
    "Recall: 0.7\n",
    "F1: 0.71\n",
    "Accuracy: 0.72\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "words, bigram, conclusion\n",
    "\n",
    "lr.train('movie_reviews/train', batch_size=1, n_epochs=2, eta=0.1)\n",
    "\n",
    "Metrics for Positive Class:\n",
    "Precision: 0.6562\n",
    "Recall: 0.8485\n",
    "F1: 0.7401\n",
    "Metrics for Negative Class:\n",
    "Precision: 0.7917\n",
    "Recall: 0.5644\n",
    "F1: 0.659\n",
    "Overall Accuracy of Model: 0.7401"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "machinelearning_20220719",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
