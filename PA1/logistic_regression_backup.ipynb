{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CS542 Fall 2021 Programming Assignment 2\n",
    "# Logistic Regression Classifier\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "from math import ceil\n",
    "from random import Random\n",
    "\n",
    "'''\n",
    "Computes the logistic function.\n",
    "'''\n",
    "def sigma(z):\n",
    "    return 1 / (1 + np.exp(-z))\n",
    "\n",
    "class LogisticRegression():\n",
    "\n",
    "    def __init__(self, n_features=2):\n",
    "        # be sure to use the right class_dict for each data set\n",
    "        self.class_dict = {'neg': 0, 'pos': 1}\n",
    "        # self.class_dict = {'action': 0, 'comedy': 1}\n",
    "        self.negatives, self.positives = self.load_sentiment_data('opinion-lexicon-English')\n",
    "        # use of self.feature_dict is optional for this assignment\n",
    "        self.feature_dict = {'num_pos_words': 0, 'num_neg_words': 1}\n",
    "        # self.feature_dict = {'fast': 0, 'couple': 1, 'shoot': 2, 'fly': 3}\n",
    "\n",
    "        self.n_features = n_features\n",
    "        self.theta = np.zeros(n_features + 1) # weights (and bias)\n",
    "\n",
    "    '''\n",
    "    Loads a dataset. Specifically, returns a list of filenames, and dictionaries\n",
    "    of classes and documents such that:\n",
    "    classes[filename] = class of the document\n",
    "    documents[filename] = feature vector for the document (use self.featurize)\n",
    "    '''\n",
    "    def load_data(self, data_set):\n",
    "        filenames = []\n",
    "        classes = dict()\n",
    "        documents = dict()\n",
    "\n",
    "        # iterate over documents\n",
    "        for root, dirs, files in os.walk(data_set):\n",
    "            for name in files:\n",
    "                with open(os.path.join(root, name)) as f:\n",
    "                    # your code here\n",
    "                    # BEGIN STUDENT CODE\n",
    "                    filenames.append(name)\n",
    "                    classes[name] = self.class_dict[os.path.basename(root)] # store class of document with filename as key and class as index\n",
    "                    document_words = f.read().split() # read in document into list of words\n",
    "                    documents[name] = self.featurize(document_words)  # send a doc as a list of words to be featurized\n",
    "                    # END STUDENT CODE\n",
    "        return filenames, classes, documents\n",
    "\n",
    "    def load_sentiment_data(self, path):\n",
    "        negatives = []\n",
    "        positives = []\n",
    "\n",
    "        # Dataset from \n",
    "        ''' Minqing Hu and Bing Liu. \"Mining and Summarizing Customer Reviews.\" \n",
    "            Proceedings of the ACM SIGKDD International Conference on Knowledge \n",
    "            Discovery and Data Mining (KDD-2004), Aug 22-25, 2004, Seattle, \n",
    "            Washington, USA, \n",
    "        '''\n",
    "        # open text file and read in lines\n",
    "        with open(os.path.join(path, 'negative-words.txt')) as f:\n",
    "            for line in f:\n",
    "                negatives.append(line[:-1]) # remove newline character\n",
    "        with open(os.path.join(path, 'positive-words.txt')) as f:\n",
    "            for line in f:\n",
    "                positives.append(line[:-1]) # remove newline character\n",
    "\n",
    "        return negatives, positives\n",
    "\n",
    "    '''\n",
    "    Given a document (as a list of words), returns a feature vector.\n",
    "    Note that the last element of the vector, corresponding to the bias, is a\n",
    "    \"dummy feature\" with value 1.\n",
    "    '''\n",
    "    def featurize(self, document):\n",
    "        vector = np.zeros(self.n_features + 1)\n",
    "        # BEGIN STUDENT CODE\n",
    "        # count all neg and pos words in document\n",
    "        for word in document:\n",
    "            if word in self.negatives:\n",
    "                vector[self.feature_dict['num_neg_words']] += 1\n",
    "            elif word in self.positives:\n",
    "                vector[self.feature_dict['num_pos_words']] += 1\n",
    "        # END STUDENT CODE\n",
    "        vector[-1] = 1\n",
    "        return vector\n",
    "\n",
    "    '''\n",
    "    Trains a logistic regression classifier on a training set.\n",
    "    '''\n",
    "    def train(self, train_set, batch_size=3, n_epochs=1, eta=0.1):\n",
    "        filenames, classes, documents = self.load_data(train_set)\n",
    "        filenames = sorted(filenames)\n",
    "        n_minibatches = ceil(len(filenames) / batch_size)\n",
    "        for epoch in range(n_epochs):\n",
    "            print(\"Epoch {:} out of {:}\".format(epoch + 1, n_epochs))\n",
    "            loss = 0\n",
    "            for i in range(n_minibatches):\n",
    "                # list of filenames in minibatch\n",
    "                minibatch = filenames[i * batch_size: (i + 1) * batch_size]\n",
    "                # BEGIN STUDENT CODE\n",
    "                # create and fill in matrix x and vector y\n",
    "                # Initialize matrix X and vector Y for the minibatch\n",
    "                X = np.zeros((len(minibatch), self.n_features + 1))\n",
    "                Y = np.zeros(len(minibatch))\n",
    "                \n",
    "                # Fill in X and Y with each files vector and class info\n",
    "                for i, name in enumerate(minibatch):\n",
    "                    X[i] = documents[name]\n",
    "                    Y[i] = classes[name]\n",
    "\n",
    "                # compute y_hat\n",
    "\n",
    "                y_hat = sigma(np.dot(X, self.theta)) # order of X and theta matters here\n",
    "                # y_hat = sigma(X @ self.theta) # should be the same as above\n",
    "                # print('y_hat calculated')\n",
    "\n",
    "                # update cross entropy loss\n",
    "                loss += -np.sum(Y * np.log(y_hat) + (1 - Y) * np.log(1 - y_hat))\n",
    "                # # unsure if we need to do this but this would give the average loss for the batch\n",
    "                # loss += batch_loss/len(minibatch)\n",
    "                # print('loss calculated: ' + str(loss))\n",
    "\n",
    "                # compute gradient\n",
    "                gradient = np.dot(X.T, y_hat - Y) / len(minibatch)\n",
    "                # gradient = (X.T @ (y_hat - Y)) / len(minibatch) # should be the same as above\n",
    "                # print('gradient calculated')\n",
    "\n",
    "                # update weights (and bias)\n",
    "                self.theta = self.theta - (eta * gradient)\n",
    "                # print('weights updated')\n",
    "\n",
    "                # END STUDENT CODE\n",
    "            loss /= len(filenames)\n",
    "            print(\"Average Train Loss: {}\".format(loss))\n",
    "            # randomize order\n",
    "            Random(epoch).shuffle(filenames)\n",
    "\n",
    "    '''\n",
    "    Tests the classifier on a development or test set.\n",
    "    Returns a dictionary of filenames mapped to their correct and predicted\n",
    "    classes such that:\n",
    "    results[filename]['correct'] = correct class\n",
    "    results[filename]['predicted'] = predicted class\n",
    "    '''\n",
    "    def test(self, dev_set):\n",
    "        results = defaultdict(dict)\n",
    "        filenames, classes, documents = self.load_data(dev_set)\n",
    "        for name in filenames:\n",
    "            # BEGIN STUDENT CODE\n",
    "            # get most likely class (recall that P(y=1|x) = y_hat)\n",
    "            y_hat = sigma(np.dot(self.theta, documents[name]))\n",
    "            \n",
    "            # Determine the predicted class\n",
    "            if y_hat > 0.5:\n",
    "                predicted_class = 1    \n",
    "            else:\n",
    "                predicted_class = 0\n",
    "            \n",
    "            # Return a dictionary of filenames mapped to their correct and predicted\n",
    "            results[name]['correct'] = classes[name]\n",
    "            results[name]['predicted'] = predicted_class\n",
    "            # END STUDENT CODE\n",
    "        return results\n",
    "\n",
    "    '''\n",
    "    Given results, calculates the following:\n",
    "    Precision, Recall, F1 for each class\n",
    "    Accuracy overall\n",
    "    Also, prints evaluation metrics in readable format.\n",
    "    '''\n",
    "    def evaluate(self, results):\n",
    "\n",
    "        # accuracy = (TP + TN) / (TP + TN + FP + FN)\n",
    "        # precision = TP / (TP + FP)\n",
    "        # recall = TP / (TP + FN)\n",
    "        # F1 = 2 * (precision * recall) / (precision + recall)\n",
    "        \n",
    "\n",
    "        TP = 0  # true positive\n",
    "        FP = 0  # false positive\n",
    "        TN = 0  # true negative\n",
    "        FN = 0  # false negative\n",
    "\n",
    "        for name in results:\n",
    "            # true positive\n",
    "            if results[name]['correct'] == 1 and results[name]['predicted'] == 1:\n",
    "                TP += 1\n",
    "            # false positive\n",
    "            elif results[name]['correct'] == 0 and results[name]['predicted'] == 1:\n",
    "                FP += 1\n",
    "            # true negative\n",
    "            elif results[name]['correct'] == 0 and results[name]['predicted'] == 0:\n",
    "                TN += 1\n",
    "            # false negative\n",
    "            elif results[name]['correct'] == 1 and results[name]['predicted'] == 0:\n",
    "                FN += 1\n",
    "\n",
    "        # calculate precision, recall, F1, and accuracy\n",
    "        precision = TP / (TP + FP)\n",
    "        recall = TP / (TP + FN)\n",
    "        F1 = 2 * (precision * recall) / (precision + recall)\n",
    "        accuracy = (TP + TN) / (TP + TN + FP + FN)\n",
    "\n",
    "        # print results\n",
    "        print('Precision: ' + str(round(precision,2)))\n",
    "        print('Recall: ' + str(round(recall,2)))\n",
    "        print('F1: ' + str(round(F1,2)))\n",
    "        print('Accuracy: ' + str(round(accuracy,2)))\n",
    "\n",
    "        return precision, recall, F1, accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 out of 1\n",
      "Average Train Loss: nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ericb\\anaconda3\\envs\\machinelearning_20220719\\lib\\site-packages\\ipykernel_launcher.py:123: RuntimeWarning: divide by zero encountered in log\n",
      "c:\\Users\\ericb\\anaconda3\\envs\\machinelearning_20220719\\lib\\site-packages\\ipykernel_launcher.py:123: RuntimeWarning: invalid value encountered in multiply\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.56\n",
      "Recall: 0.96\n",
      "F1: 0.71\n",
      "Accuracy: 0.6\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    lr = LogisticRegression(n_features=4)\n",
    "    # make sure these point to the right directories\n",
    "    lr.train('movie_reviews/train', batch_size=3, n_epochs=1, eta=0.1)\n",
    "    # lr.train('movie_reviews_small/train', batch_size=3, n_epochs=1, eta=0.1)\n",
    "    results = lr.test('movie_reviews/dev')\n",
    "    # results = lr.test('movie_reviews_small/test')\n",
    "    lr.evaluate(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "machinelearning_20220719",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
